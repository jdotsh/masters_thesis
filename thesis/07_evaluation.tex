\chapter{Results \& Discussion} \label{ch:evaluation}

In this section, we compare the performance of several different concurrent priority
queue implementations:

\begin{itemize}
\item \textit{GlobalLock} An instance of the \lstinline|std::priority_queue<T>| class provided
      by the C++ standard library, protected by a single global lock.
\item \textit{Heap} \citeauthor{hunt1996efficient} provide an implementation
      of their Heap-based design \cite{hunt1996efficient} at \url{ftp://ftp.cs.rochester.edu/pub/packages/concurrent_heap/}.
      However, as the original code was written for the MIPS architecture, we chose to
      use the alternative implementation in \emph{libcds}\footnote{\url{http://sourceforge.net/p/libcds/code/}}
      instead. The heap capacity was $2^{18}$ --- lower values led to a quasi-deadlock situation under high concurrency
      as described in \cite{dragicevic2008survey}.
\item \textit{Noble} An implementation of the \citeauthor{sundell2003fast} priority queue \cite{sundell2003fast}
      is available in the Noble library\footnote{\url{http://www.non-blocking.com/Eng/download-demo.aspx}}.
      \textit{Noble} is lock-free, based on SkipLists, and limited to unique priorities.
      According to the authors, the commercial Noble library includes a more efficient
      version of this data structure that can also handle duplicate priority values.
\item \textit{Linden} Code for the \citeauthor{linden2013skiplist} priority queue \cite{linden2013skiplist}
      is provided by the authors under an open source license\footnote{\url{http://user.it.uu.se/~jonli208/priorityqueue}}.
      It is lock-free and uses \citeauthor{fraser2004practical}'s lock-free
      SkipList. The aim of this implementation is to minimize contention in
      calls to \lstinline|DeleteMin|. We chose $32$ as the \lstinline|BoundOffset| in order to optimize
      performance on a single socket. A \lstinline|BoundOffset| of $128$ performed only marginally better
      at high thread counts.
\item \textit{SprayList} A relaxed concurrent priority queue based on \citeauthor{fraser2004practical}'s
      SkipList using random walks to spread data accesses
      incurred by \lstinline|DeleteMin| calls. Code provided by \citeauthor{alistarhspraylist} is
      available on Github\footnote{\url{https://github.com/jkopinsky/SprayList}}.
\end{itemize}

Unfortunately, we were not able to obtain an implementation of the \citeauthor{shavit2000skiplist}
priority queue for benchmarking. The \citeauthor{wimmer2013data} data structure was omitted in the
following benchmarks since it is tightly coupled with the task-scheduling framework \emph{Pheet},
and cannot directly be compared to other implementations in its current form.

In each test run, the examined priority queue was initially filled with $2^{15}$
elements. We then ran a tight loop of $50\%$ insertions and $50\%$ deletions
for a total of $10$ seconds, where all \lstinline|Insert|
operations within this context implicitly choose a key uniformly at random from
the range of all 32-bit integers. This methodology seems to be the de facto standard for concurrent
priority queue benchmarks
\cite{alistarhspraylist,linden2013skiplist,shavit2000skiplist,sundell2003fast}.
Each run was repeated for a total of $10$ times
and we report on the average throughput.

All benchmarks were compiled with \verb|-O3| using GCC 4.8.2, and executed with threads pinned to
cores. Evaluations took place
on an 80-core Intel Xeon E7-8850 machine with each processor clocked at 2 GHz.
The benchmarking code was adapted from \citeauthor{linden2013skiplist}'s
benchmarking suite and is available at \url{https://github.com/schuay/seminar_in_algorithms}.

\begin{center}
\includegraphics[width = \textwidth]{graphics/fig.eps}\end{center}

The \textit{Linden} priority queue emerged as the clear winner of these benchmarks,
with impressive scalability while all threads remain on the same socket. In fact, it is the only
tested implementation to scale to any significant degree; \textit{Heap} achieves minor gains
for up to 3 threads, while all others immediately lose throughput under concurrency.
Performance of the \textit{Linden} queue drops significantly once it is executed on more than 10
threads, thus incurring communication overhead across sockets. This effect is repeated to a lesser
degree every time an additional socket becomes active. However, the \textit{Linden} queue has
clearly superior throughput until the tested maximum of 80 threads.

\textit{GlobalLock} also performs surprisingly well at high concurrency levels.
It remains competitive when compared to \textit{Linden} at about $\frac{2}{3}$ of its throughput.
For single-threaded execution, it achieves the highest throughput of all tested implementations.
It is interesting to note that the \textit{GlobalLock} shows complementary behavior to
\textit{Linden} when new sockets become active; we believe that the additional latency incurred
by cross-socket communications might lead to reduced contention at the single lock.

All remaining implementations behave similarly, displaying very low throughput under concurrency.
This is a surprise in the case of \textit{Noble}; the original benchmarks performed by
\citeauthor{sundell2003fast} led us to expect improvements upon both \textit{GlobalLock}
and \textit{Heap} \cite{sundell2003fast}. However, we are somewhat reassured in our benchmarks by
the fact that \citeauthor{linden2013skiplist}'s results are similar.

The \textit{SprayList} result is even more unexpected, and we believe that it may be caused by
our use of \lstinline|malloc| instead of the included custom lock-free allocator, which we were
unable to execute without errors.

As a final observation we note that even the most efficient strict concurrent priority queue
never exceeds the throughput of the simple sequential heap executed on a single thread by more
than a factor of two. There is hope however since relaxed data structures continue to scale
beyond 10 threads (see the benchmarks performed in \cite{alistarhspraylist} for an example).
