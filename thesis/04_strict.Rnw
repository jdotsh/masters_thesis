\chapter{Strict Concurrent Priority Queues} \label{ch:strict}

This chapter deals with priority queues with strict semantics which guarantee
that deletions return the minimal element within the queue. We provide an overview
of the development of such priority queues, from early locking heap-based designs,
through more recent lock-free queues backed by SkipList, to one of the latest
designs called the \ac{CBPQ}.

\section{Fine-grained Locking Heaps} \label{sec:hunt}

In the following two chapters, we will discuss implementations of several concurrent priority queue implementations.
Early designs have mostly been based on search trees \cite{boyar1994chromatic,johnson1991highly} and
heaps \cite{ayani1990lr,biswas1987simultaneous,das1996distributed,deo1992parallel,huang1991evaluation,
luchetti1993some,mans1998portable,olariu1991optimal,prasad1995parallel}.
We chose the priority queue by \citeauthor{hunt1996efficient} \cite{hunt1996efficient}
as a representative of early concurrent priority queues since it has been shown to
perform well \cite{shavit2000skiplist} in comparison to other efforts of the time such as \cite{nageshwara1988concurrent,ayani1990lr,yan1998lock}. % Many others \cite{shavit2000skiplist}.
It is based on a Heap
structure and attempts to minimize lock contention between threads by a) adding per-node
locks, b) spreading subsequent insertions through a bit-reversal technique, % Elaborate on these.
and c) letting insertions traverse bottom-up in order to minimize conflicts with
top-down deletions.

However, significant limitations to scalability remain. A global lock is required
to protect accesses to a variable storing the Heap's size which all operations
must obtain for a short time. Disjoint-access through bit-reversal breaks down
once a certain amount of traffic is reached, since only subsequent insertions
are guaranteed to take disjoint paths towards the root node. Note also that
the root node is a severe serial bottleneck, since it is potentially part of
every insertion path, and necessarily of every \lstinline|delete_min| operation.
Finally, in contrast to later dynamic SkipList-based designs, the capacity of Hunt Heaps
is fixed upon creation.

Benchmarking results in the literature have been mixed; a sequential priority
queue protected by a single global lock outperforms the \citeauthor{hunt1996efficient}
Heap in most cases \cite{hunt1996efficient,sundell2003fast}. Speed-up only occurs once
the size of the Heap reaches a certain threshold such that concurrency
can be properly exploited instead of being dominated by global locking overhead.

\section{Lock-free Priority Queues} \label{sec:lockfree}

Traditional data structures such as heaps have fallen out of favor;
instead, SkipLists \cite{pugh1990skip,pugh1998concurrent} have become the focus
of modern concurrent priority queue research
\cite{shavit2000skiplist,sundell2003fast,herlihy2012art,linden2013skiplist,alistarhspraylist}.
SkipLists are both conceptually simple as well as easy to implement; they also exhibit
excellent disjoint-access parallelism properties, and do not require rebalancing due to their
reliance on randomization.

A state of the art lock-free SkipList implementation based on the \ac{CAS} instruction
by \citeauthor{fraser2004practical} \cite{fraser2004practical} is freely available\footnote{
\url{http://www.cl.cam.ac.uk/research/srg/netos/lock-free/}, last visited on December 9\textsuperscript{th}, 2015.}
under a BSD license.
\citeauthor{fraser2004practical} exploits unused pointer bits to mark nodes as logically
deleted, with physical deletion following as a second step.

SkipLists are dynamic data structures in the sense that they grow and shrink
at runtime. In consequence, careful handling of memory accesses and (de)allocations
are required. As an additional requirement, these memory management schemes must
themselves be both scalable and lock-free to avoid limiting the SkipList themselves.
\citeauthor{fraser2004practical} in particular employs lock-free epoch-based garbage-collection,
which frees a memory segment once all threads that could have seen a pointer to it have
exited the data structure.

The following sections cover several prominent examples of such lock-free priority queues.


\section{The \citeauthor{shavit2000skiplist} Priority Queue} \label{sec:shavit}

\citeauthor{shavit2000skiplist} were the first to propose the use of SkipLists
for priority queues \cite{linden2013skiplist}. Their initial locking implementation
\cite{shavit2000skiplist} builds on \citeauthor{pugh1998concurrent}'s concurrent
SkipList \cite{pugh1998concurrent}, which uses one lock per node per level.

A crucial observation is that nodes which are only partially connected (in the sense
that while the lowest level has been connected but not necessarily all other levels)
do not affect correctness of the data structure. As soon as the first level (i.e. \lstinline|node.next[0]|)
has been successfully connected, a node is considered to be in the SkipList.
Therefore, both insertions and deletions can be split into steps --- insertions
proceed bottom-up while deletions proceed top-down. Locks are held only for the current level
which helps to reduce contention between threads.

\begin{figure}[ht]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as in Figure (@*\textcolor{Gray}{\ref{fig:skiplist_members}}*@). */
  atomic<bool> deleted; /**< Initially false. */
  time_t timestamp;
  lock_t locks[level + 1];
};
\end{lstlisting}
\caption{\citeauthor{shavit2000skiplist} structure.}
\label{fig:shavitsl}
\end{figure}

Likewise, deletions are split into a logical phase (atomically setting the \lstinline|node.deleted|
flag) and a physical phase which performs the actual pointer manipulations and can be seen as a simple
call to the underlying SkipList's \lstinline|delete_min| function.

A \lstinline|delete_min| call starts at the list head, and attempts to atomically set
the deletion flag using a \lstinline|CAS(node.deleted, false, true)| call (or equivalent constructs).
If it succeeds, the current node is physically deleted and returned to the caller. Otherwise,
\lstinline|node.next[0]| is set as the new current node and the procedure is repeated.
If the end of the list is reached, \lstinline|delete_min| returns false to indicate an empty list.

% Logical/physical deletion separation allows concurrent physical deletions.

\begin{figure}
\centering
\begin{tikzpicture}[start chain,
    ->,
    every node/.style={font = \small},
    label/.style={rectangle,minimum size = 5mm}
  ]

    \begin{scope}[start chain = 1 going right]
	\node [on chain=1] (t1_start) {Thread A};
	\node [on chain=1, xshift = 90mm] (t1_end) {};
    \end{scope}

    \begin{scope}[start chain = 2 going right]
	\node [on chain=2, below = of t1_start, yshift = 7.5mm] (t2_start) {Thread B};
	\node [on chain=2, xshift = 90mm] (t2_end) {};
    \end{scope}

    \path[dashed, -stealth'] (t1_start) edge node [above] {} (t1_end);
    \path[dashed, -stealth'] (t2_start) edge node [above] {} (t2_end);

    \path[line width = 1mm, serif cm-serif cm, every node/.style= { font = \footnotesize }]
	($(t1_start.east) + (10mm, 0)$) edge node [above] {\lstinline|delete_min(2)|}
	($(t1_start.east) + (90mm, 0)$);

    \path[line width = 1mm, serif cm-serif cm, every node/.style= { font = \footnotesize }]
	($(t2_start.east) + (20mm, 0)$) edge node [above] {\lstinline|insert(1)|}
	($(t2_start.east) + (40mm, 0)$)
	($(t2_start.east) + (50mm, 0)$) edge node [above] {\lstinline|insert(2)|}
	($(t2_start.east) + (70mm, 0)$);
\end{tikzpicture}
\caption{Non-linearizability of the \citeauthor{shavit2000skiplist} queue without timestamping.}
\label{fig:non_linearizable_sl_queue}
\end{figure}

Note that so far this implementation is not linearizable: consider the case in which a
slow thread A is in the middle of a \lstinline|delete_min| call (see Figure 
\ref{fig:non_linearizable_sl_queue}). Within this context, we refer to
the node with key $i$ as node $i$, or simply $i$. Several \lstinline|CAS|
operations have failed, and A is currently at node $j$.
A fast thread B then first inserts a node $i$, followed by a node $k$ such that
$i < j < k$, i.e. the former and latter nodes are inserted, respectively, before and after
thread A's current node. Assuming further that all nodes between $j$ and $k$ have already
been deleted, then thread A will return node $k$. This execution is not linearizable; however,
it is quiescently consistent since operations can be reordered to correspond to some sequential
execution at quiescent periods.

\citeauthor{shavit2000skiplist} counteract this by introducing a \lstinline|timestamp| for each
node which is set upon successful insertion. In this variant, each \lstinline|delete_min| operation
simply ignores all nodes it sees that have not been fully inserted at the time it was called. In the
above example, both newly inserted nodes would be ignored by the slow thread A and thus linearizability
is preserved.

Explicit memory management is required to avoid dereferencing pointers to freed memory areas
by other threads after physical deletion. This implementation uses a dedicated garbage collector
thread in combination with a timestamping mechanism which frees \lstinline|node|'s memory only
when all threads that might have seen a pointer to \lstinline|node| have exited the data structure.
All threads write their last access times into a dedicated space in shared memory. Deleted nodes
are appended to a deletion list. The garbage collector thread continually scans this list and
frees all nodes that have been deleted prior to the earliest last-accessed time by any thread.

\citeauthor{herlihy2012art} \cite{herlihy2012art} recently described and implemented a lock-free,
quiescently consistent version of this idea in Java. While mostly identical, notable differences are
that a) the new variant is based on a lock-free skiplist, b) the timestamping mechanism was not
employed and thus linearizability was lost, and c) explicit memory management is not required
because the Java virtual machine provides a garbage collector.

\section{\citeauthor{sundell2003fast}} \label{sec:sundell}

\citeauthor{sundell2003fast} proposed the first lock-free concurrent priority queue in
\citeyear{sundell2003fast} \cite{sundell2003fast}. The data structure is linearizable
and is implemented using commonly available atomic primitives \ac{CAS}, \ac{TAS}, and \ac{FAA}.
In contrast to other structures covered in this thesis, this priority queue is restricted to
contain items with distinct priorities. Inserting a new item with a priority already contained
in the list simply performs an update of the associated value.

\begin{figure}[ht]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as in Figure (@*\textcolor{Gray}{\ref{fig:skiplist_members}}*@). */
  value_t *value; /**< Values are stored by pointer. */
  size_t valid_level;
  node_t *prev;
};
\end{lstlisting}
\caption{\citeauthor{sundell2003fast} structure.}
\label{fig:sundellsl}
\end{figure}

The structure of each node is as indicated in Figure \ref{fig:skiplist_members}.
However, \citeauthor{sundell2003fast}
exploit the fact that the two least significant bits of pointers on 32- and 64-bit systems
are unused (due to alignment of allocated memory) and reuse these as deletion marks.
A simple \ac{CAS} operation can then be used to atomically update the pointer and the deletion marks:
\lstinline!CAS(node.next[i], ptr, ptr | 1)!.
A set least significant bit of a pointer signifies
that the current node is about to be deleted.
This packed use of \lstinline|node.next[i]| pointers
prevents situations in which a new node is inserted while its predecessor is being removed,
effectively deleting both nodes from the list. Likewise, the reuse of the \lstinline|node.value|
pointer ensures that updates of pointer values (which occur when a node with the inserted priority already exists)
handle concurrent node removals correctly.

As in the \citeauthor{shavit2000skiplist} priority queue, insertions proceed bottom-up while
deletions proceed top-down --- on the one hand, the choice of opposite directions reduces collisions
between concurrent insert and delete operations, while on the other hand removing nodes from top levels first
allows many other concurrent operations to simply skip these nodes, further improving performance.
The \lstinline|node.valid_level| variable is updated during inserts to always equal the highest level of the SkipList
at which pointers in this node have already been set (as opposed to \lstinline|node.level|, which equals
the final level of the node).

A helping mechanism is employed whenever a node is encountered that has its deletion bit set, which attempts
to set the deletion bits on all next pointers and then removes the node from the current level. The
\lstinline|node.prev| pointer is used as a shortcut to the previous node, avoiding a complete retraversal
of the list.

This implementation uses the lock-free memory management invented by \citeauthor{valois1996lock}
\cite{valois1995lock,valois1996lock} and corrected by \citeauthor{michael1995correction}
\cite{michael1995correction}. It was chosen in particular because this scheme can guarantee validity
of \lstinline|prev| as well as all \lstinline|next| pointers. Additionally, it does not require a separate
garbage collector thread.

A rigorous linearizability proof is provided in the original paper \cite{sundell2003fast} which shows
linearization points for all possible outcomes of all operations.

Benchmarks performed by \citeauthor{sundell2003fast} show their queue performing noticeably better than both locking
queues from Sections \ref{sec:shavit} and \ref{sec:hunt}, and slightly better than a priority queue
consisting of a SkipList protected by a single global lock.

\section{\citeauthor{linden2013skiplist}} \label{sec:linden}

A recent priority queue implementations was published by \citeauthor{linden2013skiplist}
in \citeyear{linden2013skiplist} \cite{linden2013skiplist}. They present a linearizable, lock-free concurrent priority
queue which achieves a speed-up of $30-80\%$ compared to other SkipList-based priority queues by
minimizing the number of \ac{CAS} operations within most \lstinline|delete_min| operations.

\begin{figure}
\centering
\begin{tikzpicture}[
    ->,
    start chain,
    every node/.style={font = \small},
    item/.style= box,
    label/.style={rectangle,minimum size = 5mm}
    ]

    \node[box] (3a) at (0, 15mm) {}; \node[box] (3h) at (70mm, 15mm) {};

    \node[box] (2a) at (0, 10mm) {}; \node[deleted_box] (2c) at (20mm, 10mm) {};
    \node[box] (2e) at (40mm, 10mm) {}; \node[box] (2h) at (70mm, 10mm) {};

    \node[box] (1a) at (0, 5mm) {}; \node[deleted_box] (1c) at (20mm, 5mm) {};
    \node[box] (1d) at (30mm, 5mm) {}; \node[box] (1e) at (40mm, 5mm) {}; \node[box] (1f) at (50mm, 5mm) {};
    \node[box] (1h) at (70mm, 5mm) {};

    \node[box] (0a) at (0, 0) {}; \node[deleted_box] (0b) at (10mm, 0) {0}; \node[deleted_box] (0c) at (20mm, 0) {1};
    \node[box] (0d) at (30mm, 0) {2}; \node[box] (0e) at (40mm, 0) {3}; \node[box] (0f) at (50mm, 0) {4};
    \node[box] (0g) at (60mm, 0) {5}; \node[box] (0h) at (70mm, 0) {};


    {
    [start chain] \chainin(0a); \chainin(0b) [join]; \chainin(0c) [join]; \chainin(0d) [join]; \chainin(0e) [join]; \chainin(0f) [join]; \chainin(0g) [join]; \chainin(0h) [join];
    [start chain] \chainin(1a); \chainin(1c) [join]; \chainin(1d) [join]; \chainin(1e) [join]; \chainin(1f) [join]; \chainin(1h) [join];
    [start chain] \chainin(2a); \chainin(2c) [join]; \chainin(2e) [join]; \chainin(2h) [join];
    [start chain] \chainin(3a); \chainin(3h) [join];
    }

    \node[flag_true] at (1.5mm, -1.5mm) {};
    \node[flag_true] at (11.5mm, -1.5mm) {};
    \node[flag_false] at (21.5mm, -1.5mm) {};
    \node[flag_false] at (31.5mm, -1.5mm) {};
    \node[flag_false] at (41.5mm, -1.5mm) {};
    \node[flag_false] at (51.5mm, -1.5mm) {};
    \node[flag_false] at (61.5mm, -1.5mm) {};
\end{tikzpicture}
\caption[A \citeauthor{linden2013skiplist} priority queue with two deleted elements.]
        {A \citeauthor{linden2013skiplist} priority queue with two deleted elements.
        Deletion flags are shown as a filled (if set) or clear (unset) circle
        at the lowest node level.
        Note that deletion flags are located in their predecessors of deleted nodes $0$ and $1$.}
\end{figure}

% TODO_FINAL: Why is the following paragraph here?

A priority queue implementation is called strict when \lstinline|delete_min| is
guaranteed to return the minimal element currently within the queue
(in contrast to relaxed data structures which are discussed further in the next chapter).
All such priority queues have an inherent bottleneck, since all threads calling \lstinline|delete_min| compete
for the minimal element, causing both contention through concurrent \ac{CAS} operations on the same variable
as well as serialization effort by the cache coherence protocol for all other processor accessing the same cache
line.

\begin{figure}[ht]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as in Figure (@*\textcolor{Gray}{\ref{fig:skiplist_members}}*@). */
  atomic<bool> inserting;
};
\end{lstlisting}
\caption{\citeauthor{linden2013skiplist} structure.}
\label{fig:lindensl}
\end{figure}

In this implementation, most \lstinline|delete_min| operations only perform logical deletion by setting
the deletion flag with a single \ac{FAO} call; nodes are only deleted physically once a certain
threshold of logically deleted nodes is reached.

This mechanism guarantees that the set of all logically deleted nodes must always
form a prefix of the SkipList. Recall that in the \citeauthor{sundell2003fast} queue, deletion flags
for node \lstinline|n| were packed into \lstinline|n.next| pointers, preventing insertion of new
nodes \emph{after} deleted nodes. This implementation instead places the deletion flag into the
lowest level \lstinline|next| pointer of the previous node, preventing insertions \emph{before}
logically deleted nodes.

Once the prefix of logically deleted nodes reaches a specified length (represented by \lstinline|BoundOffset|),
the first thread to observe this fact within a \lstinline|delete_min| performs the actual physical
deletions by updating \lstinline|slist.head[0]|, the lowest level pointer to the head of the list,
to point at the last logically deleted node with a
single \ac{CAS} operation. The remaining \lstinline|slist.head| pointers are then updated, and
all physically deleted nodes are marked for recycling.

Since at any time, the data structure contains a prefix of logically deleted nodes, all \lstinline|delete_min|
operations must traverse this sequence before reaching a non-deleted node. In general, reads of nonmodified
memory locations are very cheap; however, benchmarks in \cite{linden2013skiplist} have shown that
after a certain point, the effort spent in long read sequences significantly outweighs the reduced
number of \ac{CAS} calls. It is therefore crucial to choose \lstinline|BoundOffset| carefully, with the
authors recommending a prefix length bound of 128 for 32 threads.

% TODO: Clarify pseudocode below.

The actual \lstinline|delete_min| and \lstinline|insert| implementations are surprisingly simple.
Deletions simply traverse the list until the first node for which \lstinline|(ptr, d) = FAO(node.next[0], 1)|
returns an unset deletion flag (\lstinline|d = 0|), and then return \lstinline|ptr|.
Insertions occur bottom-up and follow the basic \citeauthor{fraser2004practical} algorithm \cite{fraser2004practical},
taking the separation of deletion flags and nodes into account. The \lstinline|node.inserting| flag
is set until the node has been fully inserted, and prevents moving the list head past a partially
inserted node. \citeauthor{fraser2004practical}'s epoch-based reclamation scheme \cite{fraser2004practical}
is used for memory management.

The authors provide high level correctness and linearizability proofs as well as a model for the
SPIN model checker. Performance has been shown to compare favorably to both
\citeauthor{sundell2003fast} and \citeauthor{shavit2000skiplist} queues, with throughput improved by
up to $80\%$.

\section{\acl{CBPQ}} \label{sec:cbpq}

The latest strict priority queue of interest, called the \ac{CBPQ}, was published very recently in a dissertation
by \citeauthor{cbpq} \cite{cbpq}. It is primarily based on two main ideas: the chunk linked list
\cite{braginsky2011locality} replaces SkipLists and heaps as the backing data structure,
and use of the more efficient \ac{FAA} instruction is preferred over the more powerful
\ac{CAS} instruction.

The chunked linked list is a lock-free implementation of a linked list of arrays (chunks), each of
which is sized to approximately fit into a memory page and thus provide high locality
upon traversal. Within the \ac{CBPQ}, each chunk is assigned to a particular keyrange,
and a skiplist is used to efficiently index into the available set of chunks during insertions.

A chunk itself consists of an element array, the upper bound for keys within the chunk, the index of
the first unused field within the array, and several further fields related to memory management 
(which we ignore in this context --- for further details about the utilized 
lock-free memory management scheme called `Drop the Anchor', refer to \cite{braginsky2011locality,braginsky2013drop}).

One of the key elements of the \ac{CBPQ} is that in general, items within chunks are not
ordered and can thus be inserted in a lock-free manner by simply fetching and incrementing
a chunk's index field using an \ac{FAA} instruction, and then physically inserting the item
in the appropriate spot. If a chunk exceeds its maximum size after insertion, it is split
into two new chunks, each owning half of the original keyrange.

Deletions take items exclusively from the first chunk since it contains the lowest range of keys.
This initial chunk is a special case within the design; its item array is immutable (read-only)
and may thus be efficiently read
by all threads with reduced cache coherence traffic. Again, a shared index is used in
conjuction with the \ac{FAA} instruction to determine an item to remove in an
efficient manner.

Whenever a new initial chunk must be constructed for any reason (e.g., a new item
within its keyrange is inserted, or the current initial chunk is empty), 
the respective items are first inserted into an buffer. The inserting thread waits
for a short amount of time in order to possibly batch together insertions
from several threads. All participating threads then cooperate to create a new
initial chunk.

Benchmarks compare the \ac{CBPQ} against the \citeauthor{linden2013skiplist} queue and
lock-free as well as lock-based versions of the Mound priority queue \cite{liu2012lock}
for different workloads. The \ac{CBPQ} clearly outperforms the other queues in mixed workloads
($50\%$ insertions, 50\% insertions) and deletion workloads, and exhibits similar
behavior as the \citeauthor{linden2013skiplist} queue in insertion workloads, where Mounds
are dominant.
