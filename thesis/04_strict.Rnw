\chapter{Strict Concurrent Priority Queues} \label{ch:strict}

\section{Fine-grained Locking Heaps} \label{sec:hunt}

In the following two chapters, we will discuss implementations of several concurrent priority queue implementations.
Early designs have mostly been based on search trees \cite{boyar1994chromatic,johnson1991highly} and
heaps \cite{ayani1990lr,biswas1987simultaneous,das1996distributed,deo1992parallel,huang1991evaluation,
luchetti1993some,mans1998portable,olariu1991optimal,prasad1995parallel}.
We chose the priority queue by \citeauthor{hunt1996efficient} \cite{hunt1996efficient}
as a representative of early concurrent priority queues since it has been proven to
perform well \cite{shavit2000skiplist} in comparison to other efforts of the time such as \cite{nageshwara1988concurrent,ayani1990lr,yan1998lock}. % Many others \cite{shavit2000skiplist}.
It is based on a Heap
structure and attempts to minimize lock contention between threads by a) adding per-node
locks, b) spreading subsequent insertions through a bit-reversal technique, % Elaborate on these.
and c) letting insertions traverse bottom-up in order to minimize conflicts with
top-down deletions.

However, significant limitations to scalability remain. A global lock is required
to protect accesses to a variable storing the Heap's size which all operations
must obtain for a short time. Disjoint-access through bit-reversal breaks down
once a certain amount of traffic is reached, since only subsequent insertions
are guaranteed to take disjoint paths towards the root node. Note also that
the root node is a severe serial bottleneck, since it is potentially part of
every insertion path, and necessarily of every \lstinline|DeleteMin| operation.
Finally, in contrast to later dynamic SkipList-based designs, the capacity of Hunt Heaps
is fixed upon creation.

Benchmarking results in the literature have been mixed; a sequential priority
queue protected by a single global lock outperforms the \citeauthor{hunt1996efficient}
Heap in most cases \cite{hunt1996efficient,sundell2003fast}. Speed-up only occurs once
the size of the Heap reaches a certain threshold such that concurrency
can be properly exploited instead of being dominated by global locking overhead.

\section{Lock-free Priority Queues} \label{sec:lockfree}

Traditional data structures such as the Heap have fallen out of favor;
instead, SkipLists \cite{pugh1990skip,pugh1998concurrent} have become the focus
of modern concurrent priority queue research
\cite{shavit2000skiplist,sundell2003fast,herlihy2012art,linden2013skiplist,alistarhspraylist}.
SkipLists are both conceptually simple as well as easy to implement; they also exhibit
excellent disjoint-access parallelism properties, and do not require rebalancing due to their
reliance on randomization.

A state of the art lock-free SkipList implementation based on the \ac{CAS} instruction
by \citeauthor{fraser2004practical} \cite{fraser2004practical} is freely available\footnote{
\url{http://www.cl.cam.ac.uk/research/srg/netos/lock-free/}} under a BSD license.
\citeauthor{fraser2004practical} exploits unused pointer bits to mark nodes as logically
deleted, with physical deletion following as a second step.

SkipLists are dynamic data structures in the sense that they grow and shrink
at runtime. In consequence, careful handling of memory accesses and (de)allocations
are required. As an additional requirement, these memory management schemes must
themselves be both scalable and lock-free to avoid limiting the SkipList itself.
\citeauthor{fraser2004practical} in particular employs lock-free epoch-based garbage-collection,
which frees a memory segment only once all threads that could have seen a pointer to it have
exited the data structure.


\section{\citeauthor{shavit2000skiplist}} \label{sec:shavit}

\citeauthor{shavit2000skiplist} were the first to propose the use of SkipLists
for priority queues \cite{linden2013skiplist}. Their initial locking implementation
\cite{shavit2000skiplist} builds on \citeauthor{pugh1998concurrent}'s concurrent
SkipList \cite{pugh1998concurrent}, which uses one lock per node per level.

A crucial observation is that nodes which are only partially connected
do not affect correctness of the data structure. As soon as the first level (i.e. \lstinline|node.level[0]|)
has been successfully connected, a node is considered to be in the SkipList.
Therefore, both insertions and deletions can be split into steps --- insertions
proceed bottom-up while deletions proceed top-down. Locks are held only for the current level
which helps to reduce contention between threads.

\begin{figure}[ht]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as above. */
  atomic<bool> deleted; /**< Initially false. */
  time_t timestamp;
  lock_t locks[level + 1];
};
\end{lstlisting}
\caption{\citeauthor{shavit2000skiplist} structure.}
\label{fig:shavitsl}
\end{figure}

Likewise, deletions are split into a logical phase (atomically setting the \lstinline|node.deleted|
flag) and a physical phase which performs the actual pointer manipulations and can be seen as a simple
call to the underlying SkipList's \lstinline|sl_delete| function.

A \lstinline|DeleteMin| call starts at the list head, and attempts to atomically set
the deletion flag using a \lstinline|CAS(node.deleted, false, true)| call (or equivalent constructs).
If it succeeds, the current node is physically deleted and returned to the caller. Otherwise,
\lstinline|node.next[0]| is set as the new current node and the procedure is repeated.
If the end of the list is reached, \lstinline|DeleteMin| returns false to indicate an empty list.

% Logical/physical deletion separation allows concurrent physical deletions.

Note that so far this implementation is not linearizable: consider the case in which a
slow thread A is in the middle of a \lstinline|DeleteMin| call. Within this context, we refer to
the node with key $i$ as node $i$, or simply $i$. Several \lstinline|CAS|
operations have failed, and A is currently at node $j$.
A fast thread B then first inserts a node $i$, followed by a node $k$ such that
$i < j < k$, i.e. the former and latter nodes are inserted, respectively, before and after
thread A's current node. Assuming further that all nodes between $j$ and $k$ have already
been deleted, then thread A will return node $k$. This execution is not linearizable; however,
it is quiescently consistent since operations can be reordered to correspond to some sequential
execution at quiescent periods.

\citeauthor{shavit2000skiplist} counteract this by introducing a \lstinline|timestamp| for each
node which is set upon successful insertion. In this variant, each \lstinline|DeleteMin| operation
simply ignores all nodes it sees that have not been fully inserted at the time it was called.

Explicit memory management is required to avoid dereferencing pointers to freed memory areas
by other threads after physical deletion. This implementation uses a dedicated garbage collector
thread in combination with a timestamping mechanism which frees \lstinline|node|'s memory only
when all threads that might have seen a pointer to \lstinline|node| have exited the data structure.

\citeauthor{herlihy2012art} \cite{herlihy2012art} recently described and implemented a lock-free,
quiescently consistent version of this idea in Java. While mostly identical, notable differences are
that a) the new variant is based on a lock-free skiplist, b) the timestamping mechanism was not
employed and thus linearizability was lost, and c) explicit memory management is not required
because the Java virtual machine provides a garbage collector.

\section{\citeauthor{sundell2003fast}} \label{sec:sundell}

\citeauthor{sundell2003fast} proposed the first lock-free concurrent priority queue in
\citeyear{sundell2003fast} \cite{sundell2003fast}. The data structure is linearizable
and is implemented using commonly available atomic primitives \ac{CAS}, \ac{TAS}, and \ac{FAA}.
In contrast to other structures covered in this thesis, this priority queue is restricted to
contain items with distinct priorities. Inserting a new item with a priority already contained
in the list simply performs an update of the associated value.
A real-time version is also provided which we will not discuss further (interested readers are
referred to \cite{sundell2003fast}).

\begin{figure}[ht]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as above. */
  size_t valid_level;
  node_t *prev;
};
\end{lstlisting}
\caption{\citeauthor{sundell2003fast} structure.}
\label{fig:sundellsl}
\end{figure}

The structure of each node is basically identical to Figure \ref{fig:basicsl}. However, \citeauthor{sundell2003fast}
exploit the fact that the two least significant bits of pointers on 32- and 64-bit systems
are unused and reuse these as deletion marks. A set least significant bit on a pointer signifies
that the current node is about to be deleted.
Reuse of \lstinline|node.level[i]| pointers
prevents situations in which a new node is inserted while its predecessor is being removed,
effectively deleting both nodes from the list. Likewise, the reuse of the \lstinline|node.value|
pointer ensures that updates of pointer values (which occur when a node with the inserted priority already exists)
handle concurrent node removals correctly.

As in the \citeauthor{shavit2000skiplist} priority queue, insertions proceed bottom-up while
deletions proceed top-down --- on the one hand, the choice of opposite directions reduces collisions
between concurrent insert and delete operations, while on the other hand removing nodes from top levels first
allows many other concurrent operations to simply skip these nodes, further improving performance.
\lstinline|node.valid_level| is updated during inserts to always equal the highest level of the SkipList
at which pointers in this node have already been set (as opposed to \lstinline|node.level|, which equals
the final level of the node).

A helping mechanism is employed whenever a node is encountered that has its deletion bit set, which attempts
to set the deletion bits on all next pointers and then removes the node from the current level. The
\lstinline|node.prev| pointer is used as a shortcut to the previous node, avoiding a complete retraversal
of the list.

This implementation uses the lock-free memory management invented by \citeauthor{valois1996lock}
\cite{valois1995lock,valois1996lock} and corrected by \citeauthor{michael1995correction}
\cite{michael1995correction}. It was chosen in particular because this scheme can guarantee validity
of \lstinline|prev| as well as all \lstinline|next| pointers. Additionally, it does not require a separate
garbage collector thread.

A rigorous linearizability proof is provided in the original paper \cite{sundell2003fast} which shows
linearization points for all possible outcomes of all operations.

Benchmarks performed by \citeauthor{sundell2003fast} show their queue performing noticeably better than both locking
queues from Sections \ref{sec:shavit} and \ref{sec:hunt}, and slightly better than a priority queue
consisting of a SkipList protected by a single global lock.

\section{\citeauthor{linden2013skiplist}} \label{sec:linden}

One of the most recent priority queue implementations was published by \citeauthor{linden2013skiplist}
in \citeyear{linden2013skiplist} \cite{linden2013skiplist}. They present a linearizable, lock-free concurrent priority
queue which achieves a speed-up of $30-80\%$ compared to other SkipList-based priority queues by
minimizing the number of \ac{CAS} operations within most \lstinline|DeleteMin| operations.

A priority queue implementation is called deterministic when the algorithm does not contain randomized elements.
It is called strict when \lstinline|DeleteMin| is guaranteed to return the minimal element currently within the queue
(in contrast to relaxed data structures which are discussed further in the next chapter).
All such priority queues share an inherent bottleneck, since all threads calling \lstinline|DeleteMin| compete
for the minimal element, causing both contention through concurrent \ac{CAS} operations on the same variable
as well as serialization effort by the cache coherence protocol for all other processor accessing the same cache
line.

\begin{figure}[ht]
\begin{lstlisting}
struct node_t {
  [...] /**< Standard node members as above. */
  atomic<bool> inserting;
};
\end{lstlisting}
\caption{\citeauthor{linden2013skiplist} structure.}
\label{fig:lindensl}
\end{figure}

In this implementation, most \lstinline|DeleteMin| operations only perform logical deletion by setting
the deletion flag with a single \ac{FAO} call; nodes are only deleted physically once a certain
threshold of logically deleted nodes is reached.

This mechanism requires a new invariant, in that the set of all logically deleted nodes must always
form a prefix of the SkipList. Recall that in the \citeauthor{sundell2003fast} queue, deletion flags
for node \lstinline|n| were packed into \lstinline|n.next| pointers, preventing insertion of new
nodes \emph{after} deleted nodes. This implementation instead places the deletion flag into the
lowest level \lstinline|next| pointer of the previous node, preventing insertions \emph{before}
logically deleted nodes.

Once the prefix of logically deleted nodes reaches a specified length (represented by \lstinline|BoundOffset|),
the first thread to observe this fact within \lstinline|DeleteMin| performs the actual physical
deletions by updating \lstinline|slist.head[0]| to point at the last logically deleted node with a
single \ac{CAS} operation. The remaining \lstinline|slist.head| pointers are then updated, and
all physically deleted nodes are marked for recycling.

Since at any time, the data structure contains a prefix of logically deleted nodes, all \lstinline|DeleteMin|
operations must traverse this sequence before reaching a non-deleted node. In general, reads of nonmodified
memory locations are very cheap; however, benchmarks in \cite{linden2013skiplist} have shown that
after a certain point, the effort spent in long read sequences significantly outweighs the reduced
number of \ac{CAS} calls. It is therefore crucial to choose \lstinline|BoundOffset| carefully, with the
authors recommending a prefix length bound of 128 for 32 threads.

The actual \lstinline|DeleteMin| and \lstinline|Insert| implementations are surprisingly simple.
Deletions simply traverse the list until the first node for which \lstinline|(ptr, d) = FAO((node.next[0], d), 1)|
returns a previously unset deletion flag (\lstinline|d = 0|) and then return \lstinline|ptr|.
Insertions occur bottom-up and follow the basic \citeauthor{fraser2004practical} algorithm \cite{fraser2004practical},
taking the separation of deletion flags and nodes into account. The \lstinline|node.inserting| flag
is set until the node has been fully inserted, and prevents moving the list head past a partially
inserted node. \citeauthor{fraser2004practical}'s epoch-based reclamation scheme \cite{fraser2004practical}
is used for memory management.

The authors provide high level correctness and linearizability proofs as well as a model for the
SPIN model checker. Performance has been shown to compare favorably to both
\citeauthor{sundell2003fast} and \citeauthor{shavit2000skiplist} queues, with throughput improved by
up to $80\%$.

\section{\acl{CBPQ}} \label{sec:cbpq}

The latest strict priority of interest, called the \ac{CBPQ}, was published very recently in a dissertation
by \citeauthor{cbpq} \cite{cbpq}. It is primarily based on two ideas: the chunk linked list
\cite{braginsky2011locality} replaces SkipLists and heaps as the backing data structure,
and use of the more efficient \ac{FAA} instruction is preferred over the more powerful
\ac{CAS} instruction.

The chunked linked list is a lock-free implementation of a linked list of arrays (chunks), each of
which is sized to approximately fit into a memory page and thus provide high locality
upon traversal. Within the \ac{CBPQ}, each chunk is assigned to a particular keyrange,
and a skiplist is used to efficiently index into the available set of chunks during insertions.

A chunk itself consists of an element array, the upper bound for keys within the chunk, the index of
the first unused field within the array, and several further fields related to memory management 
(which we ignore in this context --- for further details about the utilized 
lock-free memory management scheme called 'Drop the Anchor', refer to \cite{braginsky2011locality,braginsky2013drop}).

One of the key elements of the \ac{CBPQ} is that in general, items within chunks are not
ordered and can thus be inserted in a lock-free manner by simply fetching and incrementing
a chunk's index field using an \ac{FAA} instruction, and then physically inserting the item
in the appropriate spot. If a chunk exceeds its maximum size after insertion, it is split
into two new chunks, each owning half of the original keyrange.

Deletions take items exclusively from the first chunk since it contains the lowest range of keys.
This initial chunk is a special case within the design; its item array is immutable 
and may thus be efficiently read
by all threads with reduced cache coherence traffic. Again, a shared index in
conjuction with the \ac{FAA} instruction to determine an item to remove in an
efficient manner.

Whenever a new initial chunk must be constructed for any reason (e.g., a new item
within its keyrange is inserted, or the current initial chunk is empty), 
the respective items are first inserted into an buffer. The inserting thread waits
for a short amount of time in order to possibly batch together insertions
from several threads. All participating threads then cooperate to create a new
initial chunk.

Benchmarks compare the \ac{CBPQ} against the \cite{linden2013skiplist} queue and 
lock-free as well as lock-based versions of the Mound priority queue \cite{liu2012lock}
for different workloads. The \ac{CBPQ} clearly outperforms the other queues mixed workloads 
($50\%$ insertions, 50\% insertions) and deletion workloads, and exhibits similar
behavior as the \cite{linden2013skiplist} queue in insertion workloads, where Mounds
are dominant.