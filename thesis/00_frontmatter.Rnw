\begin{acknowledgements*}
To Martin and Jesper --- without your enthusiasm, ideas, and support, this thesis
would not have been possible.
\end{acknowledgements*}

\begin{kurzfassung}
\todo{Ihr Text hier.}
\end{kurzfassung}

\begin{abstract}
Priority queues are abstract data structures which store a set of key/value pairs
and allow efficient access to the item with the minimal (maximal) key. Such queues are an important
element in various areas of computer science such as algorithmics (i.e. Dijkstra's shortest
path algorithm) and operating system (i.e. priority schedulers).

The recent trend towards multiprocessor computing requires new implementations of basic
data structures which are able to be used concurrently and scale well to a large number
of threads. In particular, lock-free structures promise superior scalability by avoiding
the use of blocking synchronization primitives.

Concurrent priority queues have been extensively researched over the past decades.
In recent years, most research within the field has focused on SkipList-based structures,
based mainly on the fact that they exhibit very high disjoint access parallelism, i.e.,
modifications on different elements access disjoint parts of the data structure.
Contention between threads and traffic through the cache-coherency protocol
is therefore reduced.

However, scalability improvements have been very limited. On the one hand,
strict priority queues have an inherent sequential bottleneck since each \lstinline|delete_min|
operation attempts to access the single minimal (maximal) element. On the other,
SkipLists have less than optimal cache locality since each node is
usually allocated dynamically, which in turn results in fairly low throughput
for SkipList-based designs.

Relaxed data structures are a new and promising approach in which quality
guarantees are weakened in return for improved scalability.
The \klsm is a concurrent, lock-free, and relaxed priority queue design which
aims to improve scalability by 1) using arrays as backing data structures and the standard
merge algorithm as its central operation (for high cache locality); and 2) by relaxing
semantic guarantees to allow the \lstinline|delete_min| operation to return one
of the $(k + 1) \cdot P$ elements, where $P$ is the number of threads.

During the course of this thesis, we have implemented an improved standalone version of
the \klsm and explain its design and implementation in detail. We finally evaluate the
the \klsm against other state-of-the-art concurrent priority queues including
the Spraylist, the \citeauthor{linden2013skiplist} queue, and
\citeauthor{rihani2014multiqueues}'s Multiqueues.

In some benchmarking scenarios, including
the popular generic throughput benchmark, the \klsm shows exceptional scalability,
outperforming all other queues by up to a factor of ten; in others however,
its throughput drops dramatically. Out of the other designs, Multiqueues are the
clear winner, performing consistently across all benchmarks.
\end{abstract}
