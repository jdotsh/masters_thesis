\chapter{Sequential Priority Queues} \label{ch:sequential}

\begin{comment}
* What is a priority queue? Exact semantics.
* Brief overview of important sequential pqs - check books and surveys.
* More detailed explanation of relevant queue types: heaps, possibly binary
  trees & skiplists.
\end{comment}

Priority queues have a long history, having been extensively studied since the early
days of the field of computer science. Some of the oldest designs, such as the
heap, have remained popular in practical use up today. This chapter will
detail syntax and semantics of general shared memory priority queues,
and provide an overview of important sequential priority queue types,
going into further detail for those designs which are also relevant for
use in a concurrent environment.

\section{Syntax and Semantics}

Before going further, we define syntax and semantics for strict, sequential
priority queues as used in this thesis.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class priority_queue {
public:
    void insert(const K &key, const V &val);
    bool delete_min(V &val);
};
\end{lstlisting}
\caption{A generic priority queue class.}
\label{fig:priority_queue_interface}
\end{figure}

A priority queue is a data structure which holds a collection of key-value
pairs, and provides two methods (see Figure \ref{fig:priority_queue_interface}):
\lstinline|insert|, which takes a key
and a value argument, possibly of different types, and inserts them into the queue;
and \lstinline|delete_min|,
which writes the value of the least item within the queue into the given parameter,
and returns true if the operation succeeded. \lstinline|delete_min| may fail if,
for instance, the priority queue is empty.

There are various further possible extensions to the given interface:
\begin{itemize}
\item \lstinline|empty| states whether the queue is empty.
\item \lstinline|peek_min| returns the minimal item's value without removing
      it from the queue.
\item \lstinline|decrease_key| decreases the key of a given item. This operation
      is vital e.g. for an efficient implementation of the Dijkstra shortest
      paths algorithm\cite{dijkstra1959note}.
\item \lstinline|meld| merges two priority queues.
\end{itemize}
However, these are outside the scope of this thesis.

Each priority queue is associated with a priority function, which maps
the key domain $K$ to a priority domain $P$, over which there exists
a reflective, antisymmetric, and transitive ordering relation $\leq: P \times P$.

When $K$ is the set of integers $\mathbb{Z}$, a priority queue is termed a
"max-priority queue" if the priority function is the identity function $f(x) = x$,
i.e., if higher keys have higher priority.
It is called a "min-priority queue" if the priority function is the additive
inverse $f(x) = -x$. All queues within this thesis are min-queues.

We are interested mainly in general priority queues within the context of shared
memory systems. General priority queues are data structures which
allow arbitrary keys of a given, possibly infinite set, and furthermore can
hold multiple distinct items with identical keys. They are
are congruent the sorting problem, and thus have a lower bound of $O(\log n)$
complexity for at least one of the main operations
(\lstinline|insert| and \lstinline|delete_min|). Note that these bounds do not
hold for more specialized priority queues; for instance, queues which are limited
to a small, previously known set of keys may use a bucket for each key and
implement both insertions and deletions in constant time.

The semantics of general sequential priority priority queues are as follows:
\begin{itemize}
\item The contents of a priority queue $PQ$ are a set of key-value pairs:
      $PQ \subseteq K \times V$, where $K$ and $V$ are, respectively, the key
      and value domains.
\item Upon creation, at time $t = 0$, a priority queue is empty:
      $PQ_{t = 0} = \emptyset$.
\item Let $PQ^+_t$ be the set of all key-value pairs inserted into the priority
      queue up to time $t$, and let $PQ^-_t$ be the set of all items
      removed up to time $t$. At each instant in time $t$, the set of items
      within the priority queue is then $PQ_t = PQ^+_t \setminus PQ^-_t$.
\item An insertion of key $k$ and value $v$ simply adds the pair to the set of
      inserted items.
\item A deletion at time $t$ finds the item $(k, v) \in PQ_t$ such that
      $\forall (x, y) \in PQ_t: f(x) \leq f(k)$ according to the priority function defined
      above.
      If such an item does not exist, i.e. the queue is empty, false is returned.
      Otherwise, $(k, v)$ is added to the set of deleted items, and $(\text{true}, v)$
      is returned to the caller.
\end{itemize}
Note that we do not define an order within items of identical keys, and thus the
priority queue is allowed to choose freely among items of equal priority.

In the following, we assume a min-priority queue and do not distinguish precisely
between keys and their associated priority function. For instance, we simply
use "the minimal item" to refer to the item within the queue of highest priority.

% TODO: Fix all quotes in entire thesis.

\section{Overview}

A naive implementation of a priority queue could be realized by using an cyclical array
sorted according to the priority function. Access to the minimal element is
possible in constant time since it is located at the head of the array. However,
insertions need to insert the new item into its correct position within the
array, and move all larger items back by one position, resulting in a worst-case
complexity of $O(n)$.

More efficient implementation techniques have been known for over a half century.
Binary heaps \cite{williams1964algorithm} were invented as part of a sorting
algorithm in \citedate{williams1964algorithm} and are still arguable the most
popular design for general purpose priority queues. For example, the priority queue
container class contained in the C++ \ac{STL} implements a binary heap on top of
any compatible backing structure. Both deletions and insertions have logarithmic
worst-case complexity.

While heaps use an implicit tree structure, it is also possible to use explicit
balanced \acp{BST} as priority queues with identical worst case bounds as heaps.
Popular \ac{BST} variants are e.g. Red-black trees \cite{bayer1972symmetric} and
AVL trees \cite{adelsonvelskii1963algorithm}.
Insertions use the standard \ac{BST} insert algorithm, while deletions remove
the tree's leftmost item. Alternatively, SkipLists \cite{pugh1990skip} also
provide logarithmic complexity but do not require periodic balancing operations
since they rely on randomization to preserve a beneficial data structure.

Fibonacci heaps \cite{fredman1987fibonacci} were invented by
\citeauthor{fredman1987fibonacci} and are based on a collection of
heap-sorted trees. In addition to the standard insertion and deletion operations
(in amortized logarithmic time),
they also support efficient \lstinline|merge| and \lstinline|decrease_key|
operations in constant time, thus decreasing the complexity of Dijkstra's shortest
path algorithm to $O(|E| + |V| \log |V|)$. However, due to the programming complexity
and involved constant factors, they are not widely used in practise.
Subsequent publications have proposed various alternatives to Fibonacci heaps
such as relaxed heaps \cite{driscoll1988relaxed} and strict Fibonacci heaps
\cite{brodal2012strict}.

Other well-known priority queue designs are the Skew Heap \cite{sleator1986self}
based on a heap-ordered binary tree and the \lstinline|meld| operation;
the Splay Tree \cite{sleator1985self}, a heuristically balanced \ac{BST}
which attempts to move frequently used sections towards the root of the tree;
pairing heaps \cite{fredman1986pairing}, a self-adjusting version of the
binomial heap \cite{vuillemin1978data}; and many more. For further
information, we refer the reader to reviews such as
\cite{ronngren1997comparative,jones1986empirical}.

\section{Binary Heaps}

Binary heaps  \cite{williams1964algorithm} are one of the most common (and also one of the simplest)
methods of implementing priority queues. They are based on two concepts; first,
that of balanced binary trees, composed of nodes with exactly two children each.
And second, that of heap ordering, the notion that the priority of each child
node is less than that of its parent node. Furthermore, instead of using an
explicit representation of a binary tree, heaps implicitly encode the position
of their contents.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class heap {
public:
    void insert(const K &key, const V &val);
    bool delete_min(V &val);

private:
    std::pair<K, V> items[CAPACITY];
    size_t n;
};
\end{lstlisting}
\caption{A binary heap class.}
\label{fig:heap_members}
\end{figure}

As shown in Figure \ref{fig:heap_members},
the backing data structure of a binary heap is a flat array \lstinline|items| together
with a variable tracking the number of items within the heap, \lstinline|n|. The root item,
i.e. the item with the highest priority, is located at the head of the array
\lstinline|items[1]|\footnote{For ease
of presentation, we use 1-based indexing within this section.}. For each item
at position \lstinline|items[i]|, its left child is located at \lstinline|items[2i]| and its right child
at \lstinline|items[2i + 1]|. Vice versa, the parent item of position \lstinline|items[i]| can be found
at \lstinline|items[i / 2]|.

Insertions initially append the new item at the tail of array \lstinline|items[n + 1]|.
Note that heap order might be violated at this point if the inserted item has a
higher priority than its parent. If that is the case, the item and its parent
are swapped. This operation is repeated until the entire heap again preserves
heap order. It may be repeated once for each level in the implicit tree, i.e.
at most $\lceil \log n \rceil$ times, and takes constant time in each iteration.

Deletions first remove the root item, located at \lstinline|items[1]|, and store
it as the eventual return value. The heap structure is then preserved by moving
the item at the tail of the array (\lstinline|items[n + 1]|) into the root position,
and then swapping it with its highest priority child until the item is again
in a position in which it is of higher priority than both children. Like insertions,
this operation takes constant effort in each iteration, and is repeated at most
once for each level, resulting in a worst case of $O(\log n)$.

Binary heaps are efficient in practice and conceptually simple as well as easy
to implement. They are space-efficient since they do not store any overhead per
item such as pointers (in \ac{BST}-based implementations). Their main drawbacks
become apparent only in a concurrent setting, in which their tendency to
modify large parts of the data structure during both insertions and deletions
causes contention, high cache-coherence protocol traffic, and synchronization
issues.

\section{SkipList}

As previously discussed, \acp{BST} are a viable approach to implementing
priority queues with logarithmic worst-case bounds for insertions as well as
deletions. SkipLists \cite{pugh1990skip} are very similar conceptually,
but use a different approach to physically represent the data structure,
and use randomization in order to avoid having to perform regular maintenance
operations while preserving expected logarithmic time bounds.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class skiplist {
public:
    void insert(const K &key, const V &val);
    bool delete_min(V &val);

private:
    skiplist_node<K, V> *head[MAX_HEIGHT];
};

template <class K, class V>
struct skiplist_node {
    K key;
    V val;

    size_t level;
    skiplist_node<K, V> *next[MAX_HEIGHT];
};
\end{lstlisting}
\caption{A SkipList priority queue class.}
\label{fig:skiplist_members}
\end{figure}

A SkipList is essentially a linked list of specialized nodes, each of which
contains a key-value pair, an associated node level, and a list of pointers to
the next node (Figure \ref{fig:skiplist_members}).
The node's level determines how many pointers to the next node
exist, and a pointer at level $l$ links to the next node of level $\geq l$
within the data structure. Higher level pointers thus act as shortcuts into the
list, avoiding the usual linear cost search operation of linked lists.

Insertions create a new node, assigning the level according to a geometric
distribution. The correct position for the new node is determined
by searching for the given key within the list.
Finally, the new node is physically
inserted by modifying pointers of previous nodes and the new node.

A search proceeds by starting
at the maximal level, and and repeatedly walking along its associated pointer list
until a node is found with a higher key than the desired one. The inspected level is
then decremented, and the operation is repeated (if $level > 0$), or the current
location returned as the correct position (otherwise).

\lstinline|delete_min| simply removes the head node of the list by setting its
successor at level 0 as the new head node, and then returns the previous head's value.

Note that since node levels are distributed geometrically, we expect each level $l$
to contain twice as many nodes as level $l + 1$. Thus both the expected maximal level
of all nodes, as well as the number of steps required to find any given key within
the SkipList is $O(\log n)$.

The SkipList has increasingly gained in popularity in recent research into
parallel priority queues, since it does not require the frequent rebalancing
operations of \acp{BST}, and operations on nodes of different keys usually access
spacially disjoint parts of the data structure. In the context of concurrent
operations, both of these properties are very desirable: if several concurrent
insertions modify disjoint parts of the SkipList, contention is reduced;
and since the SkipList does not require balancing, both insertions and deletions
are simpler and more efficient to implement.

Unfortunately, and as we will see in Chapter \ref{ch:evaluation}, SkipLists
have turned out to be rather slow in practice. This is most likely due to the fact
that each node is usually dynamically allocated, and located in non-contiguous
parts of memory, resulting in very bad cache locality.
