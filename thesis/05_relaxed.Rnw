\chapter{Relaxed Concurrent Priority Queues} \label{ch:relaxed}

The body of work discussed in previous sections
creates the impression that the outer limits of strict, deterministic priority queues have been reached.
In particular, \citeauthor{linden2013skiplist} conclude that scalability is solely limited by \lstinline|DeleteMin|,
and that less than one modified memory location per thread and operation would have to be read
in order to achieve improved performance \cite{linden2013skiplist}.

Recently, relaxation of provided guarantees have been investigated as another method of reducing
contention and improving disjoint-access parallelism.
For instance, k-FIFO queues \cite{kirsch2012fast} have achieved considerable
speed-ups compared to strict FIFO queues by allowing {\lstset{breaklines,breakatwhitespace} \lstinline|Dequeue|} to return elements
up to $k$ positions out of order.

Relaxation has also been applied to concurrent priority queues with some success, and in the following
sections we discuss two such approaches.

\section{SprayList} \label{sec:spraylist}

The SprayList is another recent approach towards a relaxed priority queue by \citeauthor{alistarhspraylist}
\cite{alistarhspraylist}. Instead of the distributed approach described in the previous section,
the SprayList is based on a central data structure, and uses a random walk in \lstinline|DeleteMin|
in order to spread accesses over the $O(P \log^3 P)$ first elements with high probability, where $P$
is again the number of participating threads.

\citeauthor{fraser2004practical}'s lock-free SkipList \cite{fraser2004practical} again serves as the
basis for the priority queue implementation. The full source code is available online\footnote{
\url{https://github.com/jkopinsky/SprayList}}. In the SprayList, \lstinline|Insert| calls are simply
forwarded to the underlying SkipList.

The \lstinline|DeleteMin| operation however executes a random walk, also called a \emph{spray}, the
purpose of which is to spread accesses over a certain section of the SkipList uniformly such that
collisions between multiple concurrent \lstinline|DeleteMin| calls are unlikely. This is achieved by
starting at some initial height, walking a randomized number of steps, descending a randomized number of levels,
and repeating this procedure until a node $n$ is reached on the lowest level. If $n$ is not deleted,
it is logically deleted and returned. Otherwise, a \emph{spray} is either reattempted, or the thread
becomes a cleaner, traversing the lowest level of the SkipList and physically removing logically deleted
nodes it comes across. A number of dummy nodes are added to the beginning of the list in order to counteract
the algorithm's bias against initial items.

The \emph{spray} parameters are chosen such that with high probability, one of the $O(P \log^3 P)$
first elements is returned, and that each of these elements is chosen roughly uniformly at random.
The final effect is that accesses to the data structure are spread out, reducing contention and resulting
in a noticeably lower number of \ac{CAS} failures in comparison to strict priority queues described
in Section \ref{sec:lockfree}.

The authors do not provide any statement as to the linearizability (or other concurrent correctness
criteria) of the SprayList, and it is not completely clear how to define it since no
sequential semantics are given.

Benchmarks show promising results: the SprayList scales well at least up to 80 threads,
and performs close (within a constant factor) to an implementation using a random remove instead
of \lstinline|DeleteMin|, which the authors consider as the performance ideal.

\section{Multiqueue} \label{sec:multiq}

% TODO.

\section{$k$-LSM} \label{sec:wimmer}

\begin{comment}
We need to more or less rewrite this entire section since the technical description is unclear and 
outdated. Structure could be as follows:

* LSM - what is it?
* Dist LSM.
* Shared LSM.
* K LSM.

Detail should be such that it is clear how the PQ works, but not too low level since that is what
the next section will be about. We should briefly mention memory management but not go into too much
detail.
\end{comment}

We now finally come to the main topic of this thesis, the relaxed, linearizable, and lock-free
$k$-LSM priority queue.
\citeauthor{wimmer2013data} first presented this data structure in \citedate{wimmer2013data}
\cite{wimmer2013data} and have improved on it continuously since, with the most recent results
being published in \cite{wimmer2015lock}. The original $k$-LSM queue is integrated as a priority scheduler
into their \emph{Pheet} task-scheduling system, and an open-source implementation is
available at \url{http://pheet.org}. In this section we describe its design in detail, while
Chapter \ref{ch:implementation} covers our implementation and improvements.

Taken as a black box, the $k$-LSM conforms to the interface as shown in Figure \ref{fig:klsm_interface}.
It is a template data structure in the sense that it has a configuration parameter $k$, which
determines how far quality guarantees may be relaxed. Given $k$, the \lstinline|delete_min| operation
$(k + 1) \cdot P$ minimal items, where $P$ is the number of threads.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class k_lsm {
public:
    void insert(const K &key, const V &value);
    bool delete_min(V &val);
};
\end{lstlisting}
\caption{The $k$-LSM interface.}
\label{fig:klsm_interface}
\end{figure}

The semantics are as follows: when \lstinline|insert| completes, the given key-value pair has been
inserted into the queue. \lstinline|delete_min| can either complete successfully, in which case
it returns \lstinline|true|, one of the $(k + 1) \cdot P$ minimal items is removed from the queue
and its value is written into the given argument; or it can fail, in which case no item is removed
from the queue and the \lstinline|false| is returned. Failures of \lstinline|delete_min| may occur
spuriously even if the queue is non-empty, but the number of such failures are low in practice.

The \klsm is a composite data structure consisting of a global component called the \ac{SLSM},
and a thread-local component called the \ac{DLSM}. As implied by their names, both the \ac{SLSM}
and \ac{DLSM} are based on the \ac{LSM} \cite{o1996log} data structure which will be elaborated
upon below. Both structures may be used as standalone priority queues, but have complementing
advantages and disadvantages which motivates their composition.

\subsection{\acl{LSM}} \label{ssec:lsm}

\aclp{LSM} were introduced to the database community in \citedate{o1996log} \cite{o1996log}
and reinvented indepently by \citeauthor{wimmer2013data} based on the requirements of concurrent
priority queues. \acp{LSM} are one of the basic building blocks of the \ac{SLSM} and \ac{DLSM}
and hence also the \klsm.

By itself, the
\ac{LSM} is not a parallel data structure, and special care must be taken when accessing shared
(i.e. not exclusively owned, thread-local) \acp{LSM}. For our purposes,
we imagine it to have an interface as shown in Figure \ref{fig:lsm_interface}. \lstinline|insert|
simply inserts the given item into the set, while \lstinline|peek_min| returns the
minimal item and \lstinline|true|, or \lstinline|false| if the set is empty.

\begin{figure}[ht]
\begin{lstlisting}
template <class Item>
class lsm {
public:
    void insert(const Item &item);
    bool peek_min(Item &item);
};
\end{lstlisting}
\caption{The \ac{LSM} interface.}
\label{fig:lsm_interface}
\end{figure}

In more detail, the \ac{LSM} consists of an ordered set of sorted arrays fulfilling a set of
invariants after the completion of each operation:

\begin{itemize}
\item Each block $B$ is associated with a level $i$ such that $B$ has a capacity of $2^i$.
\item If block $B$ (with associated level $i$) is located before block $B'$ (with level $i'$),
      then $i < i'$.
\item For each block $B$ with level $i$, $|B| \geq 2^{i-1}$ where $|B|$ is the number of items
      stored in $B$.
\end{itemize}

Insertions initially create a singleton block $B$ containing the given item. $B$ is then inserted
into the \ac{LSM}, possibly violating invariants temporarily. While invariants remain unsatisfied,
i.e. while the \ac{LSM} contains multiple blocks of the same level, $B$ is then merged with its
successor block.

Peek operations iterate through each block in the \ac{LSM} and return the minimal encountered item.
Note that this operation is $O(1)$ for each block since the minimal item is
located at the head of head array.

Both insertions and deletions are of amortized complexity $O(\log n)$ and are usually extremely
cache-efficient since items are mostly accessed with high locality (e.g. during merge operations).

\subsection{\acl{DLSM}} \label{ssec:dlsm}

The \ac{DLSM} is a priority queue with purely thread-local guarantees. It adheres to the same
interface as the \klsm as given in Figure \ref{fig:klsm_interface}, but with subtly altered semantics
in that there is no relaxation parameter $k$ --- the \ac{DLSM}'s \lstinline|delete_min| simply
removes the thread-locally minimal item. If the local queue is empty an operation called \lstinline|spy|
is performed, which attempts to copy all items from a randomly chosen thread's queue.

The implementation is essentially a thin wrapper on top of a thread-local \ac{LSM} with some complicating
factors. Items store both the key-value pair as well as an atomic flag indicating whether an item
has been taken (removed) from the \ac{DLSM} or not. The \ac{LSM} itself stores pointers to these items.
\lstinline|delete_min| operations perform
a \lstinline|peek_min| on the underlying local \ac{LSM} and mark the returned item as taken using
an atomic \ac{CAS} instruction. Special care is taken in order to eventually physically remove taken items
from the \ac{LSM}: merges skip taken items, and whenever a block is noted to be less than half-full
the block is shrunk and \ac{LSM} invariants are reasserted.

So far, all operations have been exclusively thread-local, and thus no attention had to be given
to concurrency complications. Unfortunately, \lstinline|spy| does access other thread's \acp{LSM}
and we cannot ignore these issues entirely. The set of blocks in the \ac{LSM} is modelled
as a doubly-linked list of blocks. The set of pointers to the previous element may be accessed
exclusively by the owning thread, while the set of pointers to the next element can by read by
all threads (but written only by the owning thread). When a maintenance operation triggers
changes to the block set, merges are performed locally and the new tail of the linked list is
spliced in atomically using a \ac{CAS} instruction. While other threads currently reading the local list
may encounter pointers to the same item multiple items within a single \lstinline|spy| operation if the linked
list has been updated concurrently, this is not an issue since the item may only be taken successfully
by a single thread when its flag is set atomically.

The \ac{DLSM} is essentially embarassingly parallel as long as all local queues remain nonempty,
and scales exceedingly well even at very high thread counts. This is to be expected, since the \ac{LSM}
is very efficient, and most, if not all operations are thread-local. However, its major weakness
is the lack of global quality guarantees. The next section discusses the \ac{SLSM}, which is utilized
in the \klsm in order to amend this point.

\subsection{\acl{SLSM}} \label{ssec:slsm}

\begin{comment}
Their paper presents several variations on the common theme of priority queues: a distributed work-stealing
queue which can give no guarantees as to global ordering since it consists of separate priority queues
at each thread; a relaxed centralized priority queue in which no more than $k$ items are missed
by any processor; and a relaxed hybrid data structure which combines both ideas and provides
a guarantee that no thread misses more than $kP$ items where $P$ is the number of participating threads.
In this section, we examine only the hybrid variant since it provides both the scalability of work-stealing
queues and the ordering guarantees of the centralized priority queue.

\begin{figure}[ht]
\begin{lstlisting}
struct globals_t {
  list_of_item_t global_list;
};

/* Thread-local items. */
struct locals_t {
  list_of_item_t local_list;
  pq_t prio_queue;
  size_t remaining_k;
};
\end{lstlisting}
\caption{\citeauthor{wimmer2013data} structure.}
\label{fig:wimmerq}
\end{figure}

The hybrid queue consists of a list of globally visible items and one local item list as well as a local
sequential priority queue per thread. The thread-local counter \lstinline|remaining_k| tracks how many more
items may be added to the local queue until all local items must be made globally visible to avoid
breaking guarantees.

Whenever an item is added, it is first added locally to both \lstinline|local_list| and \lstinline|prio_queue|
and \lstinline|remaining_k| is decremented. If \lstinline|remaining_k| reaches zero, then the local
item list is appended to the global list, and all not yet seen (by this thread) items of the global
queue are added to the local priority queue.

\lstinline|DeleteMin| simply pops the local priority queue as long is it is non-empty and the popped
item has already been deleted. When a non-deleted item is popped, it is atomically marked as deleted
and returned to the caller. If instead we are faced with an empty local queue, we attempt to spy,
i.e. copy items from another thread's local list.

Both \lstinline|Insert| and \lstinline|DeleteMin| periodically synchronize with the global list
by adding all items that have not yet been seen locally to \lstinline|prio_queue|.
Memory allocations are handled using the wait-free memory manager by \citeauthor{wimmer2013wait} \cite{wimmer2013wait}.

The \citeauthor{wimmer2013data} priority queue was evaluated using a label-correcting variant
of Dijkstra's shortest path algorithms. Their model creates a task for each node
expansion, and therefore comes with a considerable task scheduling overhead. Nonetheless,
the parallel implementation scales well up to 10 threads, and further limited performance
gains are made until 40 threads. To date, no direct comparisons to other concurrent
priority queues have been possible since the data structure is strongly tied to \emph{Pheet},
but a separate implementation of k-priority queues is planned.
\end{comment}
