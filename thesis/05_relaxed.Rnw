\chapter{Relaxed Concurrent Priority Queues} \label{ch:relaxed}

The body of work discussed in previous sections
creates the impression that the upper limits of strict priority queues have been reached.
In particular, \citeauthor{linden2013skiplist} conclude that scalability is solely limited by \lstinline|delete_min|,
and that less than one modified memory location per thread and operation would have to be read
in order to achieve improved performance \cite{linden2013skiplist} through SkipLists. 
Through a novel concept, the CBPQ \cite{cbpq} manages to improve on the Linden queue
by up to a factor of two, but only scales while executed on a single processor.

Recently, relaxation of provided guarantees have been investigated as another method of reducing
contention and improving disjoint-access parallelism.
For instance, k-FIFO queues \cite{kirsch2012fast} have achieved considerable
speed-ups compared to strict FIFO queues by allowing {\lstset{breaklines,breakatwhitespace} \lstinline|Dequeue|} to return elements
up to $k$ positions out of order.

Relaxation has also been applied to concurrent priority queues with some success, and in the following
sections we discuss three such approaches.

\section{SprayList} \label{sec:spraylist}

The SprayList is a recent relaxed priority queue design by \citeauthor{alistarhspraylist}
\cite{alistarhspraylist}. Instead of taking a distributed approach,
the SprayList is based on a central data structure, and uses a random walk in \lstinline|delete_min|
in order to spread accesses over the $O(P \log^3 P)$ first elements with high probability, where $P$
is the number of participating threads.

\citeauthor{fraser2004practical}'s lock-free SkipList \cite{fraser2004practical} again serves as the
basis for the priority queue implementation. The full source code is available online at 
\url{https://github.com/jkopinsky/SprayList} (last visited on December 9\textsuperscript{th}, 2015).
In the SprayList, \lstinline|insert| calls are simply
forwarded to the underlying SkipList.

The \lstinline|delete_min| operation however executes a random walk, also called a \emph{spray}, the
purpose of which is to spread accesses over a certain section of the SkipList uniformly such that
collisions between multiple concurrent \lstinline|delete_min| calls become unlikely. This is achieved by
starting at some initial level, walking a randomized number of steps, descending a randomized number of levels,
and repeating this procedure until a node $n$ is reached on the lowest level. If $n$ is not deleted,
it is logically deleted and returned. Otherwise, a \emph{spray} is either reattempted, or the thread
becomes a cleaner, traversing the lowest level of the SkipList and physically removing logically deleted
nodes it comes across. A number of dummy nodes are added to the beginning of the list in order to counteract
the algorithm's bias against initial items.

The \emph{spray} parameters are chosen such that with high probability, one of the $O(P \log^3 P)$
first elements is returned, and that each of these elements is chosen roughly uniformly at random.
The final effect is that accesses to the data structure are spread out, reducing contention and resulting
in a noticeably lower number of \ac{CAS} failures in comparison to strict priority queues described
in Chapter \ref{ch:strict}.

The authors do not provide any statement as to the linearizability (or other concurrent correctness
criteria) of the SprayList, and it is not completely clear how to define it since no
sequential semantics are given.

Their benchmarks show promising results: the SprayList scales well at least up to 80 threads,
and performs close (within a constant factor) to an implementation using a random remove instead
of \lstinline|delete_min|, which the authors consider as the performance ideal since contention
is minimized (although it might be
more fair to narrow that statement down to SkipList-based structures).

\section{Multiqueue} \label{sec:multiq}

Multiqueues, published in \citeyear{rihani2014multiqueues} by \citeauthor{rihani2014multiqueues} \cite{rihani2014multiqueues}, 
are a simple and elegant relaxed 
priority queues design using probabilistic techniques. The published design uses
a lock-based approach in combination with any sequential priority queue to construct
a scalable concurrent priority queue, but unfortunately without being able to offer
any defined quality guarantees (probabilistic or otherwise). In their benchmarks,
Multiqueues outperform the SprayList in both throughput (by around a factor of two)
and quality.

Similar to the \klsm (see Section \ref{sec:wimmer} and Chapter \ref{ch:implementation}),
the Multiqueue is a configurable data structure. It takes a parameter $c$, which
controls the number of created sequential priority queues. Each thread creates
$c$ queues, adding up to $cP$ queues in total.

In addition, each sequential queue is associated with its own lock and a cached
copy of its smallest key. Insertions
choose a random queue $q_i, i \in [0, cP[$, obtain the associated lock, and
insert the given item, updating the cached smallest key if necessary.

Deletions choose \emph{two} queues $q_i, q_j$ at random and peek at their respective cached
minimal keys. The queue with the smaller cached key is then locked and popped,
again updating the cached key. This technique is somewhat similar load balancing
through random selection \cite{richa2001power}, but no definite quality analysis
has yet been given due to the complications arising through priority queue semantics.

While the Multiqueue is inherently lock-based, it could be made lock-free
by simply using lock-free queues as its backing structures.

% TODO: It might be nice to have one long-form example within each chapter
% (sequential, strict, relaxed). In this chapter, the multiq seems to be a great
% fit since it's compact and easy to understand.

\section{$k$-LSM} \label{sec:wimmer}

\begin{comment}
We need to more or less rewrite this entire section since the technical description is unclear and 
outdated. Structure could be as follows:

* LSM - what is it?
* Dist LSM.
* Shared LSM.
* K LSM.

Detail should be such that it is clear how the PQ works, but not too low level since that is what
the next section will be about. We should briefly mention memory management but not go into too much
detail.
\end{comment}

We now finally come to the main topic of this thesis, the relaxed, linearizable, and lock-free
$k$-LSM priority queue.
\citeauthor{wimmerphd} first presented this data structure in \citedate{wimmer2013data}
\cite{wimmer2013data} and have improved on it continuously since, with the most recent results
being published in \cite{wimmer2015lock}. The original $k$-LSM queue is integrated as a priority scheduler
into \citeauthor{wimmerphd}'s \emph{Pheet} task-scheduling system, and an open-source implementation is
available at \url{http://pheet.org}. In this section we describe its design in detail, while
Chapter \ref{ch:implementation} covers our implementation and improvements.

Taken as a black box, the $k$-LSM conforms to the interface as shown in Figure \ref{fig:klsm_interface}.
It is a template data structure in the sense that it has a configuration parameter $k$, which
determines how far quality guarantees may be relaxed. Given $k$, the \lstinline|delete_min| operation
may return one of the $kP$ minimal items, where $P$ is the number of threads.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class k_lsm {
public:
    void insert(const K &key, const V &value);
    bool delete_min(V &val);
};
\end{lstlisting}
\caption{The $k$-LSM interface.}
\label{fig:klsm_interface}
\end{figure}

The semantics are as follows: when \lstinline|insert| completes, the given key-value pair has been
inserted into the queue. \lstinline|delete_min| can either complete successfully, in which case
it returns \lstinline|true|, one of the $kP$ minimal items is removed from the queue
and its value is written into the given argument; or it can fail, in which case no item is removed
from the queue and the \lstinline|false| is returned. Failures of \lstinline|delete_min| may occur
spuriously even if the queue is non-empty, but the number of such failures are low in practice.

The \klsm is a composite data structure consisting of a global component called the \ac{SLSM},
and a thread-local component called the \ac{DLSM}. As implied by their names, both the \ac{SLSM}
and \ac{DLSM} are based on the \ac{LSM} \cite{o1996log} data structure which will be elaborated
upon below. Both structures may be used as standalone priority queues, but have complementing
advantages and disadvantages which motivates their composition.

Throughout this section, we ignore over the complexities of lock-free memory management and simply
assume the existence of a garbage collector. The implementation by \citeauthor{wimmerphd}
uses their own lock-free memory allocator to handle items, while blocks and block arrays are
allocated in a pool and reused. Memory management will be discussed in further detail in Chapter
\ref{ch:implementation}.

\subsection{\aclp{LSM}} \label{ssec:lsm}

\aclp{LSM} were introduced to the database community in \citedate{o1996log} \cite{o1996log}
and reinvented indepently by \citeauthor{wimmerphd} based on the requirements of concurrent
priority queues. \acp{LSM} are the basic building blocks of the \ac{SLSM} and \ac{DLSM}
and hence also the \klsm.

By itself, the
\ac{LSM} is not a parallel data structure, and special care must be taken when accessing shared
(i.e. not exclusively owned, thread-local) \acp{LSM}. For our purposes,
we imagine it to have an interface as shown in Figure \ref{fig:lsm_interface}. \lstinline|insert|
simply inserts the given item into the set, while \lstinline|peek_min| returns the
minimal item and \lstinline|true|, or \lstinline|false| if the set is empty.

\begin{figure}[ht]
\begin{lstlisting}
template <class Item>
class lsm {
public:
    void insert(const Item &item);
    bool peek_min(Item &item);
};
\end{lstlisting}
\caption{The \ac{LSM} interface.}
\label{fig:lsm_interface}
\end{figure}

In more detail, the \ac{LSM} consists of an ordered set of sorted arrays (called blocks) fulfilling a set of
invariants after the completion of each operation:

\begin{itemize}
\item Each block $B$ is associated with a level $i$ such that $B$ has a capacity of $2^i$ items.
\item Consider block $B$ (with associated level $i$ and position within the LSM $ix$)
      and block $B'$ (with level $i'$ and position $ix'$); if $ix < ix$
      then $i > i'$.
\item For each block $B$ with level $i$, $2^{i-1} \leq |B| \leq 2^i$ where $|B|$ is the number of items
      stored in $B$.
\end{itemize}

Insertions initially create a singleton block $B$ containing the given item. $B$ is then inserted
at the tail end of the \ac{LSM}, possibly violating invariants temporarily. While invariants remain unsatisfied,
i.e. while the \ac{LSM} contains multiple blocks of the same level, $B$ is merged with its
predecessor block, replacing both $B$ and the successor block with a new block of doubled capacity.
Figure \ref{fig:lsm_insertion} displays an insertion example in which the new element with key $7$
is inserted, triggering two block merges before completion.

% TODO: Ensure we have the correct logic (head block is largest) everywhere.

Peek operations iterate through each block in the \ac{LSM} and return the minimal encountered item.
Note that this operation is $O(1)$ for each block since the minimal item is
located at the head of each block, and thus of logarithmic complexity for the entire \ac{LSM}.

\begin{figure}
\begin{minipage}[t]{.33\textwidth}
\centering
    \begin{tikzpicture}[start chain,
        ->,
        every node/.style={font = \small},
        label/.style={rectangle,minimum size = 5mm}]

        \begin{scope}[start chain = 1 going right, node distance = -0.15mm, every node/.style = box]
            \node [on chain=1, minimum width = 10mm] (ba0) {$b_0$};
            \node [on chain=1, minimum width = 10mm] (ba1) {$b_1$};
            \node [on chain=1, minimum width = 10mm] (ba2) {$b_2$};
            \node [on chain=1, minimum width = 10mm] (ba3) {$b_3$};
        \end{scope}

        \begin{scope}[start chain = 2 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm]
            \node[on chain, box, above = of ba0, yshift = 3mm] (00) {0};
            \node[on chain, box] {3};
            \node[on chain, box] {4};
            \node[on chain, box] (03) {4};
            \node[on chain, box] {6};
            \node[on chain, box] {9};
            \node[on chain, box] {};
            \node[on chain, box] (07) {};
        \end{scope}

        \begin{scope}[start chain = 4 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm, every node/.style = box]
            \node[on chain, box, above = of ba1, yshift = 3mm] (10) {5};
            \node[on chain, box] {8};
        \end{scope}

        \begin{scope}[start chain = 5 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm, every node/.style = box]
            \node[on chain, box, above = of ba2, yshift = 3mm] (20) {2};
        \end{scope}

        \begin{scope}[start chain = 6 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm, every node/.style = box]
            \node[on chain, box, above = of ba3, yshift = 3mm] (30) {7};
        \end{scope}

        \path[->, every node/.style= { font = \sffamily\small }]
        (ba0) edge node [right] {} (00)
        (ba1) edge node [right] {} (10)
        (ba2) edge node [right] {} (20)
        (ba3) edge node [right] {} (30);
    \end{tikzpicture}
\subcaption{The initial state with item 7 inserted as a new block.}
\end{minipage}%
\begin{minipage}[t]{.33\textwidth}
\centering
    \begin{tikzpicture}[start chain,
        ->,
        every node/.style={font = \small},
        label/.style={rectangle,minimum size = 5mm}]

        \begin{scope}[start chain = 1 going right, node distance = -0.15mm, every node/.style = box]
            \node [on chain=1, minimum width = 10mm] (ba0) {$b_0$};
            \node [on chain=1, minimum width = 10mm] (ba1) {$b_1$};
            \node [on chain=1, minimum width = 10mm] (ba2) {$b_2$};
            \node [on chain=1, minimum width = 10mm] (ba3) {$b_3$};
        \end{scope}

        \begin{scope}[start chain = 2 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm]
            \node[on chain, box, above = of ba0, yshift = 3mm] (00) {0};
            \node[on chain, box] {3};
            \node[on chain, box] {4};
            \node[on chain, box] (03) {4};
            \node[on chain, box] {6};
            \node[on chain, box] {9};
            \node[on chain, box] {};
            \node[on chain, box] (07) {};
        \end{scope}

        \begin{scope}[start chain = 4 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm, every node/.style = box]
            \node[on chain, box, above = of ba1, yshift = 3mm] (10) {5};
            \node[on chain, box] {8};
        \end{scope}

        \begin{scope}[start chain = 5 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm, every node/.style = box]
            \node[on chain, box, above = of ba2, yshift = 3mm] (20) {2};
            \node[on chain, box] {7};
        \end{scope}

        \path[->, every node/.style= { font = \sffamily\small }]
        (ba0) edge node [right] {} (00)
        (ba1) edge node [right] {} (10)
        (ba2) edge node [right] {} (20);
    \end{tikzpicture}
\subcaption{After the first merge.}
\end{minipage}%
\begin{minipage}[t]{.33\textwidth}
\centering
    \begin{tikzpicture}[start chain,
        ->,
        every node/.style={font = \small},
        label/.style={rectangle,minimum size = 5mm}]

        \begin{scope}[start chain = 1 going right, node distance = -0.15mm, every node/.style = box]
            \node [on chain=1, minimum width = 10mm] (ba0) {$b_0$};
            \node [on chain=1, minimum width = 10mm] (ba1) {$b_1$};
            \node [on chain=1, minimum width = 10mm] (ba2) {$b_2$};
            \node [on chain=1, minimum width = 10mm] (ba3) {$b_3$};
        \end{scope}

        \begin{scope}[start chain = 2 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm]
            \node[on chain, box, above = of ba0, yshift = 3mm] (00) {0};
            \node[on chain, box] {3};
            \node[on chain, box] {4};
            \node[on chain, box] (03) {4};
            \node[on chain, box] {6};
            \node[on chain, box] {9};
            \node[on chain, box] {};
            \node[on chain, box] (07) {};
        \end{scope}

        \begin{scope}[start chain = 4 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm, every node/.style = box]
            \node[on chain, box, above = of ba1, yshift = 3mm] (10) {2};
            \node[on chain, box] {5};
            \node[on chain, box] {7};
            \node[on chain, box] {8};
        \end{scope}

        \path[->, every node/.style= { font = \sffamily\small }]
        (ba0) edge node [right] {} (00)
        (ba1) edge node [right] {} (10);
    \end{tikzpicture}
\subcaption{After the second merge.}
\end{minipage}
\caption[Insertion of a new element into the \ac{LSM}.]
        {Insertion of a new element into the \ac{LSM}. Note that while invariants
         are violated during the operation (since block capacities are not unique),
         they are reinstated after the final merge.}
\label{fig:lsm_insertion}
\end{figure}

Both insertions and deletions are of amortized complexity $O(\log n)$ and are usually extremely
cache-efficient since the central merge operation accesses items in sequence, and items
are stored in contiguous chunks of memory.

\subsection{\acl{DLSM}} \label{ssec:dlsm}

The \ac{DLSM} is a priority queue with purely thread-local guarantees. It adheres to the same
interface as the \klsm as given in Figure \ref{fig:klsm_interface}, but with subtly altered semantics
in that there is no relaxation parameter $k$ --- the \ac{DLSM}'s \lstinline|delete_min| simply
removes the thread-locally minimal item. If the local queue is empty an operation called \lstinline|spy|
is performed, which attempts to copy items from a randomly chosen thread's queue.

The implementation is essentially a thin wrapper on top of a thread-local \ac{LSM} with some complicating
factors. Items store both the key-value pair as well as an atomic flag indicating whether an item
has been taken (removed) from the \ac{DLSM} or not. The \ac{LSM} itself stores pointers to these items.
\lstinline|delete_min| operations perform
a \lstinline|peek_min| on the underlying local \ac{LSM} and mark the returned item as taken using
an atomic \ac{CAS} instruction. Special care is taken in order to eventually physically remove taken items
from the \ac{LSM}: merges skip taken items, and whenever a block is noted to be less than half-full
the block is shrunk and \ac{LSM} invariants are reasserted.

% TODO: Revise following paragraph - explanations are unclear and I'm not sure whether
% it is even possible to encounter an item multiple times.

So far, all operations have been exclusively thread-local, and thus no attention had to be given
to concurrency complications. Unfortunately, \lstinline|spy| does access other thread's \acp{LSM}
and we cannot ignore these issues entirely. The set of blocks in the \ac{LSM} is implemented
as a doubly-linked list of blocks. The set of pointers to the previous element may be accessed
exclusively by the owning thread, while the set of pointers to the next element can by read by
all threads (but written only by the owning thread). When a maintenance operation triggers
changes to the block set, merges are performed locally and the new tail of the linked list is
spliced in atomically using a \ac{CAS} instruction. While other threads currently reading the local list
may encounter pointers to the same item multiple items within a single \lstinline|spy| operation if the linked
list has been updated concurrently, this is not an issue since the item may only be taken successfully
by a single thread when its flag is set atomically.

The \ac{DLSM} is essentially embarassingly parallel as long as all local queues remain nonempty,
and scales exceedingly well even at very high thread counts. This is to be expected, since the \ac{LSM}
is very efficient, and most, if not all operations are thread-local. However, its major weakness
is the lack of global quality guarantees. The next section discusses the \ac{SLSM}, which is utilized
in the \klsm in order to reintroduce global guarantees.

Insertions and deletions are again of amortized logarithmic complexity and benefit
from high cache locality as well as mostly thread-local operations.

\subsection{\acl{SLSM}} \label{ssec:slsm}

The \acl{SLSM} could be considered the antipole of the previous section's
\ac{DLSM}: it consists of a single, global \ac{LSM} whereas the \ac{DLSM} has a local \ac{LSM}
for each thread; it is relaxed, returning one of the $k$ minimal items from \lstinline|delete_min|
whereas the \ac{DLSM} is strict on a local basis; and unfortunately,
it has fairly limited scalability while the \ac{DLSM} is embarassingly parallel.

The \ac{SLSM}'s interface is once again identical to Figure \ref{fig:klsm_interface}.
Insertion semantics are as expected, and deletions may either succeed, removing and returning one of the $k$
minimal items within the queue; or fail without altering the queue's item set.

At a high level, the concept of its implementation is simple: it consists of a single global 
\ac{LSM} with an associated item range containing the (approximately) $k$ minimal 
items within the queue. Insertions first add the new item to a local copy of the
queue and then atomically replace the global copy with the modified local version.
Deletions randomly select one of the $k + 1$ minimal items and attempt to delete it.
If it has not yet been taken by another thread, it is removed and the call returns
successfully. Otherwise, the same procedure is repeated (possibly updating the range of minimal items)
until the queue is either empty or an untaken item is found and returned. 

From the previous description, it almost seems as if the \ac{SLSM} were a simple
data structure; however, rest assured that this is not the case. Chapter \ref{ch:implementation}
covers its implementation in more detail.

\subsection{\klsm} \label{ssec:klsm}

Taken by themselves, both the \ac{DLSM} and \ac{SLSM} are interesting but not
particularly practical data structures. The \ac{DLSM} is fast and scalable, but
cannot offer any guarantees as to the quality of returned items, while the \ac{SLSM}
has a significant global bottleneck.

\begin{figure}[ht]
\begin{tikzpicture}[mindmap,
  level 1 concept/.append style={level distance = 130, sibling angle = 60},
  level 2 concept/.append style={sibling angle = 45},
  extra concept/.append style={color=blue!50, text=black}]

  \begin{scope}[mindmap, concept color = black!10]
    \node [concept] {\klsm}[clockwise from=-60]
      child {node [concept] (dlsm) {\ac{DLSM}}
        [clockwise from = 45]
        child {node [concept] {Thread-local}}
        child {node [concept] {Distributed}}
        child {node [concept] {Efficient}}
        child {node [concept] {Scalable}}
        child {node [concept] {Strict}}}
      child {node [concept] (slsm) {\ac{SLSM}}
        [clockwise from = -90]
        child {node [concept] {Shared}}
        child {node [concept] {Global}}
        child {node [concept] {Relaxed}}
        child {node [concept] {Global Bottleneck}}};
  \end{scope}
\end{tikzpicture}
\caption{A high-level view of the \klsm concept. }
\end{figure}

The \klsm combines both of these designs to reduce these drawbacks. In order
to be able to offer global quality guarantees, the thread-local \acp{DLSM} are
limited to a capacity of $k$ per thread, and \lstinline|delete_min| returns the minimal
item returned from the \ac{DLSM} and \ac{SLSM}. Note that we skip up to $k (P - 1)$
potentially smaller items located within other thread's local \ac{DLSM} and up to
$k$ items through relaxation of the \ac{SLSM}. We therefore satisfy
the claimed guarantees of returning one of the $kP$ minimal items.

The remaining issue of the \ac{SLSM}'s scalability is ultimately caused by updates
to the global \ac{LSM}, which in turn mostly occur through item insertions.
The \klsm reduces the occurrence of these insertions by a factor of $O(k)$ by
only inserting blocks of size $> \frac{k}{2}$ into the \ac{SLSM}. This harmonizes
well with the quality guarantee solution from the previous paragraph - when
a local \ac{DLSM} exceeds its maximal size, the largest block is simply inserted
into the \ac{SLSM}.

The \klsm as presented in \cite{wimmer2015lock} exhibits varying behavior according
to the parameter $k$; at low values ($k \in \{0, 4\}$) scalability and throughput are somewhat comparable
the the \citeauthor{linden2013skiplist} queue. At higher relaxation values of $k \in \{256, 4096\}$,
the \klsm approaches linear speedup up to high thread counts.
