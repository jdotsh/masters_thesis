\chapter{Relaxed Concurrent Priority Queues} \label{ch:relaxed}

The body of work discussed in previous sections
creates the impression that the outer limits of strict, deterministic priority queues have been reached.
In particular, \citeauthor{linden2013skiplist} conclude that scalability is solely limited by \lstinline|DeleteMin|,
and that less than one modified memory location per thread and operation would have to be read
in order to achieve improved performance \cite{linden2013skiplist} through SkipLists. 
The CBPQ \cite{cbpq} manages to improve on the Linden queue using a novel concept,
but only up to a factor of two, and only while on one socket.

Recently, relaxation of provided guarantees have been investigated as another method of reducing
contention and improving disjoint-access parallelism.
For instance, k-FIFO queues \cite{kirsch2012fast} have achieved considerable
speed-ups compared to strict FIFO queues by allowing {\lstset{breaklines,breakatwhitespace} \lstinline|Dequeue|} to return elements
up to $k$ positions out of order.

Relaxation has also been applied to concurrent priority queues with some success, and in the following
sections we discuss three such approaches.

\section{SprayList} \label{sec:spraylist}

The SprayList is a recent approach towards a relaxed priority queue by \citeauthor{alistarhspraylist}
\cite{alistarhspraylist}. Instead of taking a distributed approach,
the SprayList is based on a central data structure, and uses a random walk in \lstinline|DeleteMin|
in order to spread accesses over the $O(P \log^3 P)$ first elements with high probability, where $P$
is the number of participating threads.

\citeauthor{fraser2004practical}'s lock-free SkipList \cite{fraser2004practical} again serves as the
basis for the priority queue implementation. The full source code is available online at 
\url{https://github.com/jkopinsky/SprayList}. In the SprayList, \lstinline|Insert| calls are simply
forwarded to the underlying SkipList.

The \lstinline|DeleteMin| operation however executes a random walk, also called a \emph{spray}, the
purpose of which is to spread accesses over a certain section of the SkipList uniformly such that
collisions between multiple concurrent \lstinline|DeleteMin| calls are unlikely. This is achieved by
starting at some initial height, walking a randomized number of steps, descending a randomized number of levels,
and repeating this procedure until a node $n$ is reached on the lowest level. If $n$ is not deleted,
it is logically deleted and returned. Otherwise, a \emph{spray} is either reattempted, or the thread
becomes a cleaner, traversing the lowest level of the SkipList and physically removing logically deleted
nodes it comes across. A number of dummy nodes are added to the beginning of the list in order to counteract
the algorithm's bias against initial items.

The \emph{spray} parameters are chosen such that with high probability, one of the $O(P \log^3 P)$
first elements is returned, and that each of these elements is chosen roughly uniformly at random.
The final effect is that accesses to the data structure are spread out, reducing contention and resulting
in a noticeably lower number of \ac{CAS} failures in comparison to strict priority queues described
in Chapter \ref{ch:strict}.

The authors do not provide any statement as to the linearizability (or other concurrent correctness
criteria) of the SprayList, and it is not completely clear how to define it since no
sequential semantics are given.

Benchmarks show promising results: the SprayList scales well at least up to 80 threads,
and performs close (within a constant factor) to an implementation using a random remove instead
of \lstinline|DeleteMin|, which the authors consider as the performance ideal (although it might be
more fair to narrow that statement down to SkipList-based structures).

\section{Multiqueue} \label{sec:multiq}

Multiqueues, published in \citeyear{rihani2014multiqueues} by \citeauthor{rihani2014multiqueues} \cite{rihani2014multiqueues}, 
are a simple and elegant relaxed 
priority queues design using probabilistic techniques. The published design uses
a lock-based approach in combination with any sequential priority queue to construct
a scalable concurrent priority queue, but unfortunately without being able to offer
any defined quality guarantees (probabilistic or otherwise). In their benchmarks,
Multiqueues outperform the SprayList in both throughput (by around a factor of two)
and quality.

Similar to the \klsm (see Section \ref{sec:wimmer} and Chapter \ref{ch:implementation}),
the Multiqueue is a configurable data structure. It takes a parameter $c$, which
controls the number of created sequential priority queues. Each thread creates
$c$ queues, adding up to $cP$ queues in total.

In addition, each sequential queue is associated with its own lock and a cached
copy of its smallest key. Insertions
choose a random queue $q_i, i \in [0, cP[$, obtain the associated lock, and
insert the given item, updating the cached smallest key if necessary.

Deletions choose \emph{two} queues $q_i, q_j$ at random and peek at their respective cached
minimal keys. The queue with the smaller cached key is then locked and popped,
again updating the cached key. This technique is somewhat similar load balancing
through random selection \cite{richa2001power}, but no definite quality analysis
has yet been given due to the complications arising through priority queue semantics.

While the Multiqueue is inherently lock-based, it can easily be made lock-free
by simply using lock-free queues as its backing structures. We have implemented
a version using \acp{LSM} as the local queues and present the results in Chapter
\ref{ch:evaluation}.

% TODO: It might be nice to have one long-form example within each chapter 
% (sequential, strict, relaxed). In this chapter, the multiq seems to be a great
% fit since it's compact and easy to understand.

\section{$k$-LSM} \label{sec:wimmer}

\begin{comment}
We need to more or less rewrite this entire section since the technical description is unclear and 
outdated. Structure could be as follows:

* LSM - what is it?
* Dist LSM.
* Shared LSM.
* K LSM.

Detail should be such that it is clear how the PQ works, but not too low level since that is what
the next section will be about. We should briefly mention memory management but not go into too much
detail.
\end{comment}

We now finally come to the main topic of this thesis, the relaxed, linearizable, and lock-free
$k$-LSM priority queue.
\citeauthor{wimmer2013data} first presented this data structure in \citedate{wimmer2013data}
\cite{wimmer2013data} and have improved on it continuously since, with the most recent results
being published in \cite{wimmer2015lock}. The original $k$-LSM queue is integrated as a priority scheduler
into their \emph{Pheet} task-scheduling system, and an open-source implementation is
available at \url{http://pheet.org}. In this section we describe its design in detail, while
Chapter \ref{ch:implementation} covers our implementation and improvements.

Taken as a black box, the $k$-LSM conforms to the interface as shown in Figure \ref{fig:klsm_interface}.
It is a template data structure in the sense that it has a configuration parameter $k$, which
determines how far quality guarantees may be relaxed. Given $k$, the \lstinline|delete_min| operation
may return one of the $(k + 1) \cdot P$ minimal items, where $P$ is the number of threads.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class k_lsm {
public:
    void insert(const K &key, const V &value);
    bool delete_min(V &val);
};
\end{lstlisting}
\caption{The $k$-LSM interface.}
\label{fig:klsm_interface}
\end{figure}

The semantics are as follows: when \lstinline|insert| completes, the given key-value pair has been
inserted into the queue. \lstinline|delete_min| can either complete successfully, in which case
it returns \lstinline|true|, one of the $(k + 1) \cdot P$ minimal items is removed from the queue
and its value is written into the given argument; or it can fail, in which case no item is removed
from the queue and the \lstinline|false| is returned. Failures of \lstinline|delete_min| may occur
spuriously even if the queue is non-empty, but the number of such failures are low in practice.

The \klsm is a composite data structure consisting of a global component called the \ac{SLSM},
and a thread-local component called the \ac{DLSM}. As implied by their names, both the \ac{SLSM}
and \ac{DLSM} are based on the \ac{LSM} \cite{o1996log} data structure which will be elaborated
upon below. Both structures may be used as standalone priority queues, but have complementing
advantages and disadvantages which motivates their composition.

Throughout this section, we gloss over the complexities of lock-free memory management and simply
assume the existence of a garbage collector. The implementation by \citeauthor{wimmer2013data}
uses their own lock-free memory allocator for to handle items, while blocks and block arrays are
allocated in a pool and reused. Memory management will be discussed in further detail in Chapter
\ref{ch:implementation}.

\subsection{\aclp{LSM}} \label{ssec:lsm}

\aclp{LSM} were introduced to the database community in \citedate{o1996log} \cite{o1996log}
and reinvented indepently by \citeauthor{wimmer2013data} based on the requirements of concurrent
priority queues. \acp{LSM} are the basic building blocks of the \ac{SLSM} and \ac{DLSM}
and hence also the \klsm.

By itself, the
\ac{LSM} is not a parallel data structure, and special care must be taken when accessing shared
(i.e. not exclusively owned, thread-local) \acp{LSM}. For our purposes,
we imagine it to have an interface as shown in Figure \ref{fig:lsm_interface}. \lstinline|insert|
simply inserts the given item into the set, while \lstinline|peek_min| returns the
minimal item and \lstinline|true|, or \lstinline|false| if the set is empty.

\begin{figure}[ht]
\begin{lstlisting}
template <class Item>
class lsm {
public:
    void insert(const Item &item);
    bool peek_min(Item &item);
};
\end{lstlisting}
\caption{The \ac{LSM} interface.}
\label{fig:lsm_interface}
\end{figure}

In more detail, the \ac{LSM} consists of an ordered set of sorted arrays fulfilling a set of
invariants after the completion of each operation:

\begin{itemize}
\item Each block $B$ is associated with a level $i$ such that $B$ has a capacity of $2^i$ items.
\item If block $B$ (with associated level $i$) is located before block $B'$ (with level $i'$),
      then $i < i'$.
\item For each block $B$ with level $i$, $2^{i-1} \leq |B| \leq 2^i$ where $|B|$ is the number of items
      stored in $B$.
\end{itemize}

Insertions initially create a singleton block $B$ containing the given item. $B$ is then inserted
at the front of the \ac{LSM}, possibly violating invariants temporarily. While invariants remain unsatisfied,
i.e. while the \ac{LSM} contains multiple blocks of the same level, $B$ is merged with its
successor block, replacing both $B$ and the successor block with a new block of doubled capacity.

Peek operations iterate through each block in the \ac{LSM} and return the minimal encountered item.
Note that this operation is $O(1)$ for each block since the minimal item is
located at the head of each block, and thus of logarithmic complexity for the entire \ac{LSM}.

Both insertions and deletions are of amortized complexity $O(\log n)$ and are usually extremely
cache-efficient since items are mostly accessed with high locality (e.g. during merge operations).

\subsection{\acl{DLSM}} \label{ssec:dlsm}

The \ac{DLSM} is a priority queue with purely thread-local guarantees. It adheres to the same
interface as the \klsm as given in Figure \ref{fig:klsm_interface}, but with subtly altered semantics
in that there is no relaxation parameter $k$ --- the \ac{DLSM}'s \lstinline|delete_min| simply
removes the thread-locally minimal item. If the local queue is empty an operation called \lstinline|spy|
is performed, which attempts to copy all items from a randomly chosen thread's queue.

The implementation is essentially a thin wrapper on top of a thread-local \ac{LSM} with some complicating
factors. Items store both the key-value pair as well as an atomic flag indicating whether an item
has been taken (removed) from the \ac{DLSM} or not. The \ac{LSM} itself stores pointers to these items.
\lstinline|delete_min| operations perform
a \lstinline|peek_min| on the underlying local \ac{LSM} and mark the returned item as taken using
an atomic \ac{CAS} instruction. Special care is taken in order to eventually physically remove taken items
from the \ac{LSM}: merges skip taken items, and whenever a block is noted to be less than half-full
the block is shrunk and \ac{LSM} invariants are reasserted.

% TODO: Revise following paragraph - explanations are unclear and I'm not sure whether
% it is even possible to encounter an item multiple times.

So far, all operations have been exclusively thread-local, and thus no attention had to be given
to concurrency complications. Unfortunately, \lstinline|spy| does access other thread's \acp{LSM}
and we cannot ignore these issues entirely. The set of blocks in the \ac{LSM} is modelled
as a doubly-linked list of blocks. The set of pointers to the previous element may be accessed
exclusively by the owning thread, while the set of pointers to the next element can by read by
all threads (but written only by the owning thread). When a maintenance operation triggers
changes to the block set, merges are performed locally and the new tail of the linked list is
spliced in atomically using a \ac{CAS} instruction. While other threads currently reading the local list
may encounter pointers to the same item multiple items within a single \lstinline|spy| operation if the linked
list has been updated concurrently, this is not an issue since the item may only be taken successfully
by a single thread when its flag is set atomically.

The \ac{DLSM} is essentially embarassingly parallel as long as all local queues remain nonempty,
and scales exceedingly well even at very high thread counts. This is to be expected, since the \ac{LSM}
is very efficient, and most, if not all operations are thread-local. However, its major weakness
is the lack of global quality guarantees. The next section discusses the \ac{SLSM}, which is utilized
in the \klsm in order to amend this point.

\subsection{\acl{SLSM}} \label{ssec:slsm}

The \acl{SLSM} could be considered the polar opposite of the previous section's
\ac{DLSM}: it consists of a single, global \ac{LSM} whereas the \ac{DLSM} has a local \ac{LSM}
for each thread; it is relaxed, returning one of the $k$ minimal items from \lstinline|delete_min|
whereas the \ac{DLSM} is strict; and it has fairly limited scalability while the \ac{DLSM} is
embarassingly parallel.

The \ac{SLSM}'s interface is once again identical to Figure \ref{fig:klsm_interface}.
Insertion semantics are as expected, and deletions may either succeed, removing and returning one of the $k$
minimal items within the queue; or fail without altering the queue's item set.

At a high level, the concept of its implementation is simple: it consists of a single global 
\ac{LSM} with an associated item range containing the (approximately) $k$ minimal 
items within the queue. Insertions first add the new item to a local copy of the
queue and then atomically replace the global copy with the modified local version.
Deletions query the pivot range size, and then randomly select one of the contained
items. If it has not yet been taken by another thread, it is removed and the call returns
successfully. Otherwise, the same procedure is repeated (possibly growing the pivot range)
until the queue is either empty or an untaken item is found and returned. 

From the previous description, it almost seems as if the \ac{SLSM} were a simple
data structure; however, rest assured that this is not the case. Chapter \ref{ch:implementation}
covers its implementation in great detail.

\subsection{\klsm} \label{ssec:klsm}

Taken by themselves, both the \ac{DLSM} and \ac{SLSM} are interesting but not
particularly practical data structures. The \ac{DLSM} is fast and scalable, but
cannot offer any guarantees as to the quality of returned items, while the \ac{SLSM}
has a significant global bottleneck.

The \klsm combines both of these designs to reduce these drawbacks. In order
to be able to offer global quality guarantees, the thread-local \ac{DLSM} are
limited to a capacity of $k$ each, and \lstinline|delete_min| returns the minimal
item returned from the \ac{DLSM} and \ac{SLSM}. Note that we skip up to $k (P - 1)$
potentially smaller items located within other thread's local \ac{DLSM} and up to
$k - 1$ items through relaxation of the \ac{SLSM}. We therefore still stay comfortably
within the claimed guarantees of returning one of the $(k + 1) \cdot P$ minimal items.

The remaining issue of the \ac{SLSM}'s scalability is ultimately caused by updates
to the global \ac{LSM}, which in turn mostly occur through item insertions.
The \klsm reduces the occurrence of these insertions by a factor of $O(k)$ by
only inserting blocks of size $> \frac{k}{2}$ into the \ac{SLSM}. This harmonizes
well with the quality guarantee solution from the previous paragraph - when
a local \ac{DLSM} exceeds its maximal size, the largest block is simply inserted
into the \ac{SLSM}.

The \klsm as presented in \cite{wimmer2015lock} exhibits varying behavior according
to the parameter $k$; at low values ($k \in \{0, 4\}$) scalability is somewhat comparable
the the \citeauthor{linden2013skiplist} queue. At higher relaxation values of $k \in \{256, 4096\}$,
the \klsm approaches linear speedup up to high thread counts.