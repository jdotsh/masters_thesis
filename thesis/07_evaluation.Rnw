\chapter{Results \& Discussion} \label{ch:evaluation}

This chapter presents and discusses results for a variety of benchmarks which
have been executed
on a number of different platforms. We evaluate both the standalone and
Pheet \klsm's in various degrees of relaxation
together with a selection of other representative parallel priority queues.

Section \ref{sec:benchmarks} covers the different benchmark types we used, while
Section \ref{sec:algorithms} presents the various priority queue algorithms and
Section \ref{sec:methodology} lists our systems and methodology. Results are
finally shown and discussed in Section \ref{sec:results}.

\section{Benchmarks} \label{sec:benchmarks}

Initially when we started work on the \klsm, we relied almost exlusively on
a benchmark which we call the uniformly random throughput benchmark, in which
the priority queue is prefilled with a certain number of elements, and each
thread then performs an equal mixture of insertion/deletion operations.
The operation type is chosen uniformly at random with a probability of $50\% / 50\%$
insertions/deletions, and key values for insertions are likewise chosen uniformly at random
within the range of 32-bit integers. The benchmark is run for a certain amount
of time, and the number of performed operations per second is then reported as the resulting
throughput.

This type of benchmark is widely popular within priority queue research and
has been used as the basis for performance evaluations in most publications,
including
\cite{hunt1996efficient,alistarhspraylist,linden2013skiplist,shavit2000skiplist,sundell2003fast,wimmer2015lock,cbpq}.
Its popularity is understandable: the uniform random throughput benchmark is
easy to understand as well as to implement, and it allows for some form of
consistency and comparability between different publications.

However, within the course of this thesis, it has become clear that this benchmark
has a significant drawback, since it causes priority queues to degrade to
quasi-\ac{LIFO} queue performance over time and may severely distort obtained
results. This is caused  by the fact that inserted keys are chosen uniformly at random ---
over time, as lower keys are removed from the priority queue its contents become
biased towards higher keys. Newly inserted keys have a high chance of being within
the lowest keys of the queue and quickly becoming a candidate for removal by
\lstinline|delete_min|. Especially with queues having relaxed semantics such as
the SprayList and the \klsm, this can lead to a situation in which the behavior of
a priority queue very closely approximates a relaxed \ac{LIFO} queue.

In order to get a better picture of a queue's overall performance, we have introduced
parameters to obtain variations on the random throughput benchmark. The goal
of these parameters was to preserve the aspect of the random throughput benchmark
which caused the data structure to remain at a more or less constant size, allowing
a benchmark to run without resulting in either an empty or an ever-growing queue.

The first introduced parameter is called the \emph{workload}; a balanced workload
performs a roughly equal amount of insertions and deletions on each thread, while
a split workload performs only insertions on half of all threads, and only
deletions on the other half. The second parameter concerns \emph{key generation},
and we have experimented both with uniform key generation (uniformly at random
within the range of 32-bit integers), and ascending key generation (uniformly at random
within a smaller range that grows over time, i.e. selection from $[0, U] + t$),
as well as descending generation.

Finally, the quality of the produced output is evaluated by using a variant of
the throughput benchmark. Quality in this sense compares the output sequence of
the measured priority queue against a strict sequential queue. Each insertion
and deletion is tagged with a timestamp, which is then used to reconstruct a global,
approximate linear sequence of insertions and deletions. This sequence is then
replayed using a strict sequential queue, and ranks of each deleted item
within the queue at the time of deletion are then recorded (with the rank of the least
item being $1$). Theoretically, strict queues should result in an average rank
of $1$, but in practice we can expect deviations of this value due to `simultaneous'
operations between threads and imprecise timestamps. However, even though
we note that the average rank is not an exact value, it still provides a good feel for the
quality of a queue's returned results.

The strict priority queue used within the quality benchmark is required to support an operation which
deletes a specific element and return its rank. Contrary to similar benchmarks by
\citeauthor{rihani2014multiqueues} \cite{rihani2014multiqueues} who rely on the C++ standard library's \lstinline|multiset|
class (resulting in $O(r)$ deletions, where $r$ is the rank of the deleted item), we use a custom
data structure based on a \ac{BST} which supports the required operation in logarithmic time.
This allows us to efficiently evaluate results with higher ranks than would otherwise be possible.
Additionally, we minimize interference with the actual benchmark by storing the
thread-local sequences temporarily and processing them offline, while \citeauthor{rihani2014multiqueues}
perform rank determination interleaved with the actual benchmark operations.

\section{Algorithms} \label{sec:algorithms}

In our experiments, we compare the standalone \klsm against a selection of other
priority queue algorithms, as well as the original Pheet implementation. The
used algorithms are as follows:

\begin{itemize}
\item \textit{GlobalLock} (\verb|globallock|)
      An instance of the \verb|std::priority_queue<T>| class provided
      by the C++ standard library, protected by a single global lock. This naive
      algorithm is included as a baseline, and serves to show the minimal acceptable
      performance of a concurrent priority queue.
\item \textit{Linden} (\verb|linden|)
      Code for the \citeauthor{linden2013skiplist} priority queue \cite{linden2013skiplist}
      is provided by the authors under an open source license\footnote{\url{http://user.it.uu.se/~jonli208/priorityqueue},
      last visited on December 9\textsuperscript{th}, 2015.}.
      It is lock-free and uses \citeauthor{fraser2004practical}'s lock-free
      SkipList design. The aim of this implementation is to minimize contention in
      calls to \verb|delete_min|. We chose $32$ as the \verb|BoundOffset| in order to optimize
      performance on a single processor. A \verb|BoundOffset| of $128$ performed only marginally better
      at high thread counts. The \citeauthor{linden2013skiplist} queue represents
      strict, lock-free and SkipList-based priority queues.
\item \textit{SprayList} (\verb|spray|)
      A relaxed, lock-free concurrent priority queue based on \citeauthor{fraser2004practical}'s
      SkipList using random walks to spread data accesses
      incurred by \verb|delete_min| calls. Code provided by \citeauthor{alistarhspraylist} is
      available on Github\footnote{\url{https://github.com/jkopinsky/SprayList},
      last visited on December 9\textsuperscript{th}, 2015.}.
\item \textit{Multiqueues} (\verb|multiq|)
      A simple, elegant recent relaxed concurrent priority queue
      design by \citeauthor{rihani2014multiqueues}. Contrary to the SprayList, the
      Multiqueue implementation is lock-based. Since code for the SprayList is
      not publicly available, we use our own reimplementation for benchmarks.
\item \textit{Pheet \klsm} (\verb|pheet16, pheet128, ...|)
      \citeauthor{wimmerphd}'s original implementation
      of the \klsm within the Pheet task scheduling framework is used mostly to
      verify the behavior of the standalone reimplementation. We used a range
      of values of $k$ to vary between fairly strict ($k = 16$) to fairly relaxed
      ($k = 4096$) behavior. Unlike the other priority queues, the Pheet \klsm
      was measured using the benchmarks integrated into Pheet. The code
      was retrieved from their Launchpad site\footnote{\url{http://www.pheet.org},
      last visited on December 9\textsuperscript{th}, 2015.}.
\item \textit{Standalone \klsm} (\verb|klsm16, klsm128, ...|)
      The standalone \klsm
      reimplementation was measured by using a wide range of values for the
      relaxation parameter $k$. Both the standalone and Pheet \klsm implementations
      are linearizable, lock-free, and relaxed priority queues.
      Code for the standalone \klsm, as well as the entire benchmarking suite, is available
      on Github at \url{https://github.com/schuay/kpqueue}.
\end{itemize}

Unfortunately, we were not able to benchmark all algorithms on all machines and
with all parameters. The SprayList implementation has been very unstable throughout
our tests and crashes when using either split workload or ascending key generation.
Both the Linden queue and the SprayList also require libraries which were not
available on our Solaris machine, and thus could not be compiled there. Finally,
neither the Linden queue nor the SprayList support insertion and deletion of
key-value pairs, therefore it was not possible to evaluate them using our quality
benchmark. In these cases, results are simply omitted for the affected data
structures.

\section{Environment and Methodology} \label{sec:methodology}

Three different machines are used to run benchmarks:

\begin{itemize}
\item \lstinline|mars|
      An 80-core system consisting of 8 Intel Xeon E7-8850
      processors with 10 cores each and 1 TB of RAM. The processors are clocked
      at 2 GHz and have 32 KB L1, 256 KB L2, and 24 MB of L3 cache per core.
\item \lstinline|saturn|
      A 48-core machine with 4 AMD Opteron 6168 processors
      with 12 cores each, clocked at 1.9 GHz. \lstinline|saturn| has
      64 KB of L1, 512 KB of L2, and 5 MB of L3 cache per core and 125 GB of RAM.
\item \lstinline|ceres|
      A 64-core SPARCv9-based machine with 4 processors
      of 16 cores each. Cores are clocked at 3.6 GHz and have 8-way hardware
      hyperthreading. \lstinline|ceres| has 16 KB of L1, 128 KB of L2,
      and 8 MB of L3 cache per core and 1 TB of RAM.
\end{itemize}

Figure \ref{fig:machine_architecture_mars} shows the topology of one of eight
cores on \lstinline|mars|, generated with \emph{hwloc's} \verb|lstopo| tool.
Topology graphs for all machines can be found in Appendix \ref{ch:topologies}.

\begin{figure}[ht]
\includegraphics[width = \textwidth]{graphics/lstopo_mars.pdf}
\caption{Topology of one of eight nodes on \lstinline|mars|.}
\label{fig:machine_architecture_mars}
\end{figure}

All applications are compiled using \verb|gcc| --- version \verb|5.2.1|
on \lstinline|mars| and \lstinline|saturn|, and version \verb|4.8.2| on
\lstinline|ceres|. We use an optimization level of \verb|-O3| and enable
link-time optimizations using \verb|-flto|.

Each benchmark is executed 10 times, and we report on the mean values and
confidence intervals. Priority queues are pre-filled with $10^6$ items
before the benchmark is started.

\section{Results} \label{sec:results}

\subsection{Generic throughput, uniform workload, uniform key generation}

We first examine results for the random throughput benchmark with uniform
workload (each thread does a roughly equal amount of insertions and deletions)
and uniform key generation. Unless stated otherwise, performance measurements
are reported for benchmarks on \lstinline|mars|.

<<echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
@

\begin{figure}[ht]
\begin{center}
\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151112_saturn_uni_uni")
@
\subcaption{\lstinline|saturn|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151116_ceres_uni_uni")
@
\subcaption{\lstinline|ceres|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151112_mars_uni_uni")
@
\subcaption{\lstinline|mars|}
\end{minipage}
\end{center}
\caption{Uniform workload, uniform keys.}
\label{fig:uni_uni}
\end{figure}

Figures \ref{fig:uni_uni} and \ref{fig:mars_uni_uni_slow}
displays the throughput in operations per second
over a range of threads. The \klsm dominates all other priority queues at
medium to high relaxation ($k \in \{ 128, 256, 4096 \}$), reaching over 200
\ac{MOps} and scaling up to 80 threads. The \klsm's scalability heavily depends on the value
of $k$; at low values the \klsm behaves similar to other concurrent priority
queues (see Figure \ref{fig:mars_uni_uni_slow} for a detail view of slower queues).
At $k = 128$, the \klsm scales well until around 25 threads, and higher values
of $k$ improve scalability up to 50 ($k = 256$) and 80 threads ($k = 4096$).

Multiqueues (\verb|multiq|) perform
the best out of all other data structures, reaching around 30 \ac{MOps} at 75
threads. Although their performance suffers once cores on more than one processor
are used (this occurs at 15 threads in our figure), they eventually recover and keep
scaling until almost the maximal thread count.

Likewise, the SprayList initially peaks at 10 threads, loses performance
at 15 threads, but then never manages to scale beyond the performance of a single
processor. As expected, the
\citeauthor{linden2013skiplist} queue performs well while executed on a single
processor, but throughput stays constant at just over 2 \ac{MOps} above 10 threads.
The \verb|globallock| is the highest performer when executed sequentially, but
throughput stays low at around 1 \ac{MOps} at higher thread counts.

While the general trends are consistent across machines, each of the
measured machines clearly has different potential for scalability. For instance,
on \lstinline|saturn| the \klsm with $k = 128$ only makes minor gains when
utilizing more than one processor, while the Multiqueues barely scale at all.
On the other hand, on \lstinline|ceres| even $k = 128$ seems to be able
to scale further beyond the maximum number of cores.

\FloatBarrier

\begin{figure}[ht]
\begin{center}
<<fig = TRUE, width = 10, echo = FALSE>>=
pqplot("results/20151112_mars_uni_uni_slow")
@
\end{center}
\caption{Uniform workload, uniform keys on \lstinline|mars| (detail view of slower queues).}
\label{fig:mars_uni_uni_slow}
\end{figure}

\begin{figure}[ht]
\begin{center}
<<fig = TRUE, width = 10, echo = FALSE>>=
pqplot("results/20151112_mars_uni_uni_pheet")
@
\end{center}
\caption{Uniform workload, uniform keys on \lstinline|mars|.
         Comparison of the standalone \klsm against Pheet's \klsm.}
\label{fig:mars_uni_uni_pheet}
\end{figure}

Figure \ref{fig:mars_uni_uni_pheet} compares throughput of our new standalone
\klsm to that of the Pheet \klsm implementation. Interestingly, their behavior
differs significantly, with the Pheet \klsm performing stronger at lower
thread counts but the standalone \klsm scaling better at high concurrency,
reaching higher absolute performance, and having more predictable throughput (i.e.,
less variance). It is not necessarily surprising
that the two implementations behave differently, since the standalone
\klsm has various significant differences in implementation details (e.g.,
pivot calculation based on binary-search, completely segregated \ac{DLSM}
and \ac{SLSM}, \ldots).

\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrrrr}
\toprule
& \multicolumn{2}{c}{20 threads} & \multicolumn{2}{c}{40 threads} & \multicolumn{2}{c}{80 threads} \\
\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
& Mean & Std. Dev. & Mean & Std. Dev & Mean & Std. Dev \\
\midrule
\lstinline|globallock| & 1.9 & 1.0 & 2.9 & 1.7 & 4.8 & 2.9 \\
\lstinline|klsm16|   & 20 & 15 & 23 & 20 & 15 & 9 \\
\lstinline|klsm128|  & 33 & 31 & 55 & 46 & 430 & 294 \\
\lstinline|klsm256|  & 42 & 42 & 71 & 61 & 750 & 828 \\
\lstinline|klsm4096| & 297 & 496 & 625 & 1014 & 10353 & 12667 \\
\lstinline|multiq|   & 984 & 2899 & 2252 & 7433 & 3787 & 12549 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Rank error for the uniform workload, uniform key generation benchmark
on \lstinline|mars|.}
\label{tbl:mars_uni_uni_q}
\end{table}

We also evaluated the quality of the results using a rank error metric, shown
in Table \ref{tbl:mars_uni_uni_q}. For each item that is removed from
a priority queue, a rank of $r$ signifies that it is the $r$-lowest item within
the queue. Therefore, by definition, a strict priority queue such as
\lstinline|globallock| should have a mean rank of $1.0$ and a standard deviation
of $0$. Unfortunately, our quality benchmark only reports approximate results ---
measured timestamps may differ slightly between threads, and the timestamps cannot
be read exactly at the linearization point of insertions and deletions.

Rank errors for the strict \lstinline|globallock| queue are reported in order
to show that the magnitude of the resulting inaccuracies is very low. Results
returned by the \klsm usually far exceed the provided guarantees. Recall that the
\klsm may return any of the $kP$ minimal elements, where $P$ is the number
of threads. While the \lstinline|klsm128| may, at 20 threads, return up
to a rank error of $2580$, the actual mean rank error is merely $33$.
Compared to the \klsm, Multiqueues have a high rank error at lower thread counts,
but the error increases less rapidly at higher concurrency.

\subsection{Generic throughput, uniform workload, ascending key generation}

\begin{figure}[ht]
\begin{center}
\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151112_saturn_uni_asc")
@
\subcaption{\lstinline|saturn|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151116_ceres_uni_asc")
@
\subcaption{\lstinline|ceres|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151112_mars_uni_asc")
@
\subcaption{\lstinline|mars|}
\end{minipage}
\end{center}
\caption{Uniform workload, ascending keys.}
\label{fig:uni_asc}
\end{figure}

Figure \ref{fig:uni_asc} shows results for the throughput benchmark with
a uniform workload and ascending keys. The \klsm performs radically different
to the previously discussed benchmark with uniform key generation in this scenario;
instead of exceptional performance and scalability proportional to the degree
of relaxation, the \klsm's throughput never manages to exceed that of the Multiqueues.
instead, throughput fluctuates between a minimum of roughly 5 \ac{MOps} at
processor boundaries and a maximum of around 12 \ac{MOps} otherwise. While not
shown here, we verified that the Pheet \klsm behaves similarly to the standalone
\klsm.

Multiqueues, the \citeauthor{linden2013skiplist} queue, and the \verb|globallock|
baseline perform similarly to uniform key generation, while the SprayList
crashes and could not be measured.

This result is both surprising and disappointing. Performance counters indicate
that when using uniform key generation, $97\%$ of all deleted items are taken
from the \ac{DLSM} --- and this is obviously the reason for the \klsm's high
performance. But how is this kind of skewed balance between the \ac{SLSM} and the
\ac{DLSM} possible? Since \lstinline|k_lsm::delete_min| peeks at one item from
the \ac{SLSM} and one from the \ac{DLSM}, wouldn't it be reasonable to expect
a $50/50$ proportion of items taken from the either component?

It turns out that the answer to this question is a definite ``no'' in the case of
uniform key generation: initially, after prefill is completed, the \klsm
contains a selection of keys randomly distributed over the range of integers.
However, as time goes on, lower keys are removed and the \klsm's
key range becomes skewed towards higher keys. At some point, when most low
keys have been deleted from the \ac{SLSM}, a stable state is reached in which
the global \ac{SLSM} contains mostly old, high keys while the local \acp{DLSM}
contain new, low keys. Items are usually removed from the local \ac{DLSM}, and thus
updates to the global \ac{SLSM} are infrequent; furthermore, since the
best known item within the \ac{SLSM} is cached, and items are usually taken
from the \ac{DLSM}, the item cache is highly efficient and most calls to
\lstinline|shared_lsm::peek()| do no actual work.

The ascending key generation benchmark is designed to move outside of the comfort
zone of the \klsm in which the \ac{DLSM} is highly utilized. Since the values of inserted
keys rise over time and a min-priority queue always removes the least key,
a relaxed \ac{FIFO}-like behavior results. In the case of the \klsm, items will thus
usually be initially inserted into the local \ac{DLSM}; then moved to the \ac{SLSM}
over time, and finally deleted from the \ac{SLSM} once most lower items have
been removed from the priority queue. Contrary to uniform key generation, emphasis
is placed on the \ac{SLSM} and the efficient \ac{DLSM} cannot be exploited
for its full potential.

Again, behavior across the different machines differs significantly. On \lstinline|saturn|,
no algorithm appears to scale well, with Multiqueues reaching maximal absolute
performance at 48 threads with less then 10 MOps. On \lstinline|ceres|, Multiqueues
dominate, reaching a peak of around 40 MOps, while the \klsm never exceeds 10 MOps.
Finally, behavior on \lstinline|mars| is similar, but with Multiqueues reaching
a lower initial peak at one processor, and a more pronounced oscillating behavior
of the \klsm.

\FloatBarrier

\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrrrr}
\toprule
& \multicolumn{2}{c}{20 threads} & \multicolumn{2}{c}{40 threads} & \multicolumn{2}{c}{80 threads} \\
\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
& Mean & Std. Dev. & Mean & Std. Dev & Mean & Std. Dev \\
\midrule
\lstinline|globallock| & 4.5 & 4.2 & 10.3 & 8.8 & 19.6 & 16.9 \\
\lstinline|klsm16|   & 7 & 5 & 11 & 9 & 20 & 18 \\
\lstinline|klsm128|  & 21 & 18 & 22 & 19 & 25 & 22 \\
\lstinline|klsm256|  & 38 & 33 & 38 & 33 & 49 & 37 \\
\lstinline|klsm4096| & 505 & 474 & 465 & 436 & 483 & 457 \\
\lstinline|multiq|   & 101 & 120 & 202 & 239 & 419 & 500 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Rank error for the uniform workload, ascending key generation benchmark
on \lstinline|mars|.}
\label{tbl:mars_uni_asc_q}
\end{table}

With ascending key generation, the rank error of all measured \klsm variants as
well as the Multiqueues seems to be much more stable than when using uniform
key generation (see Table \ref{tbl:mars_uni_asc_q}). On the other hand, the
measured rank error of the strict \lstinline|globallock| queue is significantly
higher. This can be explained by the nature of our ascending keygeneration, which
results in more frequent key duplicates between items; and in the case of the
\lstinline|globallock|, duplicate keys lead to higher rank errors because of the
way rank error is measured within our benchmark.

Keys are also generated in a smaller, more predictable range; for the Multiqueues,
this results in queues on all threads with very similar contents, and thus
rank errors are smaller.

As explained above, using ascending key generation
results in a bias in the \klsm such that more items are removed from the \ac{SLSM}
than the \ac{DLSM}. And since the \ac{SLSM} has a lower rank guarantee than the
\klsm ($k$ instead of $kP$), the resulting rank errors are low.

\subsection{Generic throughput, split workload, uniform key generation}

\begin{figure}[ht]
\begin{center}
\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151112_saturn_spl_uni")
@
\subcaption{\lstinline|saturn|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151116_ceres_spl_uni")
@
\subcaption{\lstinline|ceres|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 10, echo = FALSE>>=
pqplot("results/20151112_mars_spl_uni")
@
\subcaption{\lstinline|mars|}
\end{minipage}
\end{center}
\caption{Split workload, uniform keys.}
\label{fig:spl_uni}
\end{figure}

In the case of a split workload, in which each thread is either a dedicated
inserter or deleter, und uniform key generation,
the benchmarked priority queues performed roughly as
with uniform workload and ascending key generation (see Figure \ref{fig:spl_uni}).
Multiqueues perform consistently
as in other scenarios, while the \klsm does not scale at all. Surprisingly,
the \lstinline|klsm16| performs at least as well as more relaxed \klsm variants;
on \lstinline|ceres|, it even outperforms the other variants by more than a factor
of two at low thread counts.

\FloatBarrier

\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrrrr}
\toprule
& \multicolumn{2}{c}{20 threads} & \multicolumn{2}{c}{40 threads} & \multicolumn{2}{c}{80 threads} \\
\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
& Mean & Std. Dev. & Mean & Std. Dev & Mean & Std. Dev \\
\midrule
\lstinline|globallock| & 1.9 & 1.0 & 2.7 & 1.6 & 4.5 & 2.7 \\
\lstinline|klsm16|   & 23 & 10 & 52 & 20 & 120 & 55 \\
\lstinline|klsm128|  & 134 & 74 & 258 & 158 & 700 & 416 \\
\lstinline|klsm256|  & 235 & 140 & 821 & 448 & 1981 & 1112 \\
\lstinline|klsm4096| & 3814 & 2176 & 11652 & 8924 & 35001 & 21352 \\
\lstinline|multiq|   & 653 & 2559 & 478 & 1422 & 2111 & 6002 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Rank error for the split workload, uniform key generation benchmark
on \lstinline|mars|.}
\label{tbl:mars_spl_uni_q}
\end{table}

Rank errors (Table \ref{tbl:mars_spl_uni_q}) are generally higher for the
\klsm than both the uniform workload/uniform key generation and uniform/ascending
cases, possibly because the \ac{DLSM} component is often empty (or filled with
only with outdated, spied elements from another thread); thus instead of comparing
two items and returning the lesser, it simply returns the \ac{SLSM} item. Keys are
generated uniformly, and the \ac{SLSM} is therefore biased towards larger keys.

\subsection{Generic throughput, split workload, ascending key generation}

\begin{figure}[ht]
\begin{center}
\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151112_saturn_spl_asc")
@
\subcaption{\lstinline|saturn|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151116_ceres_spl_asc")
@
\subcaption{\lstinline|ceres|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151112_mars_spl_asc")
@
\subcaption{\lstinline|mars|}
\end{minipage}
\end{center}
\caption{Split workload, ascending keys.}
\label{fig:spl_asc}
\end{figure}

Finally, results for split workload and ascending key generation are shown
in Figure \ref{fig:spl_asc}. This setting combines the two variants which are
especially detrimental for the \klsm; and as expected no \klsm variants
perform well, and never exceed 5 \ac{MOps} on any machine and with any
thread count.

Contrary to the \klsm, Multiqueues seem to perform best in this benchmark
scenario, reaching up to 50 MOps on \lstinline|ceres| and 17 MOps on \lstinline|saturn|.

The biggest surprise, however, was that the strict Linden queue becomes quite
competitive, outperforming all \klsm variants and scaling up to 80 threads
on \lstinline|mars| and 48 threads on \lstinline|saturn|. We suspect that this
is because of improved spacial cache locality: inserter threads insert at the
back of the skiplist, and can keep the tail of the list as well as high-level
shortcut nodes within their cache, while deleter threads only access the front
of the skiplist.

\FloatBarrier

\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrrrr}
\toprule
& \multicolumn{2}{c}{20 threads} & \multicolumn{2}{c}{40 threads} & \multicolumn{2}{c}{80 threads} \\
\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}
& Mean & Std. Dev. & Mean & Std. Dev & Mean & Std. Dev \\
\midrule
\lstinline|globallock| & 2.6 & 2.1 & 9.1 & 8.3 & 10.2 & 17.0 \\
\lstinline|klsm16|   & 4 & 3 & 11 & 9 & 22 & 24 \\
\lstinline|klsm128|  & 18 & 15 & 21 & 17 & 38 & 126 \\
\lstinline|klsm256|  & 33 & 28 & 50 & 130 & 92 & 365 \\
\lstinline|klsm4096| & 428 & 388 & 488 & 435 & 1578 & 4186 \\
\lstinline|multiq|   & 133 & 417 & 317 & 726 & 1015 & 1946 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Rank error for the split workload, ascending key generation benchmark
on \lstinline|mars|.}
\label{tbl:mars_spl_asc_q}
\end{table}

As in the uniform workload, ascending key generation benchmark, rank errors (Table
\ref{tbl:mars_spl_asc_q} are fairly low. However, the Multiqueues show very unpredictable
behavior while within a single processor ($\leq 10$ threads), with high mean rank
errors and a wide spread ($2452$ mean rank error and $8790$ standard deviation on
10 threads).

\subsection{Generic throughput, uniform workload, descending key generation}

We have attempted to be unbiased in these benchmarks, examining both situations
beneficial to the \klsm (e.g., the uniform/uniform benchmark) as well
as less well suited scenarios (split workloads and ascending key generation).
Furthermore, we have determined throughput of the \klsm is proportional
to the degree that the distributed component is utilized.

\begin{figure}[ht]
\begin{center}
\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151120_saturn_uni_desc")
@
\subcaption{\lstinline|saturn|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151117_ceres_uni_desc")
@
\subcaption{\lstinline|ceres|}
\end{minipage}

\begin{minipage}[b]{\textwidth}
\centering
<<fig = TRUE, width = 12, echo = FALSE>>=
pqplot("results/20151123_mars_uni_desc")
@
\subcaption{\lstinline|mars|}
\end{minipage}
\end{center}
\caption{Uniform workload, descending keys.}
\label{fig:uni_desc}
\end{figure}

Ascending key generation
(which induces \ac{FIFO}-like behavior) and split workloads are on one end of the spectrum,
and place high stress on the \ac{SLSM}. The uniform workload, uniform key generation
benchmark places high emphasis on the \ac{DLSM} and results in greatly increased throughput.

In this final section, we would like to examine the most beneficial
of these situations, and induce \ac{LIFO}-like behavior by using descending key generation.
As new keys are very likely to be within the least keys of the queue, the \ac{DLSM}
is very highly utilized, and contents of the \ac{SLSM} are fairly static (with sufficient relaxation).

Results are displayed in Figure \ref{fig:uni_desc}. As expected, peak throughput
of the \klsm reaches new heights of around 150 MOps/s on \lstinline|saturn| and around 300 MOps/s
on \lstinline|ceres| and \lstinline|mars|. Multiqueue performance remains stable
and is comparable to the uniform/uniform case.

Surprisingly, lower relaxation \klsm's actually have deteriorated scalability
on \lstinline|mars|, possibly since a higher amount of low-key items enter the
\ac{SLSM} when local \acp{DLSM} overflow.
