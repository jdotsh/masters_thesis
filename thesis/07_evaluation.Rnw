\chapter{Results \& Discussion} \label{ch:evaluation}

This chapter presents and discusses results for a variety of benchmarks, and executed
on a number of different platforms. We evaluate both the \klsm
together with a selection of other representative parallel priority queues.

Section \ref{sec:benchmarks} covers the different benchmark types we used, while
Section \ref{sec:algorithms} presents the various priority queue algorithms and
Section \ref{sec:methodology} lists our systems and methodology. Results are
finally shown and discussed in Section \ref{sec:results}.

\section{Benchmarks} \label{sec:benchmarks}

Initially when we started work on the \klsm, we relied almost exlusively on
a benchmark which we call the uniformly random throughput benchmark, in which
the priority queue is prefilled with a certain number of elements, and each
thread then performs an equal mixture of insertion/deletion operations.
The operation type is chosen uniformly at random with a probability of $50\% / 50\%$
insertions/deletions, and key values for insertions are likewise chosen uniformly at random
within the range of 32-bit integers. The benchmark is run for a certain amount
of time, and the number of performed operations per second is then reported as the resulting
throughput.

This type of benchmark is hugely popular within priority queue research and
has been used as the basis for performance evaluations in most publications,
including
\cite{hunt1996efficient,alistarhspraylist,linden2013skiplist,shavit2000skiplist,sundell2003fast,wimmer2015lock,cbpq}.
Its popularity is understandable: the uniform random throughput benchmark is
easy to understand as well as to implement, and it allows for some form of
consistency and comparability between different publications.

However, within the course of this thesis, it has become clear that this benchmark
has a significant drawback, since it causes priority queues to degrade to
quasi-\ac{LIFO} queue performance over time and may severely distort obtained
results. This is caused  by the fact that inserted keys are chosen uniformly at random ---
over time, as lower keys are removed from the priority queue its contents become
biased towards higher keys. Newly inserted keys have a high chance of being within
the lowest keys of the queue and quickly becoming a candidate for removal by
\lstinline|delete_min|. Especially with queues having relaxed semantics such as
the SprayList and the \klsm, this can lead to a situation in which the behavior of
a priority queue very closely approximates a \ac{LIFO} queue.

In order to get a better picture of a queue's overall performance, we have introduced
parameters to obtain variations on the random throughput benchmark. The goal
of these parameters was to preserve the aspect of the random throughput benchmark
which caused the data structure to remain at a more or less constant size, allowing
a benchmark to run without resulting in either an empty or an ever-growing queue.

The first introduced parameter is called the \emph{workload}; a balanced workload
performs a roughly equal amount of insertions and deletions on each thread, while
an unbalanced workload performs only insertions on half of all threads, and only
deletions on the other half. The second parameter concerns \emph{key generation},
and we have experimented both with uniform key generation (uniformly at random
within the range of 32-bit integers), and ascending key generation (uniformly at random
within a smaller range that grows over time, i.e. selection from $[0, U] + t$).

Besides the artificial throughput benchmark, we also experimented with a
single source shortest paths benchmark as in \cite{wimmer2015lock}. The graphs
are randomly generated for a given key and node degree. Each thread then repeatedly
pops the queue (containing the next path portion to evaluate), generates successor
subpaths, and adds them to the queue.

Finally, the quality of the produced output is evaluated by using a variant of
the throughput benchmark. Quality in this sense compares the output sequence of
the measured priority queue against a strict sequential queue. Each insertion
and deletion is tagged with a timestamp, which is then used to reconstruct an
approximate linear sequence of insertions and deletions. This sequence is then
replayed using a strict sequential queue, and the rank of each deleted item
within the queue at the time of deletion is recorded (with the rank of the least
item being $1$). Theoretically, strict queues should result in an average rank
of $1$, but in practice we can expect deviations of this value due to 'simultaneous'
operations between threads and imprecise timestamps. However, even though
it is not an exact value, the average rank still provides a good feel for the
quality of a queue's returned results.

\section{Algorithms} \label{sec:algorithms}

In our experiments, we compare the standalone \klsm against a selection of other
priority queue algorithms, as well as the original Pheet implementation. The
used algorithms are as follows:

\begin{itemize}
\item \textit{GlobalLock} (referred to in figures as \verb|globallock|)
      An instance of the \verb|std::priority_queue<T>| class provided
      by the C++ standard library, protected by a single global lock. This naive
      algorithm is included as a baseline, and serves to show the minimal acceptable
      performance of a concurrent priority queue.
\item \textit{Linden} (\verb|linden|)
      Code for the \citeauthor{linden2013skiplist} priority queue \cite{linden2013skiplist}
      is provided by the authors under an open source license\footnote{\url{http://user.it.uu.se/~jonli208/priorityqueue}}.
      It is lock-free and uses \citeauthor{fraser2004practical}'s lock-free
      SkipList. The aim of this implementation is to minimize contention in
      calls to \verb|DeleteMin|. We chose $32$ as the \verb|BoundOffset| in order to optimize
      performance on a single socket. A \verb|BoundOffset| of $128$ performed only marginally better
      at high thread counts. The \citeauthor{linden2013skiplist} queue represents
      strict, lock-free and SkipList-based priority queues.
\item \textit{SprayList} (\verb|spray|)
      A relaxed concurrent priority queue based on \citeauthor{fraser2004practical}'s
      SkipList using random walks to spread data accesses
      incurred by \verb|DeleteMin| calls. Code provided by \citeauthor{alistarhspraylist} is
      available on Github\footnote{\url{https://github.com/jkopinsky/SprayList}}. Unfortunately,
      the implementation seems to be fairly unstable and data will be omitted for instances
      in which the SprayList crashes.
\item \textit{Multiqueues} (\verb|multiq|)
      Another recent relaxed concurrent priority queue
      design by \citeauthor{rihani2014multiqueues}. Contrary to the SprayList, the
      Multiqueue implementation is lock-based. Since code for the SprayList is
      not publically available, we use our own reimplementation for benchmarks.
\item \textit{Pheet \klsm} (\verb|pheet16, pheet128, ...|)
      \citeauthor{wimmer2015lock}'s original implementation
      of the \klsm within the Pheet task scheduling framework is used mostly to
      verify the behavior of the standalone reimplementation. We used a range
      of values of $k$ to vary between fairly strict ($k = 16$) to fairly relaxed
      ($k = 4096$) behavior. Unlike the other priority queues, the Pheet \klsm
      was measured using the benchmarks integrated into Pheet. The code
      was retrieved from their Launchpad site\footnote{\url{http://www.pheet.org}}.
\item \textit{Standalone \klsm} (\verb|klsm16, klsm128, ...|)
      The standalone \klsm
      reimplementation was measured by using a variety of values for the
      relaxation parameter $k$. Both the standalone and Pheet \klsm implementations
      are linearizable, lock-free, and relaxed priority queues.
% TODO: Include the mlsm?
\end{itemize}

Code for the standalone \klsm, as well as the entire benchmarking suite, is available
on Github at \url{https://github.com/schuay/kpqueue}.

\section{Environment and Methodology} \label{sec:methodology}

Three different machines are used to run benchmarks:

\begin{itemize}
\item \lstinline|mars| An 80-core system consisting of 8 Intel Xeon E7-8850
      processors with 10 cores each and 1 TB of RAM. The processors are clocked
      at 2 GHz.
\item \lstinline|saturn| A 48-core machine with 4 AMD Opteron 6168 processors
      with 12 cores each, clocked at 1.9 GHz. \lstinline|saturn| has 125 GB of RAM.
\item \lstinline|ceres| A 64-core SPARCv9-based machine with 4 processors
      of 16 cores each. Cores are clocked at 3.6 GHz and have 8-way hardware
      hyperthreading. \lstinline|ceres| has 1 TB of RAM.
\end{itemize}

All applications are compiled using \verb|gcc| --- version \verb|5.2.1|
on \lstinline|mars| and \lstinline|saturn|, and version \verb|4.8.2| on
\lstinline|ceres|. We use an optimization level of \verb|-O3| and enable
link-time optimizations using \verb|-flto|.

Each benchmark is executed 10 times, and we report on the mean values and
confidence intervals. Priority queues are pre-filled with $10^6$ items
before the benchmark is started.

\section{Results} \label{sec:results}

We first examine results for the random throughput benchmark with uniform
workload (each thread does a roughly equal amount of insertions and deletions)
and uniform key generation.

<<echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
@

\begin{figure}[ht]
\begin{center}
<<fig = TRUE, width = 10, echo = FALSE>>=
pqplot("results/20151112_mars_uni_uni")
@
\end{center}
\caption{Uniform workload, uniform keys on \lstinline|mars|.}
\label{fig:mars_uni_uni}
\end{figure}

\begin{figure}[ht]
\begin{center}
<<fig = TRUE, width = 10, echo = FALSE>>=
pqplot("results/20151112_mars_uni_uni_slow")
@
\end{center}
\caption{Uniform workload, uniform keys on \lstinline|mars| (detail view of slower queues).}
\label{fig:mars_uni_uni_slow}
\end{figure}

Figures \ref{fig:mars_uni_uni} and \ref{fig:mars_uni_uni_slow}
displays the throughput in operations per second
over a range of threads. The \klsm dominates all other priority queues at
medium to high relaxation ($k \in \{ 128, 256, 4096 \}$), reaching over 200
\ac{MOps} and scaling up to 80 threads. The \klsm's scalability heavily depends on the value
of $k$; at low values the \klsm behaves similar to other concurrent priority
queues (see Figure \ref{fig:mars_uni_uni_slow} for a detail view of slower queues).
At $k = 128$, the \klsm scales well until around 25 threads, and higher values
of $k$ improve scalability up to 50 ($k = 256$) and 80 threads ($k = 4096$).

Multiqueues (\verb|multiq|) perform
the best out of all other data structures, reaching around 30 \ac{MOps} at 75
threads. Although their performance suffers once cores on more than one socket
are used (this occurs at 15 threads in our figure), they slowly recover and keep
scaling until almost the maximal thread count.

The SprayList shows similar (but slower) behavior to the Multiqueues at low thread
counts, but does not scale further beyond 10 threads. As expected, the
\citeauthor{linden2013skiplist} queue performs well while executed on a single
socket, but throughput stays constant at just over 2 \ac{MOps} above 10 threads.
The \verb|globallock| is the highest performer when executed sequentially, but
throughput stays low at around 1 \ac{MOps} at higher thread counts.


\begin{figure}[ht]
\begin{center}
<<fig = TRUE, width = 10, echo = FALSE>>=
pqplot("results/20151112_mars_uni_uni_pheet")
@
\end{center}
\caption{Uniform workload, uniform keys on \lstinline|mars|.
         Comparison of the standalone \klsm against Pheet's \klsm.}
\label{fig:mars_uni_uni_pheet}
\end{figure}

Figure \ref{fig:mars_uni_uni_pheet} compares throughput of our new standalone
\klsm to that of the Pheet \klsm implementation. Interestingly, their behavior
differs significantly, with the Pheet \klsm performing stronger at lower
thread counts but the standalone \klsm scaling better at high concurrency,
and reaching higher absolute performance. It is not necessarily surprising
that the two implementations beahve differently, since the standalone
\klsm has various significant differences in implementation details (e.g.,
pivot calculation based on binary-search, completely segregated \ac{DLSM}
and \ac{SLSM}, \ldots).

\begin{figure}[ht]
\begin{center}
<<fig = TRUE, width = 10, echo = FALSE>>=
pqplot("results/20151112_mars_uni_asc")
@
\end{center}
\caption{Uniform workload, ascending keys on \lstinline|mars|.}
\label{fig:mars_uni_asc}
\end{figure}

Figure \ref{fig:mars_uni_asc} shows results for the throughput benchmark with
a uniform workload and ascending keys. The \klsm performs radically different
to the previously discussed benchmark with uniform key generation in this scenario;
instead of exceptional performance and scalability proportional to the degree
of relaxation, the \klsm's throughput never manages to exceed that of the Multiqueues.
instead, throughput fluctuates between a minimum of roughly 5 \ac{MOps} at
socket boundaries and a maximum of around 12 \ac{MOps} otherwise. While not
shown here, we verified that the Pheet \klsm behaves similarly to the standalone
\klsm.

Multiqueues, the \citeauthor{linden2013skiplist} queue, and the \verb|globallock|
baseline perform similarly to uniform key generation, while the SprayList
crashes and could not be measured.

This result is both surprising and disappointing. Performance counters indicate
that when using uniform key generation, $97\%$ of all deleted items are taken
from the \ac{DLSM} --- and this is obviously the reason for the \klsm's high
performance. But how is this kind of skewed balance between the \ac{SLSM} and the
\ac{DLSM} possibly? Since \lstinline|k_lsm::delete_min| peeks at one item from
the \ac{SLSM} and one from the \ac{DLSM}, wouldn't it be reasonable to expect
a $50/50$ proportion of items taken from the either component?

It turns out that the answer to this question is a definite "no" in the case of
uniform key generation: initially, after prefill is completed, the \klsm
contains a selection of keys randomly distributed over the range of integers.
However, as time goes on, lower keys are removed and the \klsm's
key range becomes skewed towards higher keys. At some point, when most low
keys have been deleted from the \ac{SLSM}, a stable state is reached in which
the global \ac{SLSM} contains mostly old, high keys while the local \acp{DLSM}
contain new, low keys. Items are usually removed from the local \ac{DLSM}, and thus
updates to the global \ac{SLSM} are infrequent; furthermore, since the
best known item within the \ac{SLSM} is cached, most calls to \lstinline|shared_lsm::peek()|
do no actual work.

The ascending key generation benchmark is designed to prevent this situation
in which priority queues behave similarly to a \ac{LIFO} queue. Since the values of inserted
keys rise over time and a min-priority queue always removes the least key,
a relaxed \ac{FIFO}-like behavior results. In the case of the \klsm, items will thus
usually be initially inserted into the local \ac{DLSM}; then moved to the \ac{SLSM}
over time, and finally deleted from the \ac{SLSM} once most lower items have
been removed from the priority queue. Contrary to uniform key generation, emphasis
is placed on the \ac{SLSM} and performance suffers.

\begin{comment}
Example composite figure:

\begin{figure}[ht]
\begin{center}

\begin{minipage}[b]{.5\linewidth}
\centering
<<fig = TRUE, width = 8, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
pqplot("results/20151112_mars_uni_uni")
@
\subcaption{A subfigure}\label{fig:1a}
\end{minipage}%
\begin{minipage}[b]{.5\linewidth}
\centering
<<fig = TRUE, width = 8, echo = FALSE>>=
pqplot("results/20151112_mars_uni_uni")
@
\subcaption{Another subfigure}\label{fig:1b}
\end{minipage}

\begin{minipage}[b]{.5\linewidth}
\centering
<<fig = TRUE, width = 8, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
pqplot("results/20151112_mars_uni_uni")
@
\subcaption{A subfigure}\label{fig:1a}
\end{minipage}%
\begin{minipage}[b]{.5\linewidth}
\centering
<<fig = TRUE, width = 8, echo = FALSE>>=
pqplot("results/20151112_mars_uni_uni")
@
\subcaption{Another subfigure}\label{fig:1b}
\end{minipage}

\end{center}
\caption{Some random data.}
\label{fig:random_data}
\end{figure}

* introductory paragraph(s). roughly what this chapter will cover.
* benchmarks.
* algorithms.
* methodology.
* results - but how to structure these? for the random benchmark, we have different
  types: uniform/split, uniform/ascending; plus there are also quality numbers to present.
  all of these are correlated. perhaps pair quality + throughput side by side?

Unfortunately, we were not able to obtain an implementation of the \citeauthor{shavit2000skiplist}
priority queue for benchmarking. The \citeauthor{wimmer2013data} data structure was omitted in the
following benchmarks since it is tightly coupled with the task-scheduling framework \emph{Pheet},
and cannot directly be compared to other implementations in its current form.

In each test run, the examined priority queue was initially filled with $2^{15}$
elements. We then ran a tight loop of $50\%$ insertions and $50\%$ deletions
for a total of $10$ seconds, where all \lstinline|Insert|
operations within this context implicitly choose a key uniformly at random from
the range of all 32-bit integers. This methodology seems to be the de facto standard for concurrent
priority queue benchmarks
\cite{alistarhspraylist,linden2013skiplist,shavit2000skiplist,sundell2003fast}.
Each run was repeated for a total of $10$ times
and we report on the average throughput.

All benchmarks were compiled with \verb|-O3| using GCC 4.8.2, and executed with threads pinned to
cores. Evaluations took place
on an 80-core Intel Xeon E7-8850 machine with each processor clocked at 2 GHz.
The benchmarking code was adapted from \citeauthor{linden2013skiplist}'s
benchmarking suite and is available at \url{https://github.com/schuay/seminar_in_algorithms}.

\begin{center}
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
pqplot("results/20151112_mars_uni_uni")
@
\end{center}

The \textit{Linden} priority queue emerged as the clear winner of these benchmarks,
with impressive scalability while all threads remain on the same socket. In fact, it is the only
tested implementation to scale to any significant degree; \textit{Heap} achieves minor gains
for up to 3 threads, while all others immediately lose throughput under concurrency.
Performance of the \textit{Linden} queue drops significantly once it is executed on more than 10
threads, thus incurring communication overhead across sockets. This effect is repeated to a lesser
degree every time an additional socket becomes active. However, the \textit{Linden} queue has
clearly superior throughput until the tested maximum of 80 threads.

\textit{GlobalLock} also performs surprisingly well at high concurrency levels.
It remains competitive when compared to \textit{Linden} at about $\frac{2}{3}$ of its throughput.
For single-threaded execution, it achieves the highest throughput of all tested implementations.
It is interesting to note that the \textit{GlobalLock} shows complementary behavior to
\textit{Linden} when new sockets become active; we believe that the additional latency incurred
by cross-socket communications might lead to reduced contention at the single lock.

All remaining implementations behave similarly, displaying very low throughput under concurrency.
This is a surprise in the case of \textit{Noble}; the original benchmarks performed by
\citeauthor{sundell2003fast} led us to expect improvements upon both \textit{GlobalLock}
and \textit{Heap} \cite{sundell2003fast}. However, we are somewhat reassured in our benchmarks by
the fact that \citeauthor{linden2013skiplist}'s results are similar.

The \textit{SprayList} result is even more unexpected, and we believe that it may be caused by
our use of \lstinline|malloc| instead of the included custom lock-free allocator, which we were
unable to execute without errors.

As a final observation we note that even the most efficient strict concurrent priority queue
never exceeds the throughput of the simple sequential heap executed on a single thread by more
than a factor of two. There is hope however since relaxed data structures continue to scale
beyond 10 threads (see the benchmarks performed in \cite{alistarhspraylist} for an example).
\end{comment}
