\chapter{Results \& Discussion} \label{ch:evaluation}

This chapter presents and discusses results for a variety of benchmarks, and executed
on a number of different platforms. We evaluate both the \klsm
together with a selection of other representative parallel priority queues.

Section \ref{sec:benchmarks} covers the different benchmark types we used, while
Section \ref{sec:algorithms} presents the various priority queue algorithms and
Section \ref{sec:methodology} lists our systems and methodology. Results are
finally shown and discussed in Section \ref{sec:results}.

\section{Benchmarks} \label{sec:benchmarks}

Initially when we started work on the \klsm, we relied almost exlusively on
a benchmark which we call the uniformly random throughput benchmark, in which
the priority queue is prefilled with a certain number of elements, and each
thread then performs an equal mixture of insertion/deletion operations.
The operation type is chosen uniformly at random with a probability of $50\%$
each, and key values for insertions are likewise chosen uniformly at random
within the range of 32-bit integers. The benchmark is run for a certain amount
of time, and the number of performed operations is then reported as the resulting
throughput

This type of benchmark is hugely popular within priority queue research and
has been used as the basis for performance evaluations in most publications,
including
\cite{hunt1996efficient,alistarhspraylist,linden2013skiplist,shavit2000skiplist,sundell2003fast,wimmer2015lock,cbpq}.
Its popularity is understandable: the uniform random throughput benchmark is
easy to understand as well as to implement, and it allows for some form of
consistency and comparability between different publications.

However, within the course of this thesis, it has become clear that this benchmark
has a significant drawback, since it causes priority queues degrades to
quasi-\ac{LIFO} queue performance over time and may severely distort obtained
results. This is caused  by the fact that inserted keys are chosen uniformly at random ---
over time, lower keys are removed from the priority queue and its contents become
biased towards higher keys. Newly inserted keys have a high chance of being within
the lowest keys of the queue and quickly becoming a candidate for removal by
\lstinline|delete_min|. Especially with queues having relaxed semantics such as
the SprayList and the \klsm, this can lead to a situation which very closely
approximates the behavior of a \ac{LIFO} queue.

In order to get a better picture of a queue's overall performance, we have introduced
parameters to obtain variations on the random throughput benchmark. The goal
of these parameters was to preserve the aspect of the random throughput benchmark
which caused the data structure to remain at a more or less constant size, allowing
a benchmark to run without resulting in either an empty or an ever-growing queue.

The first introduced parameter is called the \emph{workload}; a balanced workload
performs a roughly equal amount of insertions and deletions on each thread, while
an unbalanced workload performs only insertions on half of all threads, and only
deletions on the other half. The second parameter concerns \emph{key generation},
and we have experimented both with uniform key generation (uniformly at random
within the range of 32-bit integers), and ascending key generation (uniformly at random
within a smaller range that grows over time, i.e. selection from $[0, U] + t$).

Besides the artificial throughput benchmark, we also experimented with a
single source shortest paths benchmark as in \cite{wimmer2015lock}. The graphs
are randomly generated for a given key and node degree. Each thread then repeatedly
pops the queue (containing the next path portion to evaluate), generates successor
subpaths, and adds them to the queue.

Finally, the quality of the produced output is evaluated by using a variant of
the throughput benchmark. Quality in this sense compares the output sequence of
the measured priority queue against a strict sequential queue. Each insertion
and deletion is tagged with a timestamp, which is then used to reconstruct an
approximate linear sequence of insertions and deletions. This sequence is then
replayed using a strict sequential queue, and the rank of each deleted item
within the queue at the time of deletion is recorded (with the rank of the least
item being $1$). Theoretically, strict queues should result in an average rank
of $1$, but in practice we can expect deviations of this value due to 'simultaneous'
operations between threads and imprecise timestamps. However, even though
it is not an exact value, the average rank still provides a good feel for the
quality of a queue's returned results.

\section{Algorithms} \label{sec:algorithms}
\section{Systems and Methodology} \label{sec:methodology}
\section{Results} \label{sec:results}

\begin{comment}
* introductory paragraph(s). roughly what this chapter will cover.
* benchmarks.
* algorithms.
* methodology.
* results - but how to structure these? for the random benchmark, we have different
  types: uniform/split, uniform/ascending; plus there are also quality numbers to present.
  all of these are correlated. perhaps pair quality + throughput side by side?

In this section, we compare the performance of several different concurrent priority
queue implementations:

\begin{itemize}
\item \textit{GlobalLock} An instance of the \lstinline|std::priority_queue<T>| class provided
      by the C++ standard library, protected by a single global lock.
\item \textit{Heap} \citeauthor{hunt1996efficient} provide an implementation
      of their Heap-based design \cite{hunt1996efficient} at \url{ftp://ftp.cs.rochester.edu/pub/packages/concurrent_heap/}.
      However, as the original code was written for the MIPS architecture, we chose to
      use the alternative implementation in \emph{libcds}\footnote{\url{http://sourceforge.net/p/libcds/code/}}
      instead. The heap capacity was $2^{18}$ --- lower values led to a quasi-deadlock situation under high concurrency
      as described in \cite{dragicevic2008survey}.
\item \textit{Noble} An implementation of the \citeauthor{sundell2003fast} priority queue \cite{sundell2003fast}
      is available in the Noble library\footnote{\url{http://www.non-blocking.com/Eng/download-demo.aspx}}.
      \textit{Noble} is lock-free, based on SkipLists, and limited to unique priorities.
      According to the authors, the commercial Noble library includes a more efficient
      version of this data structure that can also handle duplicate priority values.
\item \textit{Linden} Code for the \citeauthor{linden2013skiplist} priority queue \cite{linden2013skiplist}
      is provided by the authors under an open source license\footnote{\url{http://user.it.uu.se/~jonli208/priorityqueue}}.
      It is lock-free and uses \citeauthor{fraser2004practical}'s lock-free
      SkipList. The aim of this implementation is to minimize contention in
      calls to \lstinline|DeleteMin|. We chose $32$ as the \lstinline|BoundOffset| in order to optimize
      performance on a single socket. A \lstinline|BoundOffset| of $128$ performed only marginally better
      at high thread counts.
\item \textit{SprayList} A relaxed concurrent priority queue based on \citeauthor{fraser2004practical}'s
      SkipList using random walks to spread data accesses
      incurred by \lstinline|DeleteMin| calls. Code provided by \citeauthor{alistarhspraylist} is
      available on Github\footnote{\url{https://github.com/jkopinsky/SprayList}}.
\end{itemize}

Unfortunately, we were not able to obtain an implementation of the \citeauthor{shavit2000skiplist}
priority queue for benchmarking. The \citeauthor{wimmer2013data} data structure was omitted in the
following benchmarks since it is tightly coupled with the task-scheduling framework \emph{Pheet},
and cannot directly be compared to other implementations in its current form.

In each test run, the examined priority queue was initially filled with $2^{15}$
elements. We then ran a tight loop of $50\%$ insertions and $50\%$ deletions
for a total of $10$ seconds, where all \lstinline|Insert|
operations within this context implicitly choose a key uniformly at random from
the range of all 32-bit integers. This methodology seems to be the de facto standard for concurrent
priority queue benchmarks
\cite{alistarhspraylist,linden2013skiplist,shavit2000skiplist,sundell2003fast}.
Each run was repeated for a total of $10$ times
and we report on the average throughput.

All benchmarks were compiled with \verb|-O3| using GCC 4.8.2, and executed with threads pinned to
cores. Evaluations took place
on an 80-core Intel Xeon E7-8850 machine with each processor clocked at 2 GHz.
The benchmarking code was adapted from \citeauthor{linden2013skiplist}'s
benchmarking suite and is available at \url{https://github.com/schuay/seminar_in_algorithms}.

\begin{center}
<<results = tex, echo = FALSE>>=
\SweaveInput{pqplot.Rnw}
pqplot("results/random.csv")
@
\end{center}

The \textit{Linden} priority queue emerged as the clear winner of these benchmarks,
with impressive scalability while all threads remain on the same socket. In fact, it is the only
tested implementation to scale to any significant degree; \textit{Heap} achieves minor gains
for up to 3 threads, while all others immediately lose throughput under concurrency.
Performance of the \textit{Linden} queue drops significantly once it is executed on more than 10
threads, thus incurring communication overhead across sockets. This effect is repeated to a lesser
degree every time an additional socket becomes active. However, the \textit{Linden} queue has
clearly superior throughput until the tested maximum of 80 threads.

\textit{GlobalLock} also performs surprisingly well at high concurrency levels.
It remains competitive when compared to \textit{Linden} at about $\frac{2}{3}$ of its throughput.
For single-threaded execution, it achieves the highest throughput of all tested implementations.
It is interesting to note that the \textit{GlobalLock} shows complementary behavior to
\textit{Linden} when new sockets become active; we believe that the additional latency incurred
by cross-socket communications might lead to reduced contention at the single lock.

All remaining implementations behave similarly, displaying very low throughput under concurrency.
This is a surprise in the case of \textit{Noble}; the original benchmarks performed by
\citeauthor{sundell2003fast} led us to expect improvements upon both \textit{GlobalLock}
and \textit{Heap} \cite{sundell2003fast}. However, we are somewhat reassured in our benchmarks by
the fact that \citeauthor{linden2013skiplist}'s results are similar.

The \textit{SprayList} result is even more unexpected, and we believe that it may be caused by
our use of \lstinline|malloc| instead of the included custom lock-free allocator, which we were
unable to execute without errors.

As a final observation we note that even the most efficient strict concurrent priority queue
never exceeds the throughput of the simple sequential heap executed on a single thread by more
than a factor of two. There is hope however since relaxed data structures continue to scale
beyond 10 threads (see the benchmarks performed in \cite{alistarhspraylist} for an example).
\end{comment}
