\chapter{Implementation of the $k$-LSM Priority Queue} \label{ch:implementation}

Contrary to the previous chapter in which the concept of the \klsm was briefly
described, this chapter will cover the implementation details of the new standalone
\klsm.

But first, a rationale --- why is a standalone \klsm implementation desirable?
There are several reasons:

\begin{itemize}
\item Comparability. The original \klsm implementation within the task-scheduling
      framework Pheet \cite{wimmer2013data,wimmer2015lock} 
      (simply referred to as the \emph{Pheet \klsm} within this
      thesis) has been demonstrated to perform very well; but how well exactly
      when compared to other state of the art queues is not easy to determine
      since the Pheet \klsm implementation is tied tightly to the Pheet framework.
      A standalone \klsm will allow other for easy comparability against other
      queues and answer how its performance compares to the state of the art.
\item Use in practice. A standalone \klsm can easily be integrated into applications
      for use in practice.
\item Maintainability. The Pheet \klsm implementation is focused on maximal performance
      and makes tradeoffs that pose challenges to maintainability. While the standalone
      \klsm also aims to achieve high performance, the reimplementation also 
      explicitly values classical software engineering concepts such as 
      composability, readability, and separation of concerns \cite{hunt2000pragmatic}.
\item Scientific reproducability. The standalone \klsm verifies performance
      claims by \citeauthor{wimmerphd}.
\item Further insight and improvements. Finally, a reimplementation by a fresh
      set of eyes can create new insight into the design and might result in
      improvements to further increase performance over the original Pheet \klsm.
\end{itemize}

The standalone \klsm (referred to as the \emph{\klsm} in the remaining chapter)
is implemented in C++11 using a minimal set of dependencies on other libraries.
The included benchmark application requires the \emph{hwloc} library for hardware-independent
thread pinning, while other priority queues included for benchmarking purposes
require the \ac{GSL}. The \klsm implementation itself has no external
dependencies and uses standard C++11 atomics, and is therefore highly portable.

Unlike the Pheet \klsm, our implementation does not guarantee locality, i.e., it
may occur that a thread skips items it has previously inserted itself. While
locality may in some cases be desirable in practise, it was irrelevant for our
experiments and omitted in our implementation to lower code complexity.
All benchmarks of the Pheet \klsm
also run with the locality option disabled to ensure comparability.

Full code for the \klsm implementation is available at 
\url{https://github.com/schuay/kpqueue}. The directory structure is as follows:
\lstinline|doc/| contains autogenerated documentation in the
Doxygen\footnote{\url{http://www.stack.nl/~dimitri/doxygen/}, last visited on
December 9\textsuperscript{th}, 2015.} format, \lstinline|lib/| contains external
source code for other priority queues used
for benchmarking, \lstinline|misc/| contains various utility scripts which handle
conversion of raw benchmarking output from various formats into plots using
R\footnote{\url{https://www.r-project.org/}, last visited on December 9\textsuperscript{th}, 2015.},
\lstinline|src/| contains code for the standalone \klsm and various benchmarking
tools, and \lstinline|test/| contains the test suite.

The following sections describe the \klsm implementation in top-down order ---
Section \ref{sec:klsm_internals} begins with the \klsm itself, Sections \ref{sec:slsm_internals}
and \ref{sec:dlsm_internals} descend to the component queues \ac{SLSM} and \ac{DLSM},
while several other components are covered in Section \ref{sec:component_internals}.
The various applied methods of lock-free memory management are covered in Section \ref{sec:mm_internals}.

\section{\klsm Internals} \label{sec:klsm_internals}

Figure \ref{fig:klsm_structure} shows an overview of the class inter-class connections
involved in the \klsm design. The \lstinline|k_lsm| itself simply holds a \ac{DLSM} and \ac{SLSM} instance
and delegates most work to these lower-level designs. Details of the \lstinline|k_lsm|
are covered in this section. The \lstinline|dist_lsm| (Section \ref{sec:dlsm_internals})
consists of a linked list of \lstinline|block| instances, which in turn contain
several \lstinline|item| instances (Section \ref{sec:component_internals}).
The \lstinline|shared_lsm| organizes blocks using a global \lstinline|block_array|,
which also contains an \lstinline|block_pivot| instance (Section \ref{sec:slsm_internals}).
Memory management is handled by several classes, covered further in Section \ref{sec:mm_internals}.

\begin{figure}
\centering
    \begin{tikzpicture}[start chain,
        ->,
        every node/.style={font = \small},
        label/.style={rectangle,minimum size = 5mm}
      ]

        \begin{scope}[start chain = 1 going {below=of \tikzchainprevious.south},
                every on chain/.style = {anchor = north},
                node distance = -0.15mm]
            \node[on chain, box] (klsm) {\verb|k_lsm|};
            \node[on chain, yshift = -5mm] (abpt) {\verb+aligned_block_ptr+};
            \node[on chain] (bpool) {\verb+block_pool+};
            \node[on chain] (italloc) {\verb|item_allocator|};
            \node[on chain] (bstorage) {\verb|block_storage|};
            \node[on chain] (mm) {\emph{\small Memory Management}};
            \node[on chain, box, yshift = -5mm] (block) {\verb|block|};
            \node[on chain, box, yshift = -5mm] (item) {\verb|item|};
        \end{scope}

        \node[box, left = of italloc] (dlsm) {\verb|dist_lsm|};
        \node[box, right = of italloc] (slsm) {\verb|shared_lsm|};

        \node[box, below = of slsm] (barray) {\verb|block_array|};
        \node[box, below = of barray] (bpivots) {\verb|block_pivots|};

        \begin{pgfonlayer}{background}
            \draw[box] ($(abpt.north west) - (2mm, 0)$) rectangle ($(mm.south east) + (1mm, 0)$);
        \end{pgfonlayer}

        \path[->, every node/.style= { font = \sffamily\small }]
            (klsm.west) edge[bend right = 30] node [right] {} (dlsm)
            (klsm.east) edge[bend left = 30] node [right] {} (slsm)
            (dlsm.east) edge[bend right = 10] node [right] {} (italloc.west)
            (dlsm.east) edge[bend right = 10] node [right] {} (bstorage.west)
            (slsm.west) edge[bend left = 10] node [right] {} (abpt.east)
            (slsm.west) edge[bend left = 10] node [right] {} (bpool.east)
            (slsm.west) edge[bend left = 10] node [right] {} (italloc.east)
            (slsm.south) edge node [right] {} (barray.north)
            (barray.west) edge[bend left = 10] node [right] {} (block.east)
            (barray.south) edge node [right] {} (bpivots.north)
            (dlsm.south) edge[bend right = 30] node [right] {} (block.west)
            (block.south) edge node [right] {} (item.north);
    \end{tikzpicture}
\caption{Overview of the \lstinline|k_lsm| class structure.}
\label{fig:klsm_structure}
\end{figure}

While the general \klsm interface has been covered previously in Section \ref{sec:wimmer},
several details have been omitted for clarity. Figure \ref{fig:klsm.h} displays the full
header file containing the \lstinline|k_lsm| class. In this instance, implementation
details such constructors and destructors, as the \lstinline|init_thread| function,
are included in order to present a complete picture of what a class implementation
looks like. Future code listings will skip these details.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class k_lsm {
public:
    k_lsm();
    virtual ~k_lsm() { }

    void insert(const K &key);
    void insert(const K &key,
                const V &val);

    bool delete_min(V &val);

    void init_thread(const size_t) const { }
    constexpr static bool supports_concurrency() { return true; }

private:
    dist_lsm<K, V, Rlx>   m_dist;
    shared_lsm<K, V, Rlx> m_shared;
};

#include "k_lsm_inl.h"
\end{lstlisting}
\caption{The \klsm header.}
\label{fig:klsm.h}
\end{figure}

The \lstinline|k_lsm| class is a template class parameterized by \lstinline|K|
and \lstinline|V|, respectively the key- and value types, as well as the relaxation
parameter \lstinline|Rlx|, which corresponds to the $k$ in \klsm. Code for template class
must be exclusively located in header files, but it is possibly to preserve readability
using a pattern of inlined headers: the main header includes a class declaration,
while an inline header contains the class definition and is itself 
included in the standard header. The \lstinline|init_thread| function is provided
for queues which require per-thread initialization.

Trivialities aside, implementation of the \lstinline|k_lsm| class is extremely simple since it relies
on a composition of the \ac{SLSM} and \ac{DLSM} queues. Insertion
simply triggers insertion into the local \ac{DLSM}, passing a pointer to the global
\ac{SLSM} queue in case the maximal size of local blocks is exceeded.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
bool
k_lsm<K, V, Rlx>::delete_min(V &val)
{
    typename block<K, V>::peek_t
            best_dist = block<K, V>::peek_t::EMPTY(),
            best_shared = block<K, V>::peek_t::EMPTY();

    do {
        m_dist.find_min(best_dist);
        m_shared.find_min(best_shared);

        if (!best_dist.empty() && !best_shared.empty()) {
            if (best_dist.m_key < best_shared.m_key) {
                return best_dist.take(val);
            } else {
                return best_shared.take(val);
            }
        }

        if (!best_dist.empty() /* and best_shared is empty */) {
            return best_dist.take(val);
        }

        if (!best_shared.empty() /* and best_dist is empty */) {
            return best_shared.take(val);
        }
    } while (m_dist.spy() > 0);

    return false;
}
\end{lstlisting}
\caption{The \lstinline|k_lsm::delete_min| implementation.}
\label{fig:k_lsm::delete_min}
\end{figure}

Deletions (Figure \ref{fig:k_lsm::delete_min}) call \lstinline|find_min| 
on both the \ac{SLSM} and \ac{DLSM} queues
and return the lesser of both items in the standard case. In case both queues
are empty, the local \ac{DLSM} attempts to copy items from another thread by
triggering a call to \lstinline|spy|, and finally retrying deletion.

These implementations seem simple; but that is because details are
delegated to responsible classes. The best example in this case is the act of
item removal --- how can we ensure that an item is taken only by a single thread
and that no items are lost?
And since memory for items is reused (see Section \ref{sec:mm_internals}),
how can we prevent the ABA problem \cite{michael2004aba} in which memory is reused after being
retrieved (i.e. the thread reads data different than had been intended)?

The answer lies within the \lstinline|peek_t| class, which contains both
a pointer to the item and an associated expected item version. Within \lstinline|peek_t::take|,
a \ac{CAS} instruction is used to atomically increment the item's version, if,
and only if it previously matched the given expected version. In the presence
of two identical \ac{CAS} calls by different threads, only one will succeed
while the other must fail. The thread which has seen a \ac{CAS} failure will
return from from \lstinline|delete_min| with a so-called spurious failure,
returning \lstinline|false| even though the queue might be non-empty.

\subsection{Linearizability, Lock-freedom \& Complexity}

The \klsm is lock-free and linearizable, and both insertions as well as
deletions have an amortized complexity of $O(\log n)$ (complexities are analyzed
by ignoring artifacts such as retries caused by concurrent operations).
We do not rigorously
prove these statements but attempt to give a rough sketch for each:
\begin{itemize}
\item \emph{Linearizability}. A successful \lstinline|delete_min| is linearizable at the point
      when the \ac{SLSM} has verified its local copy of the global array for the last time.
      Unsuccessful deletes have their linearization point upon failure of the \ac{CAS}
      call on an item's version within \lstinline|item::take|.
      \lstinline|insert|
      is linearizable at the linearization points of \lstinline|shared_lsm::insert|
      and \lstinline|dist_lsm::insert|, which will be discussed in the following
      sections.
\item \emph{Lock-freedom}. Assuming the corresponding operations on the \ac{SLSM}
      and \ac{DLSM} are lock-free, \lstinline|k_lsm::insert| is obviously lock-free
      since it merely calls \lstinline|dist_lsm::insert|. To show that
      \lstinline|delete_min| is lock-free, consider Figure \ref{fig:k_lsm::delete_min}.
      As long as either
      the \ac{SLSM} or \ac{DLSM} are non-empty, the loop body is executed exactly once.
      If, on the other hand,
      both are empty, then \lstinline|spy| attempts to copy items from another
      local \ac{DLSM}. If no items are copied, the \lstinline|k_lsm::delete_min|
      method returns; otherwise, the loop body is re-executed. If it is,
      note that the local \ac{DLSM} must be nonempty, as we have just copied items from
      another thread. There are now
      two possibilities: either the second loop iteration successfully finds
      an item and returns; or we again find both the \ac{DLSM} and \ac{SLSM}
      empty --- but if that is the case, then other threads must have made
      progress in the meantime and deleted all items which we have spied at the
      end of the previous iteration. The \lstinline|delete_min| operation is thus lock-free.
\item \emph{Complexity}. Assuming the corresponding operations on the \ac{SLSM}
      and \ac{DLSM} have amortized logarithmic complexity, then \lstinline|k_lsm::insert|
      and \lstinline|k_lsm::delete_min| are also in amortized $O(\log n)$. Insertions
      consist simply of calls to \lstinline|dist_lsm::insert|. Deletions
      find a minimal element in both the \ac{SLSM} and \ac{DLSM} and take it in
      constant time, repeating the operation at most once (ignoring retries caused by
      concurrent deletes) when both components are empty.
\end{itemize}

\section{\acl{SLSM} Internals} \label{sec:slsm_internals} 

The \ac{SLSM} is the central, global component of the \klsm and contains
all items that have overflowed the local \ac{DLSM} capacities. The \ac{SLSM}
batches insertions by inserting blocks rather than items, and relaxes deletions
by keeping track of a so-called pivot range, which contains a subset of $k$
smallest items. 

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class shared_lsm {
public:
    void insert(const K &key,
                const V &val);
    void insert(block<K, V> *b);

    bool delete_min(V &val);
    void find_min(typename block<K, V>::peek_t &best);

private:
    versioned_array_ptr<K, V, Rlx> m_global_array;
    thread_local_ptr<shared_lsm_local<K, V, Rlx>> m_local_component;
};
\end{lstlisting}
\caption{The \ac{SLSM} header.}
\label{fig:shared_lsm.h}
\end{figure}

Figure \ref{fig:shared_lsm.h} shows the header file for the \lstinline|shared_lsm|
class. The interface contains the standard insertion and deletion functions since
the \ac{SLSM} may be used as a standalone priority queue. However, it also has
a batched insertion function which takes a block argument, and a \lstinline|find_min|
function used by the \klsm to retrieve one of the smallest items without removing it.

The \ac{SLSM} itself contains a versioned pointer to the global block array, which is
used by all threads to determine the contents of the \ac{SLSM}. A versioned pointer (also known
as tagged pointers) reserves a subset of the available bits which contain a version number,
incremented each time the global block array is modified. The
pointer is versioned since block arrays are heavily reused --- each thread allocates
two block arrays and uses them in an alternating fashion when setting the global
array pointer (see Section \ref{sec:mm_internals}). 

Without versioning, this might
cause issues: consider a case in which thread A retrieves the global array
pointer $ptr$ (which has been allocated by thread B) and then stalls. Thread B then
reuses the array $ptr$ and fills it with new contents. When thread A continues
execution and successfully verifies that the global pointer appears to be unchanged, 
$ptr$ contains different content than A expects - this is an instance
of the ABA problem.

Versioning is used to solve several aspects of this issue. First, the block array
itself contains an integral version number which may compared with a local copy
the check whether the array has remained unchanged. Second, a portion of the version
number (more precisely, the least 11 bits) is packed into the global array pointer
itself to ensure valid results of \ac{CAS} operations when setting the global pointer.
The class \lstinline|versioned_array_ptr| handles operations on such packed pointers,
and itself contains an \lstinline|aligned_block_array|, which is simply a block
array allocated at a 2048 byte boundary (i.e., the least 11 bits are zeroed).

The thread-local \ac{SLSM} component is wrapped within the \lstinline|thread_local_ptr|
class (see Section \ref{sec:component_internals}), which allows lock-free retrieval
of a thread-local instance of the given class. Contrary to the standard thread-local
mechanisms provided by e.g. \ac{pthreads} (\lstinline|pthread_getspecific|) or
C++ (the \lstinline|thread_local| keyword), \lstinline|thread_local_ptr| allows
usage as a class member, and access to other threads' local instances.

The local \ac{SLSM} component, \lstinline|shared_lsm_local|, contains most
of the vital logic, and its interface is presented in Figure \ref{fig:shared_lsm_local.h}.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class shared_lsm_local {
public:
    void insert(block<K, V> *b,
                versioned_array_ptr<K, V, Rlx> &global_array);

    void peek(typename block<K, V>::peek_t &best,
              versioned_array_ptr<K, V, Rlx> &global_array);

private:
    typename block<K, V>::peek_t m_cached_best;

    /** A copy of the global block array, updated regularly. */
    block_array<K, V, Rlx> m_local_array_copy;

    /** Memory management. */
    item_allocator<item<K, V>, typename item<K, V>::reuse> m_item_pool;
    block_pool<K, V> m_block_pool;
    aligned_block_array<K, V, Rlx> m_array_pool_odds;
    aligned_block_array<K, V, Rlx> m_array_pool_evens;
};
\end{lstlisting}
\caption{The header of the local \ac{SLSM} component.}
\label{fig:shared_lsm_local.h}
\end{figure}

Members include pools for memory management of several classes, which are covered
in further depth in Section \ref{sec:mm_internals}. Furthermore, a cached copy of
the item previously returned from \lstinline|peek| (the local equivalent to \lstinline|shared_lsm::find_min|)
is kept in order to avoid useless repeated lookups. If, for instance, the \klsm
chooses the item returned by the \ac{DLSM} to return from \lstinline|k_lsm::delete_min|,
then a subsequent call to \lstinline|shared_lsm::find_min| (and thus \lstinline|shared_lsm_local::peek|)
may simply return the cached item if the global block array has remained unchanged.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
shared_lsm_local<K, V, Rlx>::insert_block(
        block<K, V> *b,
        versioned_array_ptr<K, V, Rlx> &global_array)
{
    while (true) {
        /* Fetch a consistent copy of the global array. */

        block_array<K, V, Rlx> *observed_packed;
        version_t observed_version;
        refresh_local_array_copy(observed_packed, observed_version, global_array);(@*\label{ln:shared_lsm_local::insert_block.refresh}*@)

        /* Create a new version which we will attempt to push globally. */

        auto &new_blocks = /* A local pooled instance. */
        auto new_blocks_ptr = new_blocks.ptr();
        new_blocks_ptr->copy_from(&m_local_array_copy);
        new_blocks_ptr->increment_version();
        new_blocks_ptr->insert(b, &m_block_pool);

        /* Try to update the global array. */

        if (global_array.compare_exchange_strong(observed_packed,(@* \label{ln:shared_lsm_local::insert_block.cas} *@)
                                                 new_blocks)) {
            break;
        }
    }
}
\end{lstlisting}
\caption{The \lstinline|shared_lsm_local::insert_block| implementation.}
\label{fig:shared_lsm_local::insert_block}
\end{figure}

Insertions are ultimately handled by \lstinline|insert_block|, shown in
Figure \ref{fig:shared_lsm_local::insert_block}. We skip over
single-item insertions since we are mostly concerned with the \ac{SLSM} in the
context of the \klsm (but essentially, they simply create a singleton block
and insert it).

Block insertions initially ensure that the local
array copy (a shallow copy of the global array, containing only pointers to the
original blocks) is up to date by retrieving
the current global block array and comparing versions, then updating the copy
if necessary. It is then possible to access the local copy without fear of
concurrent modifications (as long as one is careful when reading blocks
themselves).

A new block array copy is then constructed from the local array copy, its
version is incremented,
and the given block is inserted into it (details in Section \ref{ssec:block_array_internals}).

Finally, the global array pointer is updated using a \ac{CAS} operation. The
update fails if the global array has changed since it has been last read by this
thread, in which case the insertion is retried. Otherwise, the insertion
is successful. The linearization point for insertions into the \ac{SLSM}
is upon a successful \ac{CAS} in line \ref{ln:shared_lsm_local::insert_block.cas}.

It is important to realize that any work done within this function prior to a
failed \ac{CAS} is wasted work. Insertions into the block array copy trigger
expensive merges and other maintenance, and we must attempt to minimize
such failures as much as possible. Using higher values of $k$, i.e. batching
more items together during insertions is one way of reducing \ac{CAS} failures.

Experiments using performance counters show that on average, there are $0.75$
retries per insertion when running a simple throughput benchmark with $k = 256$
on 35 threads,
and $2.95$ retries per insertion when running on 80 threads. As expected, absolute
performance is higher with 35 threads, resulting in around $15\%$ more completed
insertions.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
shared_lsm_local<K, V, Rlx>::peek(
        typename block<K, V>::peek_t &best,
        versioned_array_ptr<K, V, Rlx> &global_array)
{
    if (local_array_copy_is_fresh(global_array)
            && !m_cached_best.empty()
            && !m_cached_best.taken()) {
        best = m_cached_best;
        return;
    }

    block_array<K, V, Rlx> *observed_packed;
    version_t observed_version;

    do {
        refresh_local_array_copy(observed_packed,(@* \label{ln:shared_lsm_local::peek.refresh} *@)
                                 observed_version,
                                 global_array);
        best = m_cached_best = m_local_array_copy.peek();
    } while (global_array.load()->version() != observed_version);(@* \label{ln:shared_lsm_local::peek.version} *@)
}
\end{lstlisting}
\caption{The \lstinline|shared_lsm_local::peek| implementation.}
\label{fig:shared_lsm_local::peek}
\end{figure}

Deletions (Figure \ref{fig:shared_lsm_local::peek}) follow a similar pattern:
the local copy of the global block array is refreshed, and a peek operation
is then performed on the array copy. The process is repeated if the global array
has changed since the local copy has last been updated.

As previously mentioned, if there is a cached item from a previous call to
\lstinline|peek| that has not yet been taken, and the global array has not changed
in the meantime, then the cached item can simply be returned without any additional
work.

Again, performance counters indicate that in the standard uniform
throughput benchmark (see also Chapter \ref{ch:evaluation}),
the \ac{SLSM} item cache is hit very frequently. On 35 threads,
$95\%$ of all \lstinline|peek| calls simply hit the cache and return without
doing further work. Of the remaining calls that do real work, only $2\%$ result
in a retry.

These numbers show that the \ac{SLSM} item cache is very effective; however,
the high number of cache hits also imply that almost all items are actually
taken from the \ac{DLSM} during the uniform throughput benchmark --- this might
be a symptom of a problem with the benchmark itself, as we will see in the next
chapter. The low frequency of retries show that peeks are relatively inexpensive,
since they are rarely interrupted by a concurrent insertion.

\subsection{Block Arrays} \label{ssec:block_array_internals}

Block arrays are the lowest level of the \ac{SLSM} and represent the
actual \ac{LSM}. The \lstinline|block_array| class (Figure \ref{fig:block_array.h})
contains an array of blocks
conforming to the \ac{LSM} invariants at the boundaries of functions (see Section \ref{ssec:lsm}).

\begin{figure}
\centering
    \begin{tikzpicture}[start chain,
        ->,
        every node/.style={font = \small},
        label/.style={rectangle,minimum size = 5mm}]

        \begin{scope}[start chain = 1 going right, node distance = -0.15mm, every node/.style = box]
            \node [on chain=1, minimum width = 10mm] (ba0) {$b_0$};
            \node [on chain=1, minimum width = 10mm] (ba1) {$b_1$};
            \node [on chain=1, minimum width = 10mm] (ba2) {$b_2$};
            \node [on chain=1, minimum width = 10mm] (ba3) {$b_3$};
            \node [on chain=1, minimum width = 10mm] (ba4) {$b_4$};
        \end{scope}

        \begin{scope}[start chain = 2 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm]
            \node[on chain, box, above = of ba0, yshift = 3mm] (00) {0};
            \node[on chain, box] {3};
            \node[on chain, deleted_box] {4};
            \node[on chain, box] (03) {4};
            \node[on chain, box] {6};
            \node[on chain, box] {9};
            \node[on chain, box] {};
            \node[on chain, box] (07) {};
        \end{scope}

        \begin{scope}[start chain = 3 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm]
            \node[on chain, deleted_box, above = of ba1, yshift = 3mm] (10) {0};
            \node[on chain, box] (11) {0};
            \node[on chain, deleted_box] {2};
            \node[on chain, box] (13) {3};
        \end{scope}

        \begin{scope}[start chain = 4 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm, every node/.style = box]
            \node[on chain, box, above = of ba2, yshift = 3mm] (20) {5};
            \node[on chain, box] {8};
        \end{scope}

        \begin{scope}[start chain = 5 going {above=of \tikzchainprevious.north},
                every on chain/.style={anchor=south},
                node distance = -0.15mm, every node/.style = box]
            \node[on chain, box, above = of ba3, yshift = 3mm] (30) {2};
        \end{scope}

        \path[->, every node/.style= { font = \sffamily\small }]
        (ba0) edge node [right] {} (00)
        (ba1) edge node [right] {} (10)
        (ba2) edge node [right] {} (20)
        (ba3) edge node [right] {} (30);


        \draw[serif cm-serif cm] ($(00.south west) - (0.75mm,0)$) -- node[sloped, above] {$p_0$}
                                 ($(03.north west) - (0.75mm,0)$);
        \draw[serif cm-serif cm] ($(11.south west) - (0.75mm,0)$) -- node[sloped, above] {$p_1$}
                                 ($(13.north west) - (0.75mm,0)$);
        \draw[serif cm-serif cm] ($(20.south west) - (0.75mm,0)$) -- node[sloped, above] {$p_2$}
                                 ($(20.north west) - (0.75mm,0)$);
        \draw[serif cm-serif cm] ($(30.south west) - (0.75mm,0)$) -- node[sloped, above] {$p_3$}
                                 ($(30.north west) - (0.75mm,0)$);

        \draw[serif cm-serif cm] ($(07.north west) + (0, 1mm)$) -- node[sloped, above] {Blocks}
                                 ($(07.north west) + (45mm,1mm)$);

        \draw[serif cm-serif cm] ($(ba0.south west) - (0, 1mm)$) -- node[below] {Block Array}
                                 ($(ba4.south east) - (0, 1mm)$);
    \end{tikzpicture}
\caption[Schematic block array structure]
        {Schematic block array structure containing blocks $b_i$ and pivot ranges $p_i$, $i \in [1, 4]$.
         Items shown in white have been deleted.}
\label{fig:block_array_structure}
\end{figure}

The \ac{LSM} is represented through member variables holding the block pointer
array together with its size. Additionally, block arrays contain pivot ranges
(storing ranges within the blocks which contain a subset of the $k$ smallest items),
a version (to avoid the ABA problem), and an efficient random number generator.

An example block array is shown in Figure \ref{fig:block_array_structure}. It contains
four blocks ($b_{0 \ldots 3}$), each with an associated pivot range ($p_{0 \ldots 3}$)
containing the 7 minimal items within the block array. Some items have been deleted,
and as shown it is possible for these items to be either within or before a pivot range (never after).
Deleted items within a pivot range cause so-called pivot fragmentation.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class block_array {
public:
    /** May only be called when this block is not visible to other threads. */
    void insert(block<K, V> *block,
                block_pool<K, V> *pool);

    /** Callable from other threads. */
    typename block<K, V>::peek_t peek();

private:
    block<K, V> *m_blocks[MAX_BLOCKS];
    size_t m_size;

    block_pivots<K, V, Rlx, MAX_BLOCKS> m_pivots;

    std::atomic<version_t> m_version;

    xorshf96 m_gen;
};
\end{lstlisting}
\caption{The block array header.}
\label{fig:block_array.h}
\end{figure}

During insertions, a block is first inserted into the block array such that it remains
ordered by capacity. Then, while the block array contains multiple blocks
of the same capacity, it is merged with its predecessor block until all \ac{LSM}
invariants are satisfied. The block array is then compacted: blocks which are less
than half-filled are shrunk (possibly triggering other merges), and empty blocks are removed.

Finally, pivot ranges are updated (if possible) or recalculated (if necessary).
Most pivot range modifications are incremental updates --- when a range is
less than half-filled (i.e., $\text{range size } < \frac{k}{2}$), the range
is grown from its current state. On the other hand, if new range exceeds its upper
bounds ($\text{range size } > k$), a completely new pivot range is determined.

% TODO: The figure might be redundant.
\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
typename block<K, V>::peek_t
block_array<K, V, Rlx>::peek()
{
    while (true) {
        /* Not shown: pivot range maintenance. */
        int selected_element = m_gen() % m_pivots.count(m_size);

        size_t block_ix;
        block<K, V> *b = nullptr;
        const typename block<K,  V>::block_item *best = nullptr;
        for (block_ix = 0; block_ix < m_size; block_ix++) {
            const int elems_in_range = m_pivots.count_in(block_ix);

            if (selected_element >= elems_in_range) {
                /* Element not in this block. */
                selected_element -= elems_in_range;
                continue;
            }

            b = m_blocks[block_ix];
            best = b->peek_nth(m_pivots.nth_ix_in(selected_element, block_ix));

            break;
        }

        if (!best->taken()) {
            ret = *best;
            return ret;
        } else {
            /* Not shown: pivot range maintenance. */
        }
    }
}
\end{lstlisting}
\caption{The partial \lstinline|block_array::peek| implementation.}
\label{fig:block_array::peek}
\end{figure}

% TODO: Consistency: find_min / peek.

Peeks, shown in Figure \ref{fig:block_array::peek}, use the pivot range
to determine a random item within the $k$ least items in the block array.
This is done as follows: for each block within the block array, pivots store
a range of items guaranteed to be within the $k$ least items of the array.
An item within this range is picked at random, and returned if it has not yet
been taken; otherwise, pivot range maintenance is performed and the operation
is repeated.

As it turns out, representing pivots as contiguous ranges within each block
might not be sufficient to maintain maximal performance. Experiments
show that for every successful peek, $3.5$ failed peeks must be performed,
i.e. every successful peek iterates through the loop almost 5 times on average.

This effect is caused by fragmentation of the pivot range, which has detrimental
effects both on item selection (through random selection of a taken item)
and relaxation (since the pivot range actually contains fewer elements than
its size, calculated through pivot range boundaries, implies). In Figure \ref{fig:block_array_structure},
both $p_0$ and $p_1$ are fragmented, i.e., they contain deleted items. Note
that the initial deleted item in $b_1$ is handled correctly and not included
in the pivot range.

We experimented with an alternative data structure for pivot maintainance
based on a balanced binary interval tree to store taken items for each block.
Each time a thread encountered a taken item, it would be marked in the interval
tree and excluded from further random selections. Even though operations on the tree
were in $O(\log n)$ where $n$ is the number of marked taken elements within the tree,
maintenance overhead turned out to exceed any gains we made by more accurate
pivot management.

\subsection{Block Pivots} \label{ssec:block_pivot_internals}

In more detail, block pivots consist of upper and lower index boundaries
for each block (see Figure \ref{fig:block_pivots.h}). The \lstinline|shrink|
and \lstinline|grow| methods maintain the pivot range.

Care is taken to recalculate the pivot range as infrequently as possible.
Recall that the pivot range is a member variable of the block array
(Figure \ref{fig:block_array.h}); the pivot range is thus calculated once
when the new block is created upon insertion in \lstinline|block_array::insert|,
and then propagated to other threads when local block array copies are updated.
From that point on, threads maintain their own pivot ranges --- however, the next
insertion will create a new global pivot range which all threads then acquire.
In the ideal case, pivot ranges will thus never need to be recalculated by local
threads if insertions are frequent.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx, int MaxBlocks>
class block_pivots {
public:
    size_t shrink(block<K, V> **blocks,
                  const size_t size);
    size_t grow(const int initial_range_size,
                block<K, V> **blocks,
                const size_t size);

private:
    int m_upper[MaxBlocks];
    int m_lower[MaxBlocks];
    K m_maximal_pivot;
};
\end{lstlisting}
\caption{The block pivot header.}
\label{fig:block_pivots.h}
\end{figure}

The original Pheet \klsm grows the pivot range iteratively by starting at the first block,
tentatively picking the next
key beyond the current pivot range as the new upper bound for keys within the range,
and then recalculating the pivot range for each block. If the new pivot range
size is too small, the process is repeated; if it is too large, the pivot range
is reverted to its previous state, and the process is repeated on the next block.

This algorithm can become expensive and cause a large amount of repeated and
wasted work. Instead of iteratively using existing keys, the standalone \klsm
uses binary search to arrive at the desired value of the upper key bound in
logarithmic time. As an additional optimization, the previously used key upper
bound is stored and reused as a lower bound for the binary search in \lstinline|grow|
operations (in which the new upper bound must be higher than its previous value)
and as upper bounds in \lstinline|shrink| calls.

\subsection{Linearizability, Lock-freedom \& Complexity}

The \ac{SLSM} is linearizable, lock-free, and \lstinline|insert| as well
as \lstinline|peek| have an amortized complexity of $O(\log n)$.
\begin{itemize}
\item \emph{Linearizability}. Insertions into the \ac{SLSM} have a single
      linearization point at Line \ref{ln:shared_lsm_local::insert_block.cas}
      in Figure \ref{fig:shared_lsm_local::insert_block}. Once the \ac{CAS}
      operation has successfully set the block, it is visible globally.

      Successful \lstinline|peek| operations are linearized when the local copy of
      the global array is verified for the last time in Line \ref{ln:shared_lsm_local::peek.version}
      of Figure \ref{fig:shared_lsm_local::peek}, while unsuccesful peeks
      are linearized when \lstinline|take| is called on the returned item.
\item \emph{Lock-freedom}. Insertions are lock free: they exclusively use non-blocking
      primitives, and retries are triggered only if another thread has
      made progress (Figure \ref{fig:shared_lsm_local::insert_block}, Lines
      \ref{ln:shared_lsm_local::insert_block.refresh} and \ref{ln:shared_lsm_local::insert_block.cas}).

      It is slightly more difficult to argue lock-freedom for peeks. As in the
      case of insertions, non-blocking primitives are used and retries
      are only triggered by progress of other threads (Figure \ref{fig:shared_lsm_local::peek},
      Lines \ref{ln:shared_lsm_local::peek.refresh} and \ref{ln:shared_lsm_local::peek.version}).
      But what happens if if \lstinline|block_array::peek| picks a taken item?
      Due to readability, code handling that case has been omitted from Figure
      \ref{fig:block_array::peek}; but for each picked taken item,
      we iterate from the beginning of the pivot range of the current block,
      mark encountered taken items as taken (the pivot range size therefore
      decreases by one),
      and return the first nontaken item within the range. If such an item
      does not exist, we backtrack and select another random item within
      the random range. This process is repeated until the \ac{SLSM}
      is determined to be empty (at most $\log n$ times), or an item is found.
\item \emph{Complexity}. Insertions essentially copy the given block ($O(1)$
      per item), refresh the local array and create a copy of it in logarithmic
      time; finally, the given block is inserted, possibly triggering a number
      of merge operations. These merges are of amortized logarithmic complexity:
      intuitively, note that any item in an array of size $2^l$ has been merged
      at most $l$ times, and each merge incurs constant overhead per item.
      Insertions are thus in amortized $O(\log n)$.
      % TODO: Shrinks?

      Peeks create a copy of the global array in logarithmic time and then
      call \lstinline|block_array::peek| on the local copy. In the simplest case,
      \lstinline|block_array::peek| randomly selects an item, finds it in
      logarithmic time (since it iterates through blocks), and returns it.
      If the selected item is taken, it scans the block pivot range,
      removes observed taken items from the pivot range, and returns the first untaken item.
      Since each item is
      removed from the pivot range at most once, this overhead can be charged
      to its deletion cost. The pivot range also needs to be updated when
      its size sinks below $\frac{k}{2}$, i.e. once every $\frac{k}{2}$ deletions
      from the \ac{SLSM}. Pivot maintenance is done through binary search over
      a subset of the key domain. In the case of 32-bit integers, it therefore
      has at most 32 iterations, and each iteration considers at most $k$ items,
      for a complexity of $O(k)$. Since pivot maintenance is executed once every
      $O(k)$ deletions, it has amortized constant complexity per peek.
\end{itemize}

\section{\acl{DLSM} Internals} \label{sec:dlsm_internals}

The \ac{DLSM} is a completely distributed priority queue without any
global quality guarantees. It is, however, extremely scalable, and mostly
responsible for the high performance of the \klsm under benign situations.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class dist_lsm {
public:
    void insert(const K &key,
                const V &val,
                shared_lsm<K, V, Rlx> *slsm);

    void find_min(typename block<K, V>::peek_t &best);

    int spy();

private:
    thread_local_ptr<dist_lsm_local<K, V, Rlx>> m_local;
};
\end{lstlisting}
\caption{The \ac{DLSM} header.}
\label{fig:dist_lsm.h}
\end{figure}

Its interface, shown in Figure \ref{fig:dist_lsm.h}, consists of the standard
\lstinline|insert| (not shown) and \lstinline|find_min|, a \lstinline|spy| function,
as well as a special insertion function used from the \klsm which includes an
\ac{SLSM} argument. Again, a thread-local pointer contains the local
part of the data structure.

The local component, called \lstinline|dist_lsm_local| and shown in Figure
\ref{fig:dist_lsm_local.h}, contains the actual \ac{LSM} representation,
memory management pools for blocks and items, a cached item (as in the
\ac{SLSM}), and a random number generator used to determine a victim during
\lstinline|spy| calls.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class dist_lsm_local {
public:
    void insert(const K &key,
                const V &val,
                shared_lsm<K, V, Rlx> *slsm);

    void peek(typename block<K, V>::peek_t &best);

    int spy(class dist_lsm<K, V, Rlx> *parent);

private:
    std::atomic<block<K, V> *> m_head; /**< The largest  block. */
    block<K, V>               *m_tail; /**< The smallest block. */
    block<K, V>               *m_spied;

    block_storage<K, V, 4> m_block_storage;
    item_allocator<item<K, V>, typename item<K, V>::reuse> m_item_allocator;

    typename block<K, V>::peek_t m_cached_best;

    xorshf96 m_gen;
};
\end{lstlisting}
\caption{The local \ac{DLSM} header.}
\label{fig:dist_lsm_local.h}
\end{figure}

Contrary to the \ac{SLSM}, the \ac{DLSM} uses a linked list of blocks to represent
the local \ac{LSM}; this is due to historical reasons and is not expected to make
any difference in achieved performance.
Blocks (covered further in Section \ref{sec:component_internals})
contain \lstinline|next| pointers accessible by all threads, and \lstinline|prev|
pointers which may only be used by the owning thread. Insertions proceed
similar to the \ac{SLSM}: a singleton block is created which is then inserted
at the tail of the list. Trailing blocks are then merged until there is again
only a single block of each capacity within the \ac{LSM}.

Block merges are performed locally, and are invisible to other threads until
the newly formed \ac{LSM} tail is appended atomically to the linked list of blocks
by a single \ac{CAS} instruction on a \lstinline|next| pointer (or the list head).

In order to accomodate the \klsm design in which \acp{DLSM} may never exceed
the relaxation parameter $k$ (\lstinline|Rlx| within the code) in size,
the \ac{DLSM} insertion method has an \lstinline|slsm| argument containing
the global component of the \klsm. Whenever the largest block of the \ac{DLSM}
exceeds $\frac{k}{2}$ in size at the end of an insertion, the block is removed
from the \ac{DLSM} and added to the \ac{SLSM} instead.

An interesting
facet of the current implementation is that since the \ac{DLSM} and \ac{SLSM}
use different memory management methods for blocks, the \ac{DLSM} block
must be copied into an \ac{SLSM} block upon insertion. However, since this copy
is only performed at most once for each item, it only adds an overhead of $O(1)$.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
dist_lsm_local<K, V, Rlx>::peek(typename block<K, V>::peek_t &best)
{
    /* Short-circuit. */
    if (!m_cached_best.empty() && !m_cached_best.taken()) {
        best = m_cached_best;(@* \label{ln:dist_lsm_local::peek.cached} *@)
        return;
    }

    for (auto i = m_head.load(std::memory_order_relaxed);
            i != nullptr;
            i = i->m_next.load(std::memory_order_relaxed)) {

        /* Not shown: block maintenance. Empty blocks are removed,
         * sparsely filled blocks are shrunk. */

        auto candidate = i->peek();
        if (best.empty() ||
                (!candidate.empty() && candidate.m_key < best.m_key)) {
            best = candidate;(@* \label{ln:dist_lsm_local::peek.std} *@)
        }
    }

    if (best.empty() && m_spied != nullptr) {
        best = m_spied->peek();(@* \label{ln:dist_lsm_local::peek.spied} *@)
    }

    m_cached_best = best;
}
\end{lstlisting}
\caption{The \lstinline|dist_lsm_local::peek| implementation.}
\label{fig:dist_lsm_local::peek}
\end{figure}

Code for the \lstinline|peek| function is shown in Figure
\ref{fig:dist_lsm_local::peek}. Similar to the \ac{SLSM}, the \ac{DLSM}
keeps a local cache of the previously determined item with the least key,
which is updated both during calls to \lstinline|peek| and during insertions
(in case the newly inserted item is smaller than the previously known least item).
If a valid cached least item is present when \lstinline|peek| is called, it is
simply returned and no further work is done.

Otherwise, we iterate through each block in the \ac{LSM} and inspect its least item.
Since that item is located at the head of the block, access to it can be done
in constant time. If the \ac{LSM} is nonempty, the minimal such item is returned;
alternatively, we fall back to the minimal item of the spied block. In total, there are
a logarithmic number of blocks, and thus the complexity of \lstinline|peek| is
$O(\log n)$.

The implementation of \lstinline|spy| is another area in which the standalone
\klsm deviates from the Pheet \klsm. In Pheet, spying attempts to copy the
entire contents of another thread's local \ac{LSM}, and adds the copied items
to its own linked list of blocks.

However, this has the disadvantage of raising
the frequency of \ac{DLSM} overflows coupled with resulting block insertions
into the \ac{SLSM}, as well as causing the insertion of duplicate items into the \ac{SLSM}.
Semantics of the \ac{SLSM} remain valid even in the presence of duplicates ---
recall that the \acp{LSM} store as pointers to items, and each item may only
be taken atomically by a single thread; however, such duplicate items waste both space
and time.

The standalone \klsm simplifies \lstinline|spy| to only
copy the largest block from another thread's local \ac{LSM}, and storing
that block separately from the main local block linked list. This change
makes \lstinline|spy| more efficient since only a single block needs to be copied
and no merges must be performed, and additionally prevents insertion of duplicates
into the global \ac{SLSM} because the spied block is never merged.

\subsection{Linearizability, Lock-freedom \& Complexity}

The \ac{DLSM} is linearizable, lock-free, and \lstinline|insert| as well as
\lstinline|peek| have an amortized complexity of $O(\log n)$.
\begin{itemize}
\item \emph{Linearizability}. Insertions into the \ac{DLSM} are linearizable
      when the \lstinline|m_next| pointer, accessible from other threads,
      is set atomically. Within the context of the \klsm, insertions into the
      \ac{DLSM} may also overflow the data structure, in which case the operation
      is linearized at the linearization point of \lstinline|shared_lsm::insert|.

      Peeks are linearized when \lstinline|best| is set for the last
      time (this could occur in Figure \ref{fig:dist_lsm_local::peek},
      Lines \ref{ln:dist_lsm_local::peek.cached}, \ref{ln:dist_lsm_local::peek.std},
      or \ref{ln:dist_lsm_local::peek.spied}).
\item \emph{Lock-freedom}. Both insertions and deletions use only
      non-blocking primitives and execute
      at most $\lceil \log n \rceil$ loop iterations for block merges and are thus lock-free.
\item \emph{Complexity}. An insertion executes at most a logarithmic number
      of merges. As in the \ac{SLSM}, this incurs an amortized complexity of
      $O(\log n)$ per item, since in a size $n$ \ac{LSM} each item has been
      merged at most $\log n$ times and each merge uses constant overhead per item.

      In the standard case, a peek operation inspects each of the $\log n$
      blocks once, and determines its minimal item in constant time for a
      total complexity of $O(\log n)$. Additionally, whenever a block is found
      to be less than half-full, it is shrunk and possibly merged with its successor
      block. Assume this occurs for a block of capacity $c$; then there must
      have been at least $\frac{c}{2}$ delete operations from the current block
      previously. The shrink-merge would incur an overhead of at most $\frac{c}{2}$ (shrinking)
      plus $c$ (merging), for an amortized total of $O(1)$ per item.
\end{itemize}

\section{Component Internals} \label{sec:component_internals}

This section covers the implementation of so-called components,
basic building blocks which are used throughout the \klsm. The most essential
of these are blocks and items, the base containers used to store all items
within the \klsm; and thread-local pointers, which implement local storage while
still granting access to other threads' data on demand.

\subsection{Thread-Local Pointers} \label{ssec:thread_local_ptr_internals}

Thread-local memory is a core requirement of the \klsm: the \ac{DLSM} is
inherently distributed and has one separate \ac{LSM} per thread, and the
\ac{SLSM} also caches data such as a copy of the global block array on
a per-thread basis.

There are several mechanisms and libraries which offer thread-local storage,
the most popular being the C++11 \lstinline|thread_local| keyword \cite{c++11std} and
\acs{pthreads} \lstinline|pthread_getspecific|, \lstinline|pthread_setspecific| functions \cite{butenhof1997programming}.

We have evaluated both of these for use in the \klsm; unfortunately,
neither are suitable. \lstinline|thread_local| is only usable at namespace and
block scopes or for static members, but the \klsm uses nonstatic thread-local class
members. And while POSIX threads could be used to create thread-local class members
(\lstinline|pthread_getspecific| uses a key-value mechanism), neither \ac{pthreads}
nor \lstinline|thread_local| support access to other threads' storage, which is
required by the \ac{DLSM}'s \lstinline|spy| function.

\begin{figure}[ht]
\begin{lstlisting}
template <class T>
class thread_local_ptr {
public:
    T *get();
    T *get(const int32_t tid);

    static size_t current_thread();
    static size_t num_threads();

private:
    lockfree_vector<T> m_items;
};
\end{lstlisting}
\caption{The thread-local pointer header.}
\label{fig:thread_local_ptr.h}
\end{figure}

It therefore seemed necessary to implement a custom mechanism
to handle our thread-local storage needs. The interface of the resulting class
is shown in Figure \ref{fig:thread_local_ptr.h}, and contains functions
to get the current thread's storage, other threads' storage, and determine
the current thread ID as well as the current number of threads.

The class itself is backed on a combination of a lock-free vector which provides
the actual storage space and grows as required in a lock-free manner, as well
as an artificial, strictly ascending \lstinline|thread_local| thread id which is
set by each thread when it first accesses the \lstinline|thread_local_ptr|.
The \lstinline|get| methods then simply resolve to vector accesses.

\begin{figure}[ht]
\begin{lstlisting}
template <class T>
class lockfree_vector {
public:
    T *get(const int n)
    {
        // Determines index of block containing n'th item.
        const int i = index_of(n);

        T *bucket = m_buckets[i].load(std::memory_order_relaxed);
        if (bucket == nullptr) {
            bucket = new T[1 << i];
            T *expected = nullptr;
            if (!m_buckets[i].compare_exchange_strong(expected, bucket)) {
                delete[] bucket;
                bucket = expected;
            }
        }

        return &bucket[n + 1 - (1 << i)];
    }

private:
    std::atomic<T *> m_buckets[bucket_count];
};
\end{lstlisting}
\caption{The lockfree vector header.}
\label{fig:lockfree_vector.h}
\end{figure}

The lock-free vector itself is based on the simple idea of an array of
exponentially growing arrays with capacities of $2^i, i \in \mathbb{N}$.
The full implementation is shown in Figure
\ref{fig:lockfree_vector.h}. If the array containing the desired item
does not yet exist, a new array is created and added to the array of arrays
using a \ac{CAS} instruction.

Micro-benchmarks comparing the different thread-local storage mechanisms
have shown that the \lstinline|thread_local| outperforms both \ac{pthreads}
and our custom implementation by roughly a factor of $2$. However, the increased
cost is dominated by actual work within the \klsm's \lstinline|insert|
and \lstinline|delete_min| functions, and does not contribute meaningfully to
\klsm benchmark results.

\subsection{Blocks and Items} \label{ssec:block_internals}

Blocks and items are the base containers used within the \klsm: items
store key-value pairs, blocks store items, and blocks in turn are stored
by the different \ac{LSM} variants --- linked-list \acp{LSM} in the \ac{DLSM},
and array-based \acp{LSM} in the \ac{SLSM}.

% TODO: Relaxed mem order is most probably not correct below.
\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class item {
public:
    void initialize(const K &key,
                    const V &val) {
        m_version.fetch_add(1, std::memory_order_relaxed);
        m_key = key;
        m_val = val;
    }

    bool take(const version_t version,
              V &val) {
        val = m_val;

        version_t expected = version;
        return m_version.compare_exchange_strong(
                expected,
                expected + 1,
                std::memory_order_relaxed);
    }

private:
    std::atomic<version_t> m_version;
    K m_key;
    V m_val;
};
\end{lstlisting}
\caption{The item header.}
\label{fig:item.h}
\end{figure}

Items are conceptually simple, consisting only of the key-value pair
and a version number (see Figure \ref{fig:item.h}). Even numbered versions
are considered reusable and may be reclaimed at any time by the memory manager,
while odd versions are in
use. Both \lstinline|initialize| and \lstinline|take| increment
the version by one and thereby both invalidate other concurrent \ac{CAS}
instructions and set the item's taken status.

In their most basic form (Figure \ref{fig:block.h}),
blocks are likewise simple constructs. A block
contains an array of items of a certain capacity which is always equal
to $2^n$ for some $n \in \mathbb{N}$. The array stores \lstinline|block_item| instances,
which consists of an expected version and a cached key together with a pointer
to the actual item.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class block {
public:
    struct block_item {
        K m_key;
        item<K, V> *m_item;
        version_t m_version;
    };

public:
    void insert(item<K, V> *it,
                const version_t version);

    void merge(const block<K, V> *lhs,
               const block<K, V> *rhs);

    peek_t peek();

private:
    const size_t m_capacity;
    block_item *m_block_items;
    size_t m_first; /* The first untaken item index. */
    size_t m_last;  /* The first index beyond the last item. */

    static constexpr size_t MAX_SKIPPED_PRUNES = 16;
    size_t m_skipped_prunes;
};
\end{lstlisting}
\caption{The block header.}
\label{fig:block.h}
\end{figure}

It is crucial to understand how a \lstinline|block_item| interacts with
the remaining data structure in order to guarantee atomic access to each
item. Upon insertion, while the item and its version are only visible to
a single thread, the current item version (together with the item's key)
is copied into the \lstinline|block_item|. From this point on, the
\lstinline|block_item| may be copied multiple times, but remains read-only and immutable.
Thus when a thread attempts to take
an item using a \ac{CAS} instruction with the \lstinline|block_item|'s
version, it is guaranteed that \lstinline|block_item.m_version|
has remained unchanged from the time the item has been inserted.
If any thread has taken that particular item, then any further attempts to
take it must fail.

Insertions into a block simply copy the given item's key and version into a
\lstinline|block_item| instance which is then appended to the block item array
by incrementing the \lstinline|m_last| field. \lstinline|peek| iterates
through the block beginning at index \lstinline|m_first|, and returns
the first untaken item (any taken item encountered causes an increment of
\lstinline|m_first|).

Block merges are one of the central operation of the \ac{LSM}, occurring
in both the \ac{SLSM} and \ac{DLSM} as part of most insertions (during
the actual insertion and also during concluding maintenance operations)
and of many deletions (through maintenance when blocks are less than half full).
A block merge consists of both the actual merge of both blocks' items as well
as removal of any encountered taken items, called prune.

We have evaluated several different implementations of \lstinline|merge| ---
a merge with simultaneous pruning of taken items, lazy multi-way merging at
the end of an \lstinline|insert| operation, and finally a simple two-phase
merge-prune. Each of these methods has its own strenghts and weaknesses; the
simultaneous merge \& prune iterates through each item only once per \lstinline|merge|
call, but produces lengthier code. The lazy multi-way merge bundles several
\lstinline|merge| calls into one, avoiding repeated useless work, but is even
more complex to code and understand. Finally, the simple two-phase merge
needs to iterate through each item twice, and needlessly merges taken items
before pruning them; however, it has turned out to be the most efficient
method of operation.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
void
block<K, V>::merge(const block<K, V> *lhs,
                   const size_t lhs_first,
                   const block<K, V> *rhs,
                   const size_t rhs_first)
{
    const size_t lhs_last = lhs->m_last;
    const size_t rhs_last = rhs->m_last;

    auto l = lhs->m_block_items + lhs_first;
    auto r = rhs->m_block_items + rhs_first;
    const auto lend = lhs->m_block_items + lhs_last;
    const auto rend = rhs->m_block_items + rhs_last;

    auto dst = m_block_items;
    while (l < lend && r < rend) {
        *dst++ = (l->m_key < r->m_key) ? *l++ : *r++;
    }

    while (l < lend) *dst++ = *l++;
    while (r < rend) *dst++ = *r++;

    /* Prune. */

    const size_t skipped_prunes = std::max(lhs->m_skipped_prunes,
            rhs->m_skipped_prunes);
    if (skipped_prunes > MAX_SKIPPED_PRUNES) {
        const auto end = dst;
        dst = m_block_items;
        for (auto src = dst; src < end; src++) {
            if (src->taken()) {
                continue;
            } else if (src != dst) {
                *dst = *src;
            }
            dst++;
        }

        m_last = dst - m_block_items;
        m_skipped_prunes = 0;
    } else {
        m_last = size;
        m_skipped_prunes = skipped_prunes + 1;
    }
}
\end{lstlisting}
\caption{The \lstinline|block::merge| implementation.}
\label{fig:block::merge}
\end{figure}

Code for \lstinline|block::merge| is shown in Figure \ref{fig:block::merge}.
The first stage consists of a standard pointer-based merge algorithm, combining
concise code with excellent spacial locality properties. When compared to
a version of the algorithm using array indexing instead of pointer walks,
the pointer walks provide noticeably superior performance. An important point
to note is that this stage does not read the atomic version variable of each
item; this is possible since item keys are cached in the \lstinline|block_item|,
and we can thus completely ignore any issues concerning reused items (with
changed keys) and taken items.

We have also settled on a strategy that does not perform a prune on each merge.
Instead, prunes are only performed once in a while, saving further expensive
reads of the atomic item version and a second iteration of the entire block
in which almost every item may be moved in the worst case.

\section{Memory Management} \label{sec:mm_internals}

Any lock-free data structure such as the \klsm also requires lock-free
memory management, and unlike standard sequential memory management,
its lock-free counterpart is not a solved problem.

To illustrate, the lock-free priority queues briefly discussed in Chapters
\ref{ch:strict} and \ref{ch:relaxed} use a wide variety of memory management methods.
The \citeauthor{shavit2000skiplist} queue uses a dedicated garbage collection
thread in combination with timestamps in order to free memory only once
it cannot be visible to any threads. The \citeauthor{sundell2003fast} priority
queue uses the memory management method by \citeauthor{valois1996lock}
\cite{valois1995lock,valois1996lock} which does not require a separate
garbage collector thread. \citeauthor{linden2013skiplist} use
\citeauthor{fraser2004practical}'s epoch-based reclamation scheme
\cite{fraser2004practical}, and the CBPQ makes use of the author's `Drop the Anchor'
lock-free memory management scheme \cite{braginsky2011locality,braginsky2013drop}.
The SprayList does not have any robust memory management at the time of writing;
it simply preallocates a set amount of memory in advance and fails in case it runs
out of space.

Memory management in both the standalone \klsm and the Pheet \klsm are based
on reuse. Once allocated, a memory chunk is never freed but reused extensively.
Lock-free memory management is required for components accessed across threads,
i.e. items, blocks, and block arrays. These components have differing requirements
and thus each has its own memory management variant, described further
in the following sections.

\subsection{Items} \label{ssec:item_mm_internals}

The standalone \klsm (as well as the Pheet \klsm) use \citeauthor{wimmerphd}'s
wait-free memory management \cite{wimmer2013data,wimmerphd} for items. A detailed
presentation of the scheme is omitted and is presented in detail in
\cite{wimmerphd}.

A general overview is as follows: each thread owns a thread-local copy of
\citeauthor{wimmerphd}'s block-based memory manager. The memory manager
has a single function called \lstinline|acquire|, which returns a pointer to
an available item. Internally, \lstinline|acquire| operates on a list of arrays
of items. An amortized complexity of $O(1)$ per allocation is guaranteed by
iterating through items and returning the first unused one found as long as
amortized guarantees are not violated, and simply creating a new block if they
are.

As explained previously, the ABA problem is handled by linking each item with
a specific integral version, with even versions representing unused items and
odd versions being in use. Marking items used can thus use an \ac{FAA} instruction
(since it can only be allocated by its owner thread),
while taking an item (i.e. marking it unused) can be done with an atomic \ac{CAS}.

\subsection{Blocks} \label{ssec:block_mm_internals}

Blocks (and block arrays in the next section) are managed based on the idea
of a fixed-size pool. It is possible to bound the number of blocks of any given
capacity which may be allocated at once on a single thread, and thus we may
simply pre-allocate the required number of blocks.

Handling of block allocations has subtly different requirements in the \ac{SLSM}
and \ac{DLSM}: in the \ac{DLSM}, blocks have a simple lifecycle in which
they are either used or unused. In the \ac{SLSM}, blocks may be in short-term use locally;
in long-term global use; or unused. We thus use two separate mechanisms for managing
block memory.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int N>
class block_storage {
private:
    struct block_tuple {
        block<K, V> *xs[N];
    };

public:
    block<K, V> *get_block(const size_t i);

private:
    block_tuple m_blocks[MAX_BLOCKS];
    size_t m_size;
};
\end{lstlisting}
\caption{The \lstinline|block_storage| header.}
\label{fig:block_storage.h}
\end{figure}

The \ac{DLSM} uses the more primitive memory manager, called \lstinline|block_storage|
(Figure \ref{fig:block_storage.h}). It consists of an array of block $N$-tuples,
which are dynamically allocated by \lstinline|get_block| whenever a block
of an as-of-yet unused capacity is requested.
The first unused block within the requested capacity $N$-tuple is then returned.
In the case of the \ac{DLSM}, at most four blocks of the same capacity may be
used during a single operation.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V>
class block_pool {
private:
    enum block_status {
        BLOCK_FREE,
        BLOCK_LOCAL,
        BLOCK_GLOBAL,
    };

public:
    block<K, V> *get_block(const size_t i)
    {
        int max_global_version = /* maximal version of level i */;
        for (int j = ix(i); j < ix(i + 1); j++) {
            if (m_status[j] == BLOCK_FREE
                    || (m_status[j] == BLOCK_GLOBAL
                        && m_version[j] != max_global_version)) {
                m_status[j] = BLOCK_LOCAL;
                if (m_pool[j] == nullptr) {
                    m_pool[j] = new block<K, V>(i);
                }
                return m_pool[j];
            }
        }
        return nullptr;
    }

private:
    block<K, V> *m_pool[BLOCKS_IN_POOL];
    block_status m_status[BLOCKS_IN_POOL];
    version_t    m_version[BLOCKS_IN_POOL];
};
\end{lstlisting}
\caption{The \lstinline|block_pool| header.}
\label{fig:block_pool.h}
\end{figure}

Block management within the \ac{SLSM} is complicated by the fact that blocks
go through an additional lifecycle stage. During \lstinline|shared_lsm::insert|,
the new merged blocks are first created locally, during which allocated blocks
are marked as in local use. If the newly created block array is published
successfully, all blocks it contains are then marked as in global use. If,
on the other hand, publishing fails, then all locally used blocks are marked
as unused and the operation is restarted.

The \ac{SLSM} block manager,
the \lstinline|block_pool|, is structured similarly to \lstinline|block_storage|,
except that it additionally stores a block status and version for each block
(Figure \ref{fig:block_pool.h}). The version represents the version of the most
recently published global array containing the corresponding block and is set
when a global array is published successfully. A block may be reusable in two
circumstances: either it is marked as unused; or it is set as globally used,
but there is a more recently published block of the same capacity, in which
case the older block is safe to use since there may only be a single block of
each capacity within an \ac{LSM}. The states \lstinline|BLOCK_FREE| and
\lstinline|BLOCK_LOCAL| may transition freely between each other, but
\lstinline|BLOCK_GLOBAL| is never explicitly marked unused and may thus only
transition to \lstinline|BLOCK_LOCAL| if a newer published block of the same capacity
exists.

Blocks do not have their own versions, and are instead associated with a versioned
block array in order to avoid the ABA problem.

\subsection{Block Arrays} \label{ssec:block_array_mm_internals}

Block arrays are used within the \ac{SLSM} to represent array-based \acp{LSM}.
Memory management is simple as there may only be a single published block
array at a time. Each thread has two preallocated block arrays. Which one of these
is chosen for allocation depends on the version of the to-be-constructed array ---
one for odd versions, the other for even versions.

Like items, block arrays are associated with an integral version as previously described. The full version
is included in the block array itself, while a truncated version is packed into
the published block array pointer to prevent the ABA problem.
