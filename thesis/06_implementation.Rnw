\chapter{Implementation of the $k$-LSM Priority Queue} \label{ch:implementation}

\begin{comment}
* Initially: language and libraries used, cmake, interfaces, general guidelines
  (modularity, template use for flexible key/value types & relaxation), header programming.
  Mention tests, benching utilities (bench.py, plot.R, unpheet.sh, etc). rationale
  - why is a standalone impl needed -> measurability, composability missing in original,
  etc.
  
* top down: klsm interface, members, methods. 
  slsm interface, members, methods. dlsm interface, members, methods.
  further components.
  
* lock-free memory management. martin's item mm. block mm (2 methods), block
  array pool. instances where we need to take special care of reuse (e.g.
  when merging & size of blocks exceeds capacity (?)).
  
* important steps in development. performance jumps (-flto, binary search pivots,
  pointer walks). find more and show benchmarks.
\end{comment}

% TODO: Mention that the klsm is a min queue.

Contrary to the previous chapter in which the concept of the \klsm was briefly
described, this chapter will cover the implementation details of the new standalone
\klsm.

But first, a rationale --- why is a standalone \klsm implementation desirable?
There are several reasons:

\begin{itemize}
\item Comparability. The original \klsm implementation within the task-scheduling
      framework Pheet \cite{wimmer2013data,wimmer2015lock} 
      (simply referred to as the \emph{Pheet \klsm} within this
      thesis) has been demonstrated to perform very well; but how well exactly
      when compared to other state of the art queues is not easy to determine
      since the Pheet \klsm implementation is tied tightly to the Pheet framework.
      A standalone \klsm will allow other for easy comparability against other
      queues and answer how its performance compares to the state of the art.
\item Use in practice. A standalone \klsm can easily be integrated into applications
      for use in practice.
\item Maintainability. The Pheet \klsm implementation is focused on maximal performance
      and makes tradeoffs that pose challenges to maintainability. While the standalone
      \klsm also aims to achieve high performance, the reimplementation also 
      explicitly values classical software engineering concepts such as 
      composability, readability, and separation of concerns.
\item Further insight and improvements. Finally, a reimplementation by a fresh
      set of eyes can create new insight into the design and might result in
      improvements to further increase performance over the original Pheet \klsm.
\end{itemize}

The standalone \klsm (referred to as the \emph{klsm} in the remaining chapter)
is implemented in C++11 using a minimal set of dependencies on other libraries.
The included benchmark application requires the \emph{hwloc} library for hardware-independent
thread pinning, while other priority queues included for benchmarking purposes
require the \ac{GSL}. The raw \klsm implementation itself has no external 
dependencies.

Full code for the \klsm implementation is available at 
\url{https://github.com/schuay/kpqueue}. The directory structure is as follows:
\lstinline|doc/| contains autogenerated documentation in the Doxygen\footnote{\url{http://www.stack.nl/~dimitri/doxygen/}} format, \lstinline|lib/| contains external source code for other priority queues used
for benchmarking, \lstinline|misc/| contains various utility scripts which handle
conversion of raw benchmarking output from various formats into plots using R\footnote{\url{https://www.r-project.org/}}, \lstinline|src/| contains code for the standalone \klsm and various benchmarking
tools, and \lstinline|test/| contains the test suite.

The following sections describe the \klsm implementation in top-down order ---
Section \ref{sec:klsm_internals} begins with the \klsm itself, Sections \ref{sec:slsm_internals}
and \ref{sec:dlsm_internals} descend to the component queues \ac{SLSM} and \ac{DLSM},
while several other components are covered in Section \ref{sec:component_internals}.
The various applied methods of lock-free memory management are covered in Section \ref{sec:mm_internals};
and finally, aspects of the implementation which have proven especially relevant
for performance are highlighted in Section \ref{sec:implementation_impact}.

\section{\klsm Internals} \label{sec:klsm_internals}

While the general \klsm interface has been covered previously in Section \ref{sec:wimmer},
several details have been omitted for clarity. Figure \ref{fig:klsm.h} displays the full
header file containing the \lstinline|k_lsm| class. In this instance, implementation
details such constructors and destructors, as the \lstinline|init_thread| function,
are included in order to present a complete picture of what a class implementation
looks like. Future code listings will skip these details.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class k_lsm {
public:
    k_lsm();
    virtual ~k_lsm() { }

    void insert(const K &key);
    void insert(const K &key,
                const V &val);

    bool delete_min(V &val);

    void init_thread(const size_t) const { }
    constexpr static bool supports_concurrency() { return true; }

private:
    dist_lsm<K, V, Rlx>   m_dist;
    shared_lsm<K, V, Rlx> m_shared;
};

#include "k_lsm_inl.h"
\end{lstlisting}
\caption{The \klsm header.}
\label{fig:klsm.h}
\end{figure}

The \lstinline|k_lsm| class is a template class parameterized by \lstinline|K|
and \lstinline|V|, respectively the key- and value types, as well as the relaxation
parameter \lstinline|Rlx|, which corresponds to the $k$ in \klsm. Code for template class
must be exclusively located in header files, but it is possibly to preserve readability
using a pattern of inlined headers: the main header includes a class declaration,
while an inline header contains the class definition and is itself 
included in the standard header. The \lstinline|init_thread| function is provided
for queues which require per-thread initialization.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
k_lsm<K, V, Rlx>::insert(const K &key,
                         const V &val)
{
    m_dist.insert(key, val, &m_shared);
}
\end{lstlisting}
\caption{The \lstinline|k_lsm::insert| implementation.}
\label{fig:k_lsm::insert}
\end{figure}

Trivialities aside, implementation of the \lstinline|k_lsm| class is extremely simple since it relies
on a composition of the \ac{SLSM} and \ac{DLSM} queues. Insertion (see Figure \ref{fig:k_lsm::insert})
simply triggers insertion into the local \ac{DLSM}, passing a pointer to the global
\ac{SLSM} queue in case the maximal size of local blocks is exceeded.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
bool
k_lsm<K, V, Rlx>::delete_min(V &val)
{
    typename block<K, V>::peek_t
            best_dist = block<K, V>::peek_t::EMPTY(),
            best_shared = block<K, V>::peek_t::EMPTY();

    do {
        m_dist.find_min(best_dist);
        m_shared.find_min(best_shared);

        if (!best_dist.empty() && !best_shared.empty()) {
            if (best_dist.m_key < best_shared.m_key) {
                return best_dist.take(val);
            } else {
                return best_shared.take(val);
            }
        }

        if (!best_dist.empty() /* and best_shared is empty */) {
            return best_dist.take(val);
        }

        if (!best_shared.empty() /* and best_dist is empty */) {
            return best_shared.take(val);
        }
    } while (m_dist.spy() > 0);

    return false;
}
\end{lstlisting}
\caption{The \lstinline|k_lsm::delete_min| implementation.}
\label{fig:k_lsm::delete_min}
\end{figure}

Deletions (Figure \ref{fig:k_lsm::delete_min}) call \lstinline|find_min| 
on both the \ac{SLSM} and \ac{DLSM} queues
and return the lesser of both items in the standard case. In case both queues
are empty, the local \ac{DLSM} attempts to copy items from another thread by
triggering a call to \lstinline|spy|, and finally retrying deletion.

These implementations seem simple, even basic; but that is because details are
delegated to responsible classes. The best example in this case is the act of
item removal --- how can we ensure that an item is taken only a single thread? 
And since memory for items is reused (see Section \ref{sec:mm_internals}),
how can we prevent the ABA problem in which memory is reused after being
retrieved (i.e. the thread reads data different than had been intended)?

The answer lies within the \lstinline|peek_t| class, which contains both
a pointer to the item and an associated expected item version. Within \lstinline|peek_t::take|,
a \ac{CAS} instruction is used to atomically increment the item's version, if,
and only if it previously matched the given expected version. In the presence
of two identical \ac{CAS} calls by different threads, only one will succeed
while the other must fail. The thread which has seen a \ac{CAS} failure will
return from from \lstinline|delete_min| with a so-called spurious failure,
returning \lstinline|false| even though the queue might be non-empty.

\section{\acl{SLSM} Internals} \label{sec:slsm_internals} 

The \ac{SLSM} is the central, global component of the \klsm and contains
all items that have overflowed the local \ac{DLSM} capacities. The \ac{SLSM}
batches insertions by inserting blocks rather than items, and relaxes deletions
by keeping track of a so-called pivot range, which contains a subset of $k$
smallest items. 

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class shared_lsm {
public:
    void insert(const K &key,
                const V &val);
    void insert(block<K, V> *b);

    bool delete_min(V &val);
    void find_min(typename block<K, V>::peek_t &best);

private:
    versioned_array_ptr<K, V, Rlx> m_global_array;
    thread_local_ptr<shared_lsm_local<K, V, Rlx>> m_local_component;
};
\end{lstlisting}
\caption{The \ac{SLSM} header.}
\label{fig:shared_lsm.h}
\end{figure}

Figure \ref{fig:shared_lsm.h} shows the header file for the \lstinline|shared_lsm|
class. The interface contains the standard insertion and deletion functions since
the \ac{SLSM} may be used as a standalone priority queue. However, it also has
a batched insertion function which takes a block argument, and a \lstinline|find_min|
function used by the \klsm to retrieve one of the smallest items without removing it.

The \ac{SLSM} itself contains a versioned pointer to the global block array, which is
the single point of truth for determining the contents of the \ac{SLSM}. The
pointer is versioned since block arrays are heavily reused --- each thread allocates
two block arrays and uses them in an alternating fashion when setting the global
array pointer (see Section \ref{sec:mm_internals}). 

Without versioning, this might
cause issues: consider a case in which thread A retrieves the global array
pointer $ptr$ (which has been allocated by thread B) and then stalls. Thread B then
reuses the array $ptr$ and fills it with new contents. When thread A continues
execution and successfully verifies that the global pointer appears to be unchanged, 
$ptr$ contains different content than A expects - this is an instance
of the ABA problem.

Versioning is used to solve several aspects of this issue. First, the block array
itself contains an integral version number which may compared with a local copy
the check whether the array has remained unchanged. Second, a portion of the version
number (more precisely, the least 11 bits) is packed into the global array pointer
itself to ensure valid results of \ac{CAS} operations when setting the global pointer.
The class \lstinline|versioned_array_ptr| handles operations on such packed pointers,
and itself contains an \lstinline|aligned_block_array|, which is simply a block
array allocated at a 2048 byte boundary (i.e., the least 11 bits are zeroed).

The thread-local \ac{SLSM} component is wrapped within the \lstinline|thread_local_ptr|
class (see Section \ref{sec:component_internals}), which allows lock-free retrieval
of a thread-local instance of the given class. Contrary to the standard thread-local
mechanisms provided by e.g. POSIX threads (\lstinline|pthread_getspecific|) or
C++ (the \lstinline|thread_local| keyword), \lstinline|thread_local_ptr| allows
usage as a class member, and access to other threads' local instances.

The local \ac{SLSM} component, \lstinline|shared_lsm_local|, contains most
of the vital logic, and its interface is presented in Figure \ref{fig:shared_lsm_local.h}.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class shared_lsm_local {
public:
    void insert(block<K, V> *b,
                versioned_array_ptr<K, V, Rlx> &global_array);

    void peek(typename block<K, V>::peek_t &best,
              versioned_array_ptr<K, V, Rlx> &global_array);

private:
    typename block<K, V>::peek_t m_cached_best;

    /** A copy of the global block array, updated regularly. */
    block_array<K, V, Rlx> m_local_array_copy;

    /** Memory management. */
    item_allocator<item<K, V>, typename item<K, V>::reuse> m_item_pool;
    block_pool<K, V> m_block_pool;
    aligned_block_array<K, V, Rlx> m_array_pool_odds;
    aligned_block_array<K, V, Rlx> m_array_pool_evens;
};
\end{lstlisting}
\caption{The header of the local \ac{SLSM} component.}
\label{fig:shared_lsm_local.h}
\end{figure}

Members include pools for memory management of several classes, which are covered
in further depth in Section \ref{sec:mm_internals}. Furthermore, a cached copy of
the item previously returned from \lstinline|peek| (the local equivalent to \lstinline|shared_lsm::find_min|)
is kept in order to avoid useless repeated lookups. If, for instance, the \klsm
chooses the item returned by the \ac{DLSM} to return from \lstinline|k_lsm::delete_min|,
then a subsequent call to \lstinline|shared_lsm::find_min| (and thus \lstinline|shared_lsm_local::peek|)
may simply return the cached item if the global block array has remained unchanged.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
shared_lsm_local<K, V, Rlx>::insert_block(
        block<K, V> *b,
        versioned_array_ptr<K, V, Rlx> &global_array)
{
    while (true) {
        /* Fetch a consistent copy of the global array. */

        block_array<K, V, Rlx> *observed_packed;
        version_t observed_version;
        refresh_local_array_copy(observed_packed, observed_version, global_array);

        /* Create a new version which we will attempt to push globally. */

        auto &new_blocks = /* A local pooled instance. */
        auto new_blocks_ptr = new_blocks.ptr();
        new_blocks_ptr->copy_from(&m_local_array_copy);
        new_blocks_ptr->increment_version();
        new_blocks_ptr->insert(b, &m_block_pool);

        /* Try to update the global array. */

        if (global_array.compare_exchange_strong(observed_packed, (@* \label{ln:shared_lsm_local::insert_block.cas} *@)
                                                 new_blocks)) {
            break;
        }
    }
}
\end{lstlisting}
\caption{The \lstinline|shared_lsm_local::insert_block| implementation.}
\label{fig:shared_lsm_local::insert_block}
\end{figure}

Insertions are ultimately handled by \lstinline|insert_block|, shown in
Figure \ref{fig:shared_lsm_local::insert_block}. We skip over
single-item insertions since we are mostly concerned with the \ac{SLSM} in the
context of the \klsm (but essentially, they simply create a singleton block
and insert it).

Block insertions initially ensure that the local
array copy (a shallow copy of the global array) is up to date by retrieving
the current global block array and comparing versions, then updating the copy
if necessary. It is then possible to access the local copy without fear of
concurrent modifications (as long as one is careful when reading blocks
themselves).

A new block array copy is then constructed from the local array copy, its
version is incremented,
and the given block is inserted into it (details in Section \ref{ssec:block_array_internals}).

Finally, the global array pointer is updated using a \ac{CAS} operation. The
update fails if the global array has changed since it has been last read by this
thread, in which case the insertion is retried. Otherwise, the insertion
is successful. The linearization point for insertions into the \ac{SLSM}
is upon a successful in line \ref{ln:shared_lsm_local::insert_block.cas}.

It is important to realize that any work done within this function prior to a
failed \ac{CAS} is wasted work. Insertions into the block array copy trigger
expensive merges and other maintenance, and we must attempt to minimize
such failures as much as possible. Using higher values of $k$, i.e. batching
more items together during insertions is one way of reducing \ac{CAS} failures.

Experiments using performance counter show that on average, there are $0.75$
retries per insertion when running a simple throughput benchmark with $k = 256$
on 35 threads,
and $2.95$ retries per insertion when running on 80 threads. As expected, absolute
performance is higher with 35 threads, resulting in around $15\%$ more completed
insertions.

\subsection{Block Arrays} \label{ssec:block_array_internals}

\section{\acl{DLSM} Internals} \label{sec:dlsm_internals}
\section{Component Internals} \label{sec:component_internals}

\section{Memory Management} \label{sec:mm_internals}
\subsection{Items} \label{ssec:item_mm_internals}
\subsection{Blocks} \label{ssec:block_mm_internals}

% DLSM: Simpler mechanism since block pointers owned by thread.
% SLSM: insert_block.

\subsection{Block Arrays} \label{ssec:block_array_mm_internals}

% SLSM: insert_block.

\section{Implementation Impact on Performance} \label{sec:implementation_impact}
