\chapter{Implementation of the $k$-LSM Priority Queue} \label{ch:implementation}

\begin{comment}
* Initially: language and libraries used, cmake, interfaces, general guidelines
  (modularity, template use for flexible key/value types & relaxation), header programming.
  Mention tests, benching utilities (bench.py, plot.R, unpheet.sh, etc). rationale
  - why is a standalone impl needed -> measurability, composability missing in original,
  etc.
  
* top down: klsm interface, members, methods. 
  slsm interface, members, methods. dlsm interface, members, methods.
  further components.
  
* lock-free memory management. martin's item mm. block mm (2 methods), block
  array pool. instances where we need to take special care of reuse (e.g.
  when merging & size of blocks exceeds capacity (?)).
  
* important steps in development. performance jumps (-flto, binary search pivots,
  pointer walks). find more and show benchmarks.
\end{comment}

% TODO: Mention that the klsm is a min queue.
% TODO: State claimed qualities. Linearizability, lock-free, O(\log n) operations (really?)

Contrary to the previous chapter in which the concept of the \klsm was briefly
described, this chapter will cover the implementation details of the new standalone
\klsm.

But first, a rationale --- why is a standalone \klsm implementation desirable?
There are several reasons:

\begin{itemize}
\item Comparability. The original \klsm implementation within the task-scheduling
      framework Pheet \cite{wimmer2013data,wimmer2015lock} 
      (simply referred to as the \emph{Pheet \klsm} within this
      thesis) has been demonstrated to perform very well; but how well exactly
      when compared to other state of the art queues is not easy to determine
      since the Pheet \klsm implementation is tied tightly to the Pheet framework.
      A standalone \klsm will allow other for easy comparability against other
      queues and answer how its performance compares to the state of the art.
\item Use in practice. A standalone \klsm can easily be integrated into applications
      for use in practice.
\item Maintainability. The Pheet \klsm implementation is focused on maximal performance
      and makes tradeoffs that pose challenges to maintainability. While the standalone
      \klsm also aims to achieve high performance, the reimplementation also 
      explicitly values classical software engineering concepts such as 
      composability, readability, and separation of concerns.
\item Further insight and improvements. Finally, a reimplementation by a fresh
      set of eyes can create new insight into the design and might result in
      improvements to further increase performance over the original Pheet \klsm.
\end{itemize}

The standalone \klsm (referred to as the \emph{klsm} in the remaining chapter)
is implemented in C++11 using a minimal set of dependencies on other libraries.
The included benchmark application requires the \emph{hwloc} library for hardware-independent
thread pinning, while other priority queues included for benchmarking purposes
require the \ac{GSL}. The raw \klsm implementation itself has no external 
dependencies.

Full code for the \klsm implementation is available at 
\url{https://github.com/schuay/kpqueue}. The directory structure is as follows:
\lstinline|doc/| contains autogenerated documentation in the Doxygen\footnote{\url{http://www.stack.nl/~dimitri/doxygen/}} format, \lstinline|lib/| contains external source code for other priority queues used
for benchmarking, \lstinline|misc/| contains various utility scripts which handle
conversion of raw benchmarking output from various formats into plots using R\footnote{\url{https://www.r-project.org/}}, \lstinline|src/| contains code for the standalone \klsm and various benchmarking
tools, and \lstinline|test/| contains the test suite.

The following sections describe the \klsm implementation in top-down order ---
Section \ref{sec:klsm_internals} begins with the \klsm itself, Sections \ref{sec:slsm_internals}
and \ref{sec:dlsm_internals} descend to the component queues \ac{SLSM} and \ac{DLSM},
while several other components are covered in Section \ref{sec:component_internals}.
The various applied methods of lock-free memory management are covered in Section \ref{sec:mm_internals};
and finally, aspects of the implementation which have proven especially relevant
for performance are highlighted in Section \ref{sec:implementation_impact}.

\section{\klsm Internals} \label{sec:klsm_internals}

While the general \klsm interface has been covered previously in Section \ref{sec:wimmer},
several details have been omitted for clarity. Figure \ref{fig:klsm.h} displays the full
header file containing the \lstinline|k_lsm| class. In this instance, implementation
details such constructors and destructors, as the \lstinline|init_thread| function,
are included in order to present a complete picture of what a class implementation
looks like. Future code listings will skip these details.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class k_lsm {
public:
    k_lsm();
    virtual ~k_lsm() { }

    void insert(const K &key);
    void insert(const K &key,
                const V &val);

    bool delete_min(V &val);

    void init_thread(const size_t) const { }
    constexpr static bool supports_concurrency() { return true; }

private:
    dist_lsm<K, V, Rlx>   m_dist;
    shared_lsm<K, V, Rlx> m_shared;
};

#include "k_lsm_inl.h"
\end{lstlisting}
\caption{The \klsm header.}
\label{fig:klsm.h}
\end{figure}

The \lstinline|k_lsm| class is a template class parameterized by \lstinline|K|
and \lstinline|V|, respectively the key- and value types, as well as the relaxation
parameter \lstinline|Rlx|, which corresponds to the $k$ in \klsm. Code for template class
must be exclusively located in header files, but it is possibly to preserve readability
using a pattern of inlined headers: the main header includes a class declaration,
while an inline header contains the class definition and is itself 
included in the standard header. The \lstinline|init_thread| function is provided
for queues which require per-thread initialization.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
k_lsm<K, V, Rlx>::insert(const K &key,
                         const V &val)
{
    m_dist.insert(key, val, &m_shared);
}
\end{lstlisting}
\caption{The \lstinline|k_lsm::insert| implementation.}
\label{fig:k_lsm::insert}
\end{figure}

Trivialities aside, implementation of the \lstinline|k_lsm| class is extremely simple since it relies
on a composition of the \ac{SLSM} and \ac{DLSM} queues. Insertion (see Figure \ref{fig:k_lsm::insert})
simply triggers insertion into the local \ac{DLSM}, passing a pointer to the global
\ac{SLSM} queue in case the maximal size of local blocks is exceeded.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
bool
k_lsm<K, V, Rlx>::delete_min(V &val)
{
    typename block<K, V>::peek_t
            best_dist = block<K, V>::peek_t::EMPTY(),
            best_shared = block<K, V>::peek_t::EMPTY();

    do {
        m_dist.find_min(best_dist);
        m_shared.find_min(best_shared);

        if (!best_dist.empty() && !best_shared.empty()) {
            if (best_dist.m_key < best_shared.m_key) {
                return best_dist.take(val);
            } else {
                return best_shared.take(val);
            }
        }

        if (!best_dist.empty() /* and best_shared is empty */) {
            return best_dist.take(val);
        }

        if (!best_shared.empty() /* and best_dist is empty */) {
            return best_shared.take(val);
        }
    } while (m_dist.spy() > 0);

    return false;
}
\end{lstlisting}
\caption{The \lstinline|k_lsm::delete_min| implementation.}
\label{fig:k_lsm::delete_min}
\end{figure}

Deletions (Figure \ref{fig:k_lsm::delete_min}) call \lstinline|find_min| 
on both the \ac{SLSM} and \ac{DLSM} queues
and return the lesser of both items in the standard case. In case both queues
are empty, the local \ac{DLSM} attempts to copy items from another thread by
triggering a call to \lstinline|spy|, and finally retrying deletion.

These implementations seem simple, even basic; but that is because details are
delegated to responsible classes. The best example in this case is the act of
item removal --- how can we ensure that an item is taken only a single thread? 
And since memory for items is reused (see Section \ref{sec:mm_internals}),
how can we prevent the ABA problem in which memory is reused after being
retrieved (i.e. the thread reads data different than had been intended)?

The answer lies within the \lstinline|peek_t| class, which contains both
a pointer to the item and an associated expected item version. Within \lstinline|peek_t::take|,
a \ac{CAS} instruction is used to atomically increment the item's version, if,
and only if it previously matched the given expected version. In the presence
of two identical \ac{CAS} calls by different threads, only one will succeed
while the other must fail. The thread which has seen a \ac{CAS} failure will
return from from \lstinline|delete_min| with a so-called spurious failure,
returning \lstinline|false| even though the queue might be non-empty.

\section{\acl{SLSM} Internals} \label{sec:slsm_internals} 

The \ac{SLSM} is the central, global component of the \klsm and contains
all items that have overflowed the local \ac{DLSM} capacities. The \ac{SLSM}
batches insertions by inserting blocks rather than items, and relaxes deletions
by keeping track of a so-called pivot range, which contains a subset of $k$
smallest items. 

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class shared_lsm {
public:
    void insert(const K &key,
                const V &val);
    void insert(block<K, V> *b);

    bool delete_min(V &val);
    void find_min(typename block<K, V>::peek_t &best);

private:
    versioned_array_ptr<K, V, Rlx> m_global_array;
    thread_local_ptr<shared_lsm_local<K, V, Rlx>> m_local_component;
};
\end{lstlisting}
\caption{The \ac{SLSM} header.}
\label{fig:shared_lsm.h}
\end{figure}

Figure \ref{fig:shared_lsm.h} shows the header file for the \lstinline|shared_lsm|
class. The interface contains the standard insertion and deletion functions since
the \ac{SLSM} may be used as a standalone priority queue. However, it also has
a batched insertion function which takes a block argument, and a \lstinline|find_min|
function used by the \klsm to retrieve one of the smallest items without removing it.

The \ac{SLSM} itself contains a versioned pointer to the global block array, which is
the single point of truth for determining the contents of the \ac{SLSM}. The
pointer is versioned since block arrays are heavily reused --- each thread allocates
two block arrays and uses them in an alternating fashion when setting the global
array pointer (see Section \ref{sec:mm_internals}). 

Without versioning, this might
cause issues: consider a case in which thread A retrieves the global array
pointer $ptr$ (which has been allocated by thread B) and then stalls. Thread B then
reuses the array $ptr$ and fills it with new contents. When thread A continues
execution and successfully verifies that the global pointer appears to be unchanged, 
$ptr$ contains different content than A expects - this is an instance
of the ABA problem.

Versioning is used to solve several aspects of this issue. First, the block array
itself contains an integral version number which may compared with a local copy
the check whether the array has remained unchanged. Second, a portion of the version
number (more precisely, the least 11 bits) is packed into the global array pointer
itself to ensure valid results of \ac{CAS} operations when setting the global pointer.
The class \lstinline|versioned_array_ptr| handles operations on such packed pointers,
and itself contains an \lstinline|aligned_block_array|, which is simply a block
array allocated at a 2048 byte boundary (i.e., the least 11 bits are zeroed).

The thread-local \ac{SLSM} component is wrapped within the \lstinline|thread_local_ptr|
class (see Section \ref{sec:component_internals}), which allows lock-free retrieval
of a thread-local instance of the given class. Contrary to the standard thread-local
mechanisms provided by e.g. POSIX threads (\lstinline|pthread_getspecific|) or
C++ (the \lstinline|thread_local| keyword), \lstinline|thread_local_ptr| allows
usage as a class member, and access to other threads' local instances.

The local \ac{SLSM} component, \lstinline|shared_lsm_local|, contains most
of the vital logic, and its interface is presented in Figure \ref{fig:shared_lsm_local.h}.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class shared_lsm_local {
public:
    void insert(block<K, V> *b,
                versioned_array_ptr<K, V, Rlx> &global_array);

    void peek(typename block<K, V>::peek_t &best,
              versioned_array_ptr<K, V, Rlx> &global_array);

private:
    typename block<K, V>::peek_t m_cached_best;

    /** A copy of the global block array, updated regularly. */
    block_array<K, V, Rlx> m_local_array_copy;

    /** Memory management. */
    item_allocator<item<K, V>, typename item<K, V>::reuse> m_item_pool;
    block_pool<K, V> m_block_pool;
    aligned_block_array<K, V, Rlx> m_array_pool_odds;
    aligned_block_array<K, V, Rlx> m_array_pool_evens;
};
\end{lstlisting}
\caption{The header of the local \ac{SLSM} component.}
\label{fig:shared_lsm_local.h}
\end{figure}

Members include pools for memory management of several classes, which are covered
in further depth in Section \ref{sec:mm_internals}. Furthermore, a cached copy of
the item previously returned from \lstinline|peek| (the local equivalent to \lstinline|shared_lsm::find_min|)
is kept in order to avoid useless repeated lookups. If, for instance, the \klsm
chooses the item returned by the \ac{DLSM} to return from \lstinline|k_lsm::delete_min|,
then a subsequent call to \lstinline|shared_lsm::find_min| (and thus \lstinline|shared_lsm_local::peek|)
may simply return the cached item if the global block array has remained unchanged.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
shared_lsm_local<K, V, Rlx>::insert_block(
        block<K, V> *b,
        versioned_array_ptr<K, V, Rlx> &global_array)
{
    while (true) {
        /* Fetch a consistent copy of the global array. */

        block_array<K, V, Rlx> *observed_packed;
        version_t observed_version;
        refresh_local_array_copy(observed_packed, observed_version, global_array);

        /* Create a new version which we will attempt to push globally. */

        auto &new_blocks = /* A local pooled instance. */
        auto new_blocks_ptr = new_blocks.ptr();
        new_blocks_ptr->copy_from(&m_local_array_copy);
        new_blocks_ptr->increment_version();
        new_blocks_ptr->insert(b, &m_block_pool);

        /* Try to update the global array. */

        if (global_array.compare_exchange_strong(observed_packed, (@* \label{ln:shared_lsm_local::insert_block.cas} *@)
                                                 new_blocks)) {
            break;
        }
    }
}
\end{lstlisting}
\caption{The \lstinline|shared_lsm_local::insert_block| implementation.}
\label{fig:shared_lsm_local::insert_block}
\end{figure}

Insertions are ultimately handled by \lstinline|insert_block|, shown in
Figure \ref{fig:shared_lsm_local::insert_block}. We skip over
single-item insertions since we are mostly concerned with the \ac{SLSM} in the
context of the \klsm (but essentially, they simply create a singleton block
and insert it).

Block insertions initially ensure that the local
array copy (a shallow copy of the global array) is up to date by retrieving
the current global block array and comparing versions, then updating the copy
if necessary. It is then possible to access the local copy without fear of
concurrent modifications (as long as one is careful when reading blocks
themselves).

A new block array copy is then constructed from the local array copy, its
version is incremented,
and the given block is inserted into it (details in Section \ref{ssec:block_array_internals}).

Finally, the global array pointer is updated using a \ac{CAS} operation. The
update fails if the global array has changed since it has been last read by this
thread, in which case the insertion is retried. Otherwise, the insertion
is successful. The linearization point for insertions into the \ac{SLSM}
is upon a successful in line \ref{ln:shared_lsm_local::insert_block.cas}.

It is important to realize that any work done within this function prior to a
failed \ac{CAS} is wasted work. Insertions into the block array copy trigger
expensive merges and other maintenance, and we must attempt to minimize
such failures as much as possible. Using higher values of $k$, i.e. batching
more items together during insertions is one way of reducing \ac{CAS} failures.

Experiments using performance counter show that on average, there are $0.75$
retries per insertion when running a simple throughput benchmark with $k = 256$
on 35 threads,
and $2.95$ retries per insertion when running on 80 threads. As expected, absolute
performance is higher with 35 threads, resulting in around $15\%$ more completed
insertions.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
shared_lsm_local<K, V, Rlx>::peek(
        typename block<K, V>::peek_t &best,
        versioned_array_ptr<K, V, Rlx> &global_array)
{
    if (local_array_copy_is_fresh(global_array)
            && !m_cached_best.empty()
            && !m_cached_best.taken()) {
        best = m_cached_best;
        return;
    }

    block_array<K, V, Rlx> *observed_packed;
    version_t observed_version;

    do {
        refresh_local_array_copy(observed_packed,
                                 observed_version,
                                 global_array);
        best = m_cached_best = m_local_array_copy.peek();
    } while (global_array.load()->version() != observed_version);
}
\end{lstlisting}
\caption{The \lstinline|shared_lsm_local::peek| implementation.}
\label{fig:shared_lsm_local::peek}
\end{figure}

Deletions (Figure \ref{fig:shared_lsm_local::peek}) follow a similar pattern:
the local copy of the global block array is refreshed, and a peek operation
is then performed on the array copy. The process is repeated if the global array
has changed since the local copy has last been updated.

As previously mentioned, if there is a cached item from a previous call to
\lstinline|peek| that has not yet been taken, and the global array has not changed
in the meantime, then the cached item can simply be returned without any additional
work.

% TODO: Either introduce benchmark methodology before, or add a pointer
% to the description in the next chapter.

Experiments using performance counters show that in the standard uniform
throughput benchmark, the item cache is hit very frequently. On 35 threads,
$95\%$ of all \lstinline|peek| calls simply hit the cache and return without
doing further work. Of the remaining calls that do real work, only $2\%$ result
in a retry.

These numbers show that the \ac{SLSM} item cache is very effective; however,
the high number of cache hits also imply that almost all items are actually
taken from the \ac{DLSM} during the uniform throughput benchmark --- this might
be a symptom of a problem with the benchmark itself, as we will see in the next
chapter. The low frequency of retries show that peeks are relatively inexpensive,
since they are rarely interrupted by a concurrent insertion.

\subsection{Block Arrays} \label{ssec:block_array_internals}

Block arrays are the lowest level of the \ac{SLSM} proper and represent the
actual \ac{LSM}. The \lstinline|block_array| class (Figure \ref{fig:shared_lsm_local::peek})
contains an array of blocks
conforming to the \ac{LSM} invariants at the boundaries of functions:

\begin{itemize}
\item There is at most a single block of each level present in the array.
\item Blocks are ordered by capacity. In this case, if $i < j$, then
      \lstinline|m_blocks[i]->capacity()| $>$ \lstinline|m_blocks[j]->capacity()|.
\end{itemize}

The \ac{LSM} is represented through member variables holding the block pointer
array together with its size. Additionally, block arrays contain pivot ranges
(storing ranges within the blocks which contain a subset of the $k$ smallest items),
a version (to avoid the ABA problem), and an efficient random number generator.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class block_array {
public:
    /** May only be called when this block is not visible to other threads. */
    void insert(block<K, V> *block,
                block_pool<K, V> *pool);

    /** Callable from other threads. */
    typename block<K, V>::peek_t peek();

private:
    block<K, V> *m_blocks[MAX_BLOCKS];
    size_t m_size;

    block_pivots<K, V, Rlx, MAX_BLOCKS> m_pivots;

    std::atomic<version_t> m_version;

    xorshf96 m_gen;
};
\end{lstlisting}
\caption{The block array header.}
\label{fig:block_array.h}
\end{figure}

During insertions, a block is first such that the block array remains
ordered by capacity. Then, while the block array contains multiple blocks
of the same capacity, it is merged with its predecessor block until all \ac{LSM}
invariants are satisfied. The block array is then compacted: blocks which are less
than half-filled are shrunk and empty blocks are removed.

Finally, pivot ranges are updated (if possible) or recalculated (if necessary).
Most pivot range modifications are incremental updates --- when a range is
less than half-filled (i.e., $\text{range size } < \frac{k}{2}$), the range
is grown from its current state. On the other hand, if new range exceeds its upper
bounds ($\text{range size } > k$), a completely new pivot range is determined.

% TODO: The figure might be redundant.
\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
typename block<K, V>::peek_t
block_array<K, V, Rlx>::peek()
{
    while (true) {
        /* Not shown: pivot range maintenance. */
        int selected_element = m_gen() % m_pivots.count(m_size);

        size_t block_ix;
        block<K, V> *b = nullptr;
        const typename block<K,  V>::block_item *best = nullptr;
        for (block_ix = 0; block_ix < m_size; block_ix++) {
            const int elems_in_range = m_pivots.count_in(block_ix);

            if (selected_element >= elems_in_range) {
                /* Element not in this block. */
                selected_element -= elems_in_range;
                continue;
            }

            b = m_blocks[block_ix];
            best = b->peek_nth(m_pivots.nth_ix_in(selected_element, block_ix));

            break;
        }

        if (!best->taken()) {
            ret = *best;
            return ret;
        } else {
            /* Not shown: pivot range maintenance. */
        }
    }
}
\end{lstlisting}
\caption{The partial \lstinline|block_array::peek| implementation.}
\label{fig:block_array::peek}
\end{figure}

Deletions, shown in Figure \ref{fig:block_array::peek}, use the pivot range
to determine a random item within the $k$ least items in the block array.
This is done as follows: for each block within the block array, pivots store
a range of items guaranteed to be within the $k$ least items of the array.
An item within this range is picked at random, and returned if it has not yet
been taken; otherwise, pivot range maintenance is performed and the operation
is repeated.

As it turns out, representing pivots as contiguous ranges within each block
might not be sufficient to maintain maximal performance. Experiments
show that for every successful peek, $3.5$ failed peeks must be performed,
i.e. every successful peek iterates through the loop 5 times on average.

This effect is caused by fragmentation of the pivot range, which has detrimental
effects both on item selection (through random selection of a taken item)
and relaxation (since the pivot range actually contains fewer elements than
its size, calculated through pivot range boundaries, implies).

We experimented with an alternative data structure for pivot maintainance which
was based on a balanced binary interval tree to store taken items for each block.
Each time a thread encountered a taken item, it would be marked in the interval
tree and excluded from further random selections. Even though operations on the tree
were in $O(\log n)$ where $n$ is the number of marked taken elements within the tree,
maintenance overhead turned out to exceed any gains we made by more accurate
pivot management.

\subsection{Block Pivots} \label{ssec:block_pivot_internals}

In more detail, block pivots consist of upper and lower index boundaries
for each block (see Figure \ref{fig:block_pivots.h}). The \lstinline|shrink|
and \lstinline|grow| methods maintain the pivot range.

Care is taken to recalculate the pivot range infrequently.
Recall that the pivot range is a member variable of the block array
(Figure \ref{fig:block_array.h}); the pivot range is thus calculated once
when the new block is created upon insertion in \lstinline|block_array::insert|,
and then propagated to other threads when local block array copies are updated.
From that point on, threads maintain their own pivot ranges --- however, the next
insertion will create a new global pivot range which all threads then acquire.
In the ideal case, pivot ranges will thus never need to be recalculated by local
threads if insertions are frequent.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx, int MaxBlocks>
class block_pivots {
public:
    size_t shrink(block<K, V> **blocks,
                  const size_t size);
    size_t grow(const int initial_range_size,
                block<K, V> **blocks,
                const size_t size);

private:
    int m_upper[MaxBlocks];
    int m_lower[MaxBlocks];
    K m_maximal_pivot;
};
\end{lstlisting}
\caption{The block pivot header.}
\label{fig:block_pivots.h}
\end{figure}

The original Pheet \klsm grows the pivot range iteratively by starting at the first block,
tentatively picking the next
key beyond the current pivot range as the new upper bound for keys within the range,
and then recalculating the pivot range for each block. If the new pivot range
size is too small, the process is repeated; if it is too large, the pivot range
is reverted to its previous state, and the process is repeated on the next block.

This algorithm can become expensive and cause a large amount of repeated and
wasted work. Instead of iteratively using existing keys, the standalone \klsm
uses binary search to arrive at the desired value of the upper key bound in
logarithmic time. As an additional optimization, the previously used key upper
bound is stored and reused as a lower bound for the binary search in \lstinline|grow|
operations (in which the new upper bound must be higher than its previous value)
and as upper bounds in \lstinline|shrink| calls.

\section{\acl{DLSM} Internals} \label{sec:dlsm_internals}

The \ac{DLSM} is a completely distributed priority queue without any
global quality guarantees. It is, however, extremely scalable, and mostly
responsible for the high performance of the \klsm under benign situations.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class dist_lsm
{
public:
    void insert(const K &key,
                const V &val,
                shared_lsm<K, V, Rlx> *slsm);

    void find_min(typename block<K, V>::peek_t &best);

    int spy();

private:
    thread_local_ptr<dist_lsm_local<K, V, Rlx>> m_local;
};
\end{lstlisting}
\caption{The \ac{DLSM} header.}
\label{fig:dist_lsm.h}
\end{figure}

Its interface, shown in Figure \ref{fig:dist_lsm.h}, consists of the standard
\lstinline|insert| (not shown) and \lstinline|find_min|, as well as a special
insertion function used from the \klsm which includes a \ac{SLSM} argument,
and a \lstinline|spy| function. Again, a thread-local pointer contains the local
part of the data structure.

The local component, called \lstinline|dist_lsm_local| and shown in Figure
\ref{fig:dist_lsm_local.h}, contains the actual \ac{LSM} representation,
memory management pools for blocks and items, a cached item similar to the
\ac{SLSM}, and a random number generator used to determine a victim during
\lstinline|spy| calls.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
class dist_lsm_local
{
public:
    void insert(const K &key,
                const V &val,
                shared_lsm<K, V, Rlx> *slsm);

    void peek(typename block<K, V>::peek_t &best);

    int spy(class dist_lsm<K, V, Rlx> *parent);

private:
    std::atomic<block<K, V> *> m_head; /**< The largest  block. */
    block<K, V>               *m_tail; /**< The smallest block. */
    block<K, V>               *m_spied;

    block_storage<K, V, 4> m_block_storage;
    item_allocator<item<K, V>, typename item<K, V>::reuse> m_item_allocator;

    typename block<K, V>::peek_t m_cached_best;

    xorshf96 m_gen;
};
\end{lstlisting}
\caption{The local \ac{DLSM} header.}
\label{fig:dist_lsm_local.h}
\end{figure}

Contrary to the \ac{SLSM}, the \ac{DLSM} uses a linked list of blocks to represent
the local \ac{LSM}. Blocks (covered further in Section \ref{sec:component_internals})
contain \lstinline|next| pointers accessible by all threads, and \lstinline|prev|
pointers which may only be used by the owning thread. Insertions proceed
similar to the \ac{SLSM}: a singleton block is created which is then inserted
at the tail of the list. Trailing blocks are then merged until there is again
only a single block of each capacity within the \ac{LSM}.

Block merges are performed locally, and are invisible to other threads until
the newly formed \ac{LSM} tail is appended atomically to the linked list of blocks
by a single \ac{CAS} instruction on a \lstinline|next| pointer (or the list head).

In order to accomodate the \klsm design in which \acp{DLSM} may never exceed
the relaxation parameter $k$ (\lstinline|Rlx| within the code) in size,
the \ac{DLSM} insertion method has an \lstinline|slsm| argument containing
the global component of the \klsm. Whenever the largest block of the \ac{DLSM}
exceeds $\frac{k}{2}$ in size at the end of an insertion, the block is removed
from the \ac{DLSM} and added to the \ac{SLSM} instead.

An interesting
facet of the current implementation is that since the \ac{DLSM} and \ac{SLSM}
use different memory management methods for blocks, the \ac{DLSM} block
must be copied into an \ac{SLSM} block upon insertion. However, since this copy
is only performed at most once for each item, it only adds an overhead of $O(1)$.

\begin{figure}[ht]
\begin{lstlisting}
template <class K, class V, int Rlx>
void
dist_lsm_local<K, V, Rlx>::peek(typename block<K, V>::peek_t &best)
{
    /* Short-circuit. */
    if (!m_cached_best.empty() && !m_cached_best.taken()) {
        best = m_cached_best;
        return;
    }

    for (auto i = m_head.load(std::memory_order_relaxed);
            i != nullptr;
            i = i->m_next.load(std::memory_order_relaxed)) {

        /* Not shown: block maintenance. Empty blocks are removed,
         * sparsely filled blocks are shrunk. */

        auto candidate = i->peek();
        if (best.empty() ||
                (!candidate.empty() && candidate.m_key < best.m_key)) {
            best = candidate;
        }
    }

    if (best.empty() && m_spied != nullptr) {
        best = m_spied->peek();
    }

    m_cached_best = best;
}
\end{lstlisting}
\caption{The local \ac{DLSM} header.}
\label{fig:dist_lsm_local.h}
\end{figure}

Code for the \lstinline|peek| function is shown in Figure
\ref{fig:dist_lsm_local.h}. Similar to the \ac{SLSM}, the \ac{DLSM}
keeps a local cache of the previously determined item with the least key,
which is updated both during calls to \lstinline|peek| and during insertions
(in case the newly inserted item is smaller than the previously known least item).
If a valid cached least item is present when \lstinline|peek| is called, it is
simply returned and no further work is done.

Otherwise, we iterate through each block in the \ac{LSM} and inspect its least item.
Since that item is located at the head of the block, access to it can be done
in constant time. If the \ac{LSM} is nonempty, the minimal such item is returned;
alternatively, we fall back to the minimal item of the spied block. There is
a logarithmic number of blocks, and thus the complexity of \lstinline|peek| is
$O(\log n)$.

The implementation of \lstinline|spy| is another area in which the standalone
\klsm deviates from the Pheet \klsm. In Pheet, spying attempts to copy the
entire contents of another thread's local \ac{LSM}, and adds the copied items
to its own linked list of blocks.

However, this has the disadvantage of raising
the frequency of $k$ boundary overflows coupled with resulting block insertions
into the \ac{SLSM}, as well as the insertion of duplicate items into the \ac{SLSM}.
Semantics of the \ac{SLSM} remain valid even in the presence of duplicates ---
recall that the \acp{LSM} store as pointers to items, and each item may only
be taken atomically by a single thread; but such duplicate items waste both space
and time.

On the other hand, the standalone \klsm simplifies \lstinline|spy| to only
copy the largest block from another thread's local \ac{LSM}, and storing
that block separately from the main local block linked list. This change
makes \lstinline|spy| more efficient since only a single block needs to be copied
and no merges must be performed, and additionally prevents insertion of duplicates
into the global \ac{SLSM}.

\section{Component Internals} \label{sec:component_internals}

This section covers the implementation of so-called components,
basic building blocks which are used throughout the \klsm. The most essential
of these are blocks and items, the base containers used to store all items
within the \klsm; and thread-local pointers, which implement local storage while
still granting access to other threads' data on demand.

\subsection{Blocks and Items} \label{ssec:block_internals}
\subsection{Thread-Local Pointers} \label{ssec:thread_local_ptr_internals}


\section{Memory Management} \label{sec:mm_internals}
\subsection{Items} \label{ssec:item_mm_internals}
\subsection{Blocks} \label{ssec:block_mm_internals}

% DLSM: Simpler mechanism since block pointers owned by thread.
% SLSM: insert_block.

\subsection{Block Arrays} \label{ssec:block_array_mm_internals}

% SLSM: insert_block.

\section{Implementation Impact on Performance} \label{sec:implementation_impact}
