\chapter{Definitions} \label{ch:definitions}

This chapter introduces basic terms and concepts required by the remaining
thesis.

% Blocking, lock-free and wait-free data structures.
\section{Lock-freedom}

Concurrent data structures are intended to be accessed simultaneously by several processes
at at the same time. \emph{Lock-based} structures limit the  number of processes that may enter
a critical section at once through the use of locks.
\emph{Lock-free} data structures eschew the use of locks, and guarantee
that at least one process makes progress at all times. Since lock-free structures are
non-blocking, they are not susceptible to priority inversion (in which a high priority process is
prevented from running by a low priority process) and deadlock (two processes wait for a resource held by the other).
\emph{Wait-freedom} further guarantees that every process finishes each operation in a bounded number of steps.
In practice, wait-freedom often introduces an unacceptable overhead; lock-freedom
however has turned out to be both efficient and to scale well to large numbers of processes.
Recently, \citeauthor{kogan2012methodology} have also developed a methodology for implementing efficient
wait-free data structures from lock-free cases \cite{kogan2012methodology}.

% Linearizability, sequential and quiescent consistency.
\section{Correctness Conditions}

% TODO: Expand. Review definitions, extend explanations, relate to sequential sequence of operations.

There are several different criteria which allow reasoning about the correctness of concurrent
data structures. \emph{Linearizable} \cite{herlihy1990linearizability} operations appear to take
effect at a single instant in time between the operation invocation and response at so-called linearization points.
A sequence of concurrent linearizable operations must have an equivalent effect to some legal sequential sequence of
the same operations.
\emph{Quiescently consistent} \cite{shavit1996diffracting} data structures
guarantee that the result of a set of parallel operations is equal to the result of a sequential ordering
after a period of quiescence, i.e. an interval without active operations, has passed; however,
no guarantees are given while one or more operations are in progress.
Linearizability as well as quiescent consistency are composable ---
any data structure composed of linearizable (quiescently consistent) objects is also linearizable
(quiescently consistent).
\emph{Sequential consistency} \cite{lamport1979make} requires the result of a set of operations
executed in parallel to be equivalent to the result of some sequential ordering of the same
operations. Contrary to linearizability and quiescent consistency, sequential consistency is
not composable.

% Relaxed consistency definitions.
\section{Relaxation}

In recent years, weaker versions of these criteria have been investigated as
promising approaches towards higher scalability. Correctness criteria such as
linearizability are usually applied to all operations and all threads (so-called
\emph{global ordering semantics}). On the other hand, in \emph{local ordering semantics},
threads maintain thread-local copies of a central data structure, and modification
to distinct local copies are not linearized between threads. \emph{Quantitative
relaxation} is recent model, which relaxes semantic guarantees such that
operations may occur out of order in relation to a valid sequential operation sequence.

Quasi-linearizability \cite{afek2010quasi}, proposed in \citeyear{afek2010quasi},
was the first relaxed correctness condition and sets a fixed upper bound to the distance
to a valid sequential operation sequence. Quantitative relaxation
\cite{henzinger2013quantitative} is a closely related concept but is based
on the semantics of a data structure, e.g., allowing a priority queue to return
the second-minimal item. $\rho$-relaxation \cite{wimmer2013data,wimmerphd}
is similar to quantitative relaxation, and defines correctness guarantees in terms
of how many items may be skipped, or ignored, by an operation. \citeauthor{wimmerphd}
distinguishes between temporal $\rho$-relaxation, based on the recency of items,
and structural $\rho$-relaxation, which relies on the position of an item within
the data structure. Local linearizability \cite{haas2015local} is a recent,
relatively weak guarantee, and simply requires each thread-induced history
(containing only operations on items inserted by that thread) to be linearizable.

% Synchronization primitives.
\section{Synchronization Primitives}

Lock-free algorithms and data structures are commonly constructed using synchronization primitives
such as \acf{CAS}, \ac{FAA}, \ac{FAO}, and \ac{TAS}. The \ac{CAS} instruction, which atomically
compares a memory location to an expected value and sets it to a new value if they are equal,
is implemented on most modern architectures and can be considered a basic building block of lock-free
programming since it is the most powerful of these operations \cite{herlihy1991wait}.
More exotic primitives such as \ac{DCAS} and \ac{DCSS} exist as well, but are not yet
widely available in hardware and require software emulations (with associated overhead)
to be used \cite{harris2002practical}.

\section{Hardware Topologies}

Multiprocessor machines
are often built by combining several physical processors, each containing a collection
of processing cores (and their associated cache hierarchy), which themselves contain
one or more processing units each. Every processing unit is capable of running
one independent hardware thread.

Shared memory multiprocessor machines usually have a so-called \ac{NUMA} architecture,
in which the efficiency of memory accesses is determined by both the physical location
of the memory and the active processing core. Often, each processing core
has one or more dedicated levels of memory cache (typically L1 and L2 caches),
and a level of shared cache per processor (typically L3). Access to these
cache levels is rises by about an order of magnitude with each level; L1 caches
being the fastest, followed by accesses to L2 and L3 caches, \ac{RAM}, and finally
the hard disk.

Cache coherency is a problem which arises in programs executed concurrently
whenever multiple threads have an identical memory location (variable)
cached in their local caches. Whenever a thread updates the variable, other threads'
caches must be notified that a the variable has changed, and its value must be
updated if required. This type of traffic is called the \emph{cache coherency protocol}.

Locality is another critical aspect of effective cache use. Cache contents are
loaded by blocks containing not just the requested location, but also its neighborhood.
Thus algorithms with sequential data access patterns incur fewer cache loads than those
with random accesses, adding up to significant performance gains.

An area in memory accessed frequently by a large number of processes is said to be \emph{contended}.
Contention is a limiting factor regarding scalability: concurrent reads and writes to the same
location must be serialized by the cache coherence protocol, and only a single concurrent \ac{CAS}
can succeed while all others must retry. \emph{Disjoint-access parallelism} is the concept of
spreading such accesses in order to reduce contention as much as possible.

\section{C++11 Memory Model}

While thread support has previously only been added on to the C++ language through the
\ac{pthreads} library (and others) \cite{boehm2005threads},
the C++11 standard finally specifies a fully multi-threaded abstract state machine with a
clearly defined memory model. A memory model restricts the order in which changes to memory locations by one
thread may become visible to other threads; for instance, usage of the the \lstinline|std::atomic|
type together with its \lstinline|load()| and \lstinline|store()| operations ensures
portable multithreaded behavior across different architectures. It is possible to vary
the strictness of provided guarantees between sequential consistency (on the strict end)
and relaxed behavior (guaranteeing only atomicity).
