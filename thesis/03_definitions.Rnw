\chapter{Definitions} \label{ch:definitions}

% Blocking, lock-free and wait-free data structures.

Concurrent data structures are intended to be accessed simultaneously by several processes
at at the same time. \emph{Lock-based} structures ensure that only a limited number of processes may enter
a critical section at once through the use of locks.
\emph{Lock-free} data structures eschew the use of locks, and guarantee
that at least a single process makes progress at all times. Since lock-free structures are
non-blocking, they are not susceptible to priority inversion, deadlock, and livelock.
\emph{Wait-freedom} further guarantees that every process finishes each operation in a bounded number of steps.
In practice, wait-freedom often introduces an unacceptable overhead; lock-freedom
however has proven to be both efficient and to scale well to large numbers of processes.
Recently, \citeauthor{kogan2012methodology} have also developed a methodology for implementing efficient
wait-free data structures \cite{kogan2012methodology}.

% Linearizability, sequential and quiescent consistency.

There are several different criteria which allow reasoning about the correctness of concurrent
data structures. \emph{Linearizable} \cite{herlihy1990linearizability} operations appear to take
effect at a single instant in time at so-called linearization points.
\emph{Quiescently consistent} \cite{shavit1996diffracting} data structures
guarantee that the result of a set of parallel operations is equal to the result of a sequential ordering
after a period of quiescence, i.e. an interval without active operations has passed.
Linearizability as well as quiescent consistency are composable ---
any data structure composed of linearizable (quiescently consistent) objects is also linearizable
(quiescently consistent).
\emph{Sequential consistency} \cite{lamport1979make} requires the result of a set of operations
executed in parallel to be equivalent to the result of some sequential ordering of the same
operations.

% Relaxed consistency definitions.

In recent years, weaker versions of these criteria have been investigated as
promising approaches towards higher scalability. Correctness criteria such as
linearizability are usually applied to all operations and all threads (so-called
\emph{global ordering semantics}). On the other hand, in \emph{local ordering semantics},
threads maintain thread-local copies of a central data structure, and modification
to distinct local copies are not linearized between threads. \emph{Quantitative
relaxation} is recent model, which relaxes semantic guarantees such that
operations may occur out of order in relation to a valid sequential operation sequence.

Quasi-linearizability \cite{afek2010quasi}, proposed in \citeyear{afek2010quasi},
was the first relaxed correctness condition and sets a fixed upper bound to the distance
to a valid sequential operation sequence. Quantitative relaxation
\cite{henzinger2013quantitative} is a closely related concept but is based
on the semantics of a data structure, e.g., allowing a priority queue to return
the second-minimal item. $\rho$-relaxation \cite{wimmer2013data,wimmerphd}
is similar to quantitative relaxation, and defines correctness guarantees in terms
of how many items may be skipped, or ignored, by an operation. \citeauthor{wimmerphd}
distinguishes between temporal $\rho$-relaxation, based on the recency of items,
and structural $\rho$-relaxation, which relies on the position of an item within
the data structure. Local linearizability \cite{haas2015local} is a recent,
relatively weak guarantee, and simply requires each thread-induced history
(containing only operations on items inserted by that thread) to be linearizable.

% Synchronization primitives.

Lock-free algorithms and data structures are commonly constructed using synchronization primitives
such as \acf{CAS}, \ac{FAA}, \ac{FAO}, and \ac{TAS}. The \ac{CAS} instruction, which atomically
compares a memory location to an expected value and sets it to a new value if they are equal,
is implemented on most modern architectures and can be considered a basic building block of lock-free
programming. More exotic primitives such as \ac{DCAS} and \ac{DCSS} exist as well, but are not yet
widely available and require inefficient software emulations to be used.

% Disjoint-access parallelism.

An area in memory accessed frequently by a large number of processes is said to be \emph{contended}.
Contention is a limiting factor regarding scalability: concurrent reads and writes to the same
location must be serialized by the cache coherence protocol, and only a single concurrent \ac{CAS}
can succeed while all others must retry. \emph{Disjoint-access parallelism} is the concept of
spreading such accesses in order to reduce contention as much as possible.
